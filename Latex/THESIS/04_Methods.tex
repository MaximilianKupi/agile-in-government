%You should present initial experimental results and establish a validation strategy to be performed at the end of experimentation. At the very minimum, you should show that you have setup your data, baseline model code and evaluation metric, and run experiments to obtain some results.
%This stage should help you make progress on your thesis, practice your technical writing skills, and receive feedback on both.

%- Proposed method: details your approaches to the problem (e.g. architecture of the model and any other key methods or algorithms). You should be specific and detailed in describing your main approaches, you may want to include equations and figures. You should also describe your baseline(s). You should clearly state whether your approaches are original or provide necessary references.

%- Experiments: describe the dataset(s) used (provide references); discuss the evaluation metric(s) you used and any other details needed to understand your evaluation strategy; discuss how you ran the experiments, providing necessary technical details; and comment on your qualitative results.

%- Future work: describe what you plan to do for the rest of the project and why.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Methods}
\subsection{Study Design}
In recent years, online text mining has become an increasingly important field in social science research (\cites[37]{Ignatow2018}{Shumate2016}). The application of the technology spans from social media knowledge extraction (\cite{A.Salloumetal.2017}), to analysing leadership behavior in e-learning contexts \parencite{Xie2018} or conducting systematic literature reviews \parencite{Mergel2018}. 

Building on Michael Thelwall's idea of "Webometrics" \parencite{Thelwall2009}, this study employs an exploratory, primarily quantitative research design based on the analysis of self-crawled web content (see also \cite{Olston2010, Jaeger1998, Ignatow2018}). To answer the research question on the evolution and spread of agile governance methods and principles in German and British government institutions, the websites of German and British government cabinets and ministries on the federal and state level were crawled and analysed with respect to the appearance of the term "agil*"\footnote{The star is used to as a placeholder for all potential endings of the word, e.g. agile, agility, agiles, Agilität etc.} (following the keyword search strategy of \cite{Mergel2018}). Thereby gathered insights, in particular the number of agile related sites published as well as the depth in which the topic is being discussed on these sites, are meant to serve as a proxy measure for the evolution and spread of agile governance methods in the respective institutions (\cite{Branco2006, Ghosh2013}). The following three sub-sections describe the methods applied for the pipeline of data collection, preprocessing, and analysis – all of which were programmed and deployed by the author in the programming language Python (\cite{VanRossum1995}).\footnote{The code is publicly accessible on \url{https://github.com/MaximilianKupi/agile-in-government}.} The final prototyping subsection, briefly outlines all the testing and prototyping work that went into developing this pipeline.

\subsection{Data Collection}
The purpose of the data collection process was to efficiently gather as many potentially relevant websites as possible. The process followed the steps identified for routine web crawls by Schäfer and Bildhauer (\cite*{Schafer2012}). As the first step of data collection, a list of all official websites of federal and state cabinets and ministries in the UK and Germany was gathered manually by consulting the respective web presence listings from official government websites (see Table~\ref{tab:List of Ministry Websites} in \href{Appendix B}{Appendix B}). These websites then were crawled with the help of the open source web crawling platform \href{https://scrapy.org }{Scrapy} (\cite{Kouzis-Loukas2016}), specifying the main URLs from which the state / federal ministries can be reached (e.g. \url{https://www.gov.uk} or \url{https://www.bayern.de} etc.) as the "seed URL" \parencite[p. 115]{Barbaresi2015}. These seed URLs serve as the starting point for the crawler: All further pages linked on this page will be crawled subsequently and so on and so forth until the previously specified depth limit is reached or the links leave the previously specified domain range. In order to exclude non-governmental web sources, the crawl was limited to stay within the range of the respective domain of the seed URL as well as all its sub domains.\footnote{E.g. \url{https://www.gov.uk} and \url{https://gds.blog.gov.uk/} etc. or \url{https://www.bayern.de} and \url{https://km.bayern.de} etc.} For efficiency purposes and building on the assumption that institutions would not hide relevant information on deep pages of their websites, links with greater depth than ten pages were ignored (\cite{Scrapy2018, Wang2019}). Furthermore, the crawler was set to ignore the robots.txt file, since for many sites effective web crawling would not have been possible otherwise (\cite{Sun2007}; see also \cite[p. 125]{Barbaresi2015}). The general architecture of Scrapy is outlined in \href{Appendix C}{Appendix C}. In order to search for the respective keyword on a crawled page as well as retrieve and store the relevant web content, the Spider was setup as follows:
%
\begin{compactenum}
    \setlength
    \itemsep{0em}
        \item The crawled site was downloaded as HTML file and parsed into a "soup" object with the help of the \textit{Beautiful Soup} package (\cite{Richardson2007}).
        \item The text of the HTML file was extracted.
        \item Based on the \underline{CONDITION if the search term agil*\footnotemark  was present or not}, the following steps are performed: \footnotetext{In regex: '\textbackslash bagil.?'.}
        \begin{compactenum}
            \item IF the search term \underline{was present}:
            \begin{compactenum}
                \item A random ID for the page was created  based on its URL and stored as an item for later processing in the pipeline together with the URL and domain name.
                \item Four different (potential) publishing date instances, as well as the document's main heading were extracted with the help of XPath expressions and stored as items for later processing in the pipeline (\cite{W3schools.com2020}).
                \item The HTML file was stored in a newly created directory under the newly created ID.
                \item The path name was reported as "saved file" in the log file. 
            \end{compactenum}
            \item IF the search term \underline{was NOT present}:
            \begin{compactenum}
                \item The page was being skipped (reported as "skipping" in the log file).
            \end{compactenum}
        \end{compactenum}
        \item The process repeated from step 1. 
\end{compactenum}
To not get stuck with trying to decipher non-HTML files, all other common file extensions (e.g. "flv", "css", "pdf") were denied for extraction. After the HTML files were parsed and analysed in the Spider, the items are passed to the items pipeline (see Scrapy architecture in Figure~\ref{fig:Scrapy Architecture} in \href{Appendix C}{Appendix C}) which writes them into a CSV file and stores the file for further processing.

The final crawls were run on a specifically set up instance on the \href{https://cloud.google.com/}{Google Cloud Platform} from servers in Belgium, and closely monitored throughout the whole process via an https connected \href{https://jupyterlab.readthedocs.io/en/stable/}{JupyterLab}. File syncing with the local machine was implemented via \href{https://linux.die.net/man/1/rsync}{rsync}. All in all the crawler downloaded 171569 pages.\footnote{For the numbers per state / country see Table~\ref{tab:Number of Sites} in \href{Appendix C}{Appendix C}.}


\subsection{Data Preprocessing}
The purpose of the data  preprocessing was to strip the text from unnecessary web-script elements, extract further date timestamps while selecting the earliest one, and finally, delete duplicates and false positives from the dataset, and save everything in machine readable CSV- and text-file formats. These steps are in line with the recommendations of Lüdeling et al. (\cite*[p. 19]{Ludeling2015}) for the preprocessing of downloaded web pages. In order to achieve maximally reliable results, a mix of automated and manual preprocessing tasks was employed. The automated methods were self implemented and adapted for their language / country specific application. The following two paragraphs list and describe the methods in detail.\par
\paragraph{Automated Preprocessing Tasks:} 
\begin{compactitem}
\item Deleting cases with corrupted HTML files. In order to do so, the encoding format was detected using the \textit{chardet} package \parencite{Pilgrim2015}. In case parsing the file with the respective encoding still yielded a decoding error, the file was deleted from the dataset.\footnote{In total, only one case had to be deleted due to a corrupted HTML file.}
\item Removing all HTML code and other "boilerplate" elements such as headers or links so that only the text content would be left for further text related analyses (\cite*[p. 19]{Ludeling2015}). This step was performed with the help of the \textit{Beautiful Soup} package (\cite{Richardson2007}), and the so cleaned text was stored as a variable as well as in separate text-files.
\item Searching for more specific agil* terms in the respective cleaned text with the help of more restrictive regex expressions and saving the results as a variable.\footnote{The regex expression to catch all English matches was specified to be '\textbackslash bagil\textbackslash b\textbar \textbackslash bagility\textbackslash b', ignoring case. The German regex expression was specified to '\textbackslash bagility\textbackslash b\textbar \textbackslash bagil\textbackslash w\{0,2\}\textbackslash b\textbar \textbackslash bagilität\textbackslash b', ignoring case respectively.} 
\item Extracting the preceding and subsequent context (four words each) of the first appearance of the search term and saving them as variables in order to facilitate the later search for false positives. This was also done using regex expressions.\footnote{The respective regex expression to capture four words before and four words after the first appearance of agil* was set to \resizebox{\textwidth}{!}{'\textbackslash s*([\string^\textbackslash s]+?)\textbackslash s+([\string^\textbackslash s]+?)\textbackslash s+([\string^\textbackslash s]+?)\textbackslash s+([\string^\textbackslash s]+?)\textbackslash s+agil.*?\textbackslash s+([\string^\textbackslash s]+)\textbackslash s+([\string^\textbackslash s]+?)\textbackslash s+([\string^\textbackslash s]+?)\textbackslash s+([\string^\textbackslash s]+?)\textbackslash s+'}}
\item Retrieving further instances for (potential) release date entries from the web pages so that all relevant cases would be equipped with at least one date entry. To achieve this, the parser package \textit{lxml} (\cite{Faassen2006}) was used besides \textit{Beautiful Soup} in order to be able to read XPath (\cite{Clark1999}). In addition to the custom coded functions for date extraction on this particular set of websites, the \textit{HTMLdate} package (\cite{Barbaresi2020}) was used in order to extract further standard date instances (e.g. dates in URLs etc.). To correctly parse the dates into Python datetime objects, the \textit{dateutil} package (\cite{Niemeyer2003}) was used.
\item Selecting the earliest retrieved date instance as a new final publishing date variable. To assert that the earliest date instance would match the actual publishing date for the majority of cases, random samples were drawn from all relevant sites to check for the validity of this assumption. 
\item Searching for the appearance of digital transformation related keywords in the texts and storing the results into a variable.\footnote{The regex expressions were specified to '\textbackslash bdigital\textbackslash b\textbar \textbackslash bditialization\textbackslash b\textbar \textbackslash bdigitalisation\textbackslash b\textbar \textbackslash bdigital transformation\textbackslash b' for English and '\textbackslash bdigital\textbackslash w\{0,2\} \textbackslash b\textbar \textbackslash bdigitalisierung\textbackslash b\textbar \textbackslash bdigitale[n]? transformation\textbackslash b' for German, both ignoring case.}
\item Deleting false positives, i.e. cases that have been downloaded by the crawler but in fact turned out to have nothing to do with agile methods.\footnote{Exemplary cases are ”agile” military in the UK context or ”agile Senioren” (Eng: agile seniors) in the case of German websites.} To do so the following sub-tasks were performed:\footnote{For all tasks involving the deletion of cases the \textit{pandas} package (\cite{McKinney2010}) was used for dataframe handling.}
    \begin{compactitem}
    \item Erasing all cases, where the more specific and restrictive regex match for the agil* terms (see above) did not yield any results. 
    \item Searching for co-search terms (e.g. 'scrum', 'sprint', 'design thinking'; see Table~\ref{tab:Co search terms} in \href{Appendix A}{Appendix A} for the complete list) in the cleaned texts of the sites and deleting cases where no co-search term was found.\footnote{In order to ensure that all inflected forms of a co-search term would be matched, the search terms as well as the cleaned text was lemmatized beforehand using the \textit{spaCy} package \parencite{honnibal-johnson:2015:EMNLP}. This was particularly important for German language to ensure that 'agiles Team', 'agilen Teams', 'agiler Teams' etc. would all be matched. Furthermore, all agile mentioning co-search terms were specified in three different versions (without quotes, with single quotes, and with double quotes) so that 'agile' methods or "agile" methods would also be matched.} This dictionary-based approach has proven useful for various text classification and mining tasks (see for example Liu and Zhang (\cite*{Liu2012}) and  Tang et al. (\cite*{TANG2013})). To compile the list of co-search terms, the glossaries from Kusay-Merkle (\cite*{Kusay-Merkle2018}) and Peterjohann (\cite*{Peterjohann2020}, building on Schwaber  and  Sutherland (\cite*{Schwaber2020})) were used as a starting point. Co-search terms were then deleted or added manually, based on experimental exploration of the respective results for the given set of web pages.\footnote{All in all, more than 100 potentially wrongly erased web sites have been systematically analysed, and the respective co-search terms added to the list.} 
    \item Deleting all sites that were no 'proper' content pages like search result page or table-previews. To do so pages with the respective URL paths, e.g. "csv/preview" or "/search/", were erased.
    \item Erasing cases were the agil* term appeared only on parts of the page that were not properly removed by the boilerplate cleaning (see above) – e.g. where the term was mentioned in a side bar drop down menu or in a live twitter embedding. These cases were identified and deleted making use of the agil* term context (e.g. context post 'development (33) agile' in the case of a drop down menu entry).
    \item Deleting cases from general (not public sector related) job listings as well as from supplier listings based on the respective URL paths, e.g. "/jobs/" or "/supplier/".
    \end{compactitem}
\item Erasing German cases that originated from non-German versions of the website – e.g. French or English translations. This was done to ensure that no duplicates would be caused by having the same page in different languages. To do so the cases with the respective URL paths, e.g. '/FR/' or '/breg-en/', were deleted from the dataset.
\item Deleting duplicates based on the heading of the site or the exact matches for the agil* terms as well as the context of the first agil* term.\footnote{All duplicates were erased so that the site with the oldest publication date was preserved.}
\item Adding the organisation names based on the respective sub-domain name using self compiled dictionaries.\footnote{For Britain a list of domain names from gov.uk (\url{https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/842955/List_of_gov.uk_domain_names_as_at_28_Oct_2019.csv/preview}) was used and missing domains were amended manually on the basis of online research. For Germany there was no such list available and thus all organisation names had to be researched manually.} 
\item Adding the organisation names for all publications on the general domain 'gov.uk'. To do so, a set of seven CSS and XPath selection patterns to scrape the organisation names from the different website set-ups was designed and applied using the \textit{lxml} (\cite{Faassen2006}) and \textit{Beautiful Soup} package (\cite{Richardson2007}).\footnote{In case a site was authored by multiple institutions, the first mentioned institution was selected.} Additionally, the retrieved names had to be cleaned and adapted in order to match the names already existent in the dictionary (see point above), and organisation names for sites where the selectors did not yield any results were added manually. 
\item Deleting all cases published in 2020. Since the crawl was done in the first quarter of 2020, this was done to ensure comparability over all (fully represented) years.
\item Storing the preprocessed datasets into CSV-files, and merging all files into one final master data frame to run the analyses with the help of the \textit{pandas} package (\cite{McKinney2010}). 
\end{compactitem}

\paragraph{Manual Preprocessing:}
\begin{compactitem}
\item Deleting false positives that have not been catched by above automated methods. For example, cases where only one agil* term was found and the matched co-search term appeared in a totally unrelated section of a page or cases where an otherwise reliable co-search term was used in a non-agile related context. To do so, all pages with only one agil* term or co-search term match as well as pages originating from domains usually not related to agile methods were analysed manually by inspecting the agil* term context and/or the respective web content online. The so identified false positives were then deleted from the dataset.
\item Erasing duplicates that have not been catched by above automated methods. For example, cases where preview/summary pages of blogs incorporated other (also in the dataset included) blog articles or cases where multiple, now outdated versions of the same guide have been preserved in an online archive. For that purpose, all pages with the same context for the first appearance of the agil* term but different total agil* term matches or different headings (so that the automated method could not identify them) were analysed manually by inspecting the respective web content online. The so identified duplicates were then deleted from the dataset.
\item Re-specifying wrongly specified publishing dates. To do so, all cases where the different date instances diverged substantially (i.e. dates from different years) were analysed by inspecting the respective web content online. The so identified correct publishing date was then saved as the final publishing date variable for the respective case in the dataset. 
\end{compactitem}

\noindent
After preprocessing, a total of 465 pages was left in the dataset.\footnote{For the numbers per state / country see Table~\ref{tab:Number of Sites} in \href{Appendix C}{Appendix C}.}


\subsection{Data Analysis \& Visualisation}
The purpose of the data analysis and visualisation was to uncover all relevant patterns in the data in order to answer the paper's research question, and visualise these as well as further supplementary, topic related insights in an intuitive and reader-centred way. The major tasks to achieve this were:\footnote{For all data wrangling tasks the \textit{pandas} package (\cite{McKinney2010}) was used. Furthermore, for all plotting the \textit{matplotlib} (\cite{Hunter2007}) and/or the \textit{seaborn} package \parencite{Waskom2012} have been employed. Additional packages used are specified in the text.}
\begin{compactitem}
\item Calculating the percentage of agil* sites also mentioning digitalisation / digital transformation and plotting the results as a pie plot (see Figure~\ref{fig:digital_percentage} in the \href{Analysis}{Analysis Section} as well as in \href{Appendix A}{Appendix A}).
\item Analysing and plotting the number of sites published as well as the number of publishing domains / organisations over time (see Figure~\ref{fig:levels and organisation} in the \href{Analysis}{Analysis Section}). 
\item Comparing the publications by all British as well as German institutions / levels over time and visualising the results as stacked bar plots (see Figures~\ref{fig:German levels over time}, \ref{fig:British organisations over time}, \ref{fig:German ministries over time}, \ref{fig:British ministries over time} in the \href{Analysis}{Analysis Section}).
\item Analysing the average number of co-search terms for British as well as German institutions / levels and visualising them as bar plots (see Figures~\ref{fig:German agile depth federal and state}, \ref{fig:British agile depth organisation} in \href{Appendix A}{Appendix A}).
\item Calculating the number of agil* sites published as well as the average number of co-search terms per site for all German federal states, and visualising these as choropleth map (see Figure~\ref{fig:map} in the \href{Analysis}{Analysis Section}). In addition to \textit{matplotlib} (\cite{Hunter2007}) the \textit{geopandas} \parencite{GeoPandasDevelopers2013} and the \textit{descartes} \parencite{Gillies2020} packages have been used for producing the map visualisations.
\item Comparing the number of agil* sites published as well as the average number of co-search terms per site for all British and German ministries and visualising the results as scatter plots (see Figures~\ref{fig:Breadth and depth of agil* sites published by British and German ministries} and \ref{fig:Breadth and depth of agil* sites published by British and German ministries (with ministry names)} in the \href{Analysis}{Analysis Section} and in \href{Appendix A}{Appendix A}). 
\item Calculating the frequencies of co-search terms for all publications of German as well as British ministries, and visualising these as word clouds, where a bigger word size depicts a higher frequency (see Figure~\ref{fig:wordclouds} in the \href{Analysis}{Analysis Section}). For the word cloud visualisation the \textit{wordcloud} \parencite{Mueller2020} package was used in addition to \textit{matplotlib} (\cite{Hunter2007}).  
\item Selecting the most diverse color palettes possible for plots with a large number of qualitative distinctions so that the reader could still differentiate the colours (see for example Figure~\ref{fig:British organisations over time} in the \href{Analysis}{Analysis Section}). This was done using the \textit{RColorBrewer} package \parencite{Neuwirth2020} in the programming language R since no comparable python package was available. 
\end{compactitem}
\noindent
All preprocessing, analysis, and visualisation tasks were run on a MacBook Pro 2018 with 2.9 GHz Intel Core i9 Processor and 32 GB of RAM memory. 

\subsection{Prototyping \& Testing}
The design of the data pipeline and its methods was an iterative process involving plenty of prototyping and testing work. The following items provide a brief overview:
\begin{compactitem}
\item Selecting and setting up the most efficient and effective crawler package in order to be able to crawl the huge amount of web pages. At first, test crawls were run in R with the RCrawler package \parencite{Khalil2017}. However, it turned out that this package was way too slow and inflexible for the task at hand. For this reason, the final crawler was set up using the scrapy package \cite{Scrapy2018} in python. 
\item Running various crawler test runs on the author's local machine to debug and improve the code before executing the final crawls on \href{https://cloud.google.com/}{Google Cloud Platform}.
\item Designing the most efficient way to get rid of false positives in the data. At first, a negative (deselecting non-relevant cases), purely manual approach was used. However, due to the amount and diversity of text data this turned out to be too time intensive and error prone. Hence, the positive (selecting relevant cases), dictionary-based approach was developed.
\item Amending the code to fetch instances of (potential) publishing dates and finally selecting the date that most likely would constitute the 'real' publishing date. As the website buildup differed from institution to institution and from country to country, the new parsing modes would have to be added as soon as the old ones did not yield any results for newly crawled pages.\footnote{This is also the reason, why some initial dates were parsed during the crawl while others were parsed only at the preprocessing step.}
\item Rerunning crawls and/or merging crawls from various runs for domains where the crawler yielded errors. In cases where the crawl did not run properly – e.g. because it got stuck on certain page sections with thousands of irrelevant page entries – the crawler had to be restarted and/or the crawling results had to be merged with the results from previous runs. 
\end{compactitem}



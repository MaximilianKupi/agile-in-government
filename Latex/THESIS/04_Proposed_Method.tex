%You should present initial experimental results and establish a validation strategy to be performed at the end of experimentation. At the very minimum, you should show that you have setup your data, baseline model code and evaluation metric, and run experiments to obtain some results.
%This stage should help you make progress on your thesis, practice your technical writing skills, and receive feedback on both.

%- Proposed method: details your approaches to the problem (e.g. architecture of the model and any other key methods or algorithms). You should be specific and detailed in describing your main approaches, you may want to include equations and figures. You should also describe your baseline(s). You should clearly state whether your approaches are original or provide necessary references.

%- Experiments: describe the dataset(s) used (provide references); discuss the evaluation metric(s) you used and any other details needed to understand your evaluation strategy; discuss how you ran the experiments, providing necessary technical details; and comment on your qualitative results.

%- Future work: describe what you plan to do for the rest of the project and why.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Proposed Method}
\subsection{General Approach}
This study employs an exploratory quantitative research design (\cite{Olston2010, Jaeger1998}). In order to answer the research question on the evolution and spread of agile governance methods and principles in German and British public administrations, the websites of German and British government ministries on the federal and state level were crawled and analysed with respect to the appearance of the term "agil*" (following the keyword search strategy of \cite{Mergel2018}). Thereby gathered insights thus are meant to serve as a proxy for the relevance of agile governance methods in the respective institutions (\cite{Branco2006, Ghosh2013}). The following three sub-sections describe the methods applied for data collection, cleaning, and analysis – all of which were programmed and deployed in the programming language python (\cite{VanRossum1995}).
%
\subsection{Data Collection}
The purpose of the data collection process was to efficiently gather as many potentially relevant websites as possible. As the first step of data collection, a list of official websites of federal and state ministries in the UK and Germany was gathered manually by consulting the respective web presence listings from official government websites (see Table 3 in \hyperref[Appendix A]{Appendix A}). These websites then were crawled with the help of the open-source web-crawling platform Scrapy (\cite{Kouzis-Loukas2016}), specifying the main urls from which the state / federal ministries can be reached (e.g. "https://www.gov.uk") as the start url. The crawl was limited to stay within the range of the respective domain and its sub domains (e.g. "gov.uk"). Furthermore, the crawler was set to ignore the robots.txt file (\cite{Sun2007}). For efficiency purposes and building on the assumption that institutions would not hide information they deemed important on deep pages of their websites, links with greater depth than ten pages were ignored (\cite{Scrapy2018, Wang2019}). The crawls were run on an specifically set up instance on the Google Cloud Platform.\par
The general data flow in Scrapy is as follows (see \hyperref[fig:Scrapy Architecture]{Figure 3}; adapted from "Architecture overview" in \cite{Scrapy2018}):
\begin{enumerate}
    \setlength\itemsep{0.001em}
        \item The Engine gets the initial Requests to crawl from the Spider.
        \item The Engine schedules the Requests in the Scheduler and asks for the next Requests to crawl.
        \item The Scheduler returns the next Requests to the Engine.
        \item The Engine sends the Requests to the Downloader, passing through the Downloader Middlewares.
        \item Once the page finishes downloading the Downloader generates a Response (with that page) and sends it to the Engine, passing through the Downloader Middlewares.
        \item The Engine receives the Response from the Downloader and sends it to the Spider for processing, passing through the Spider Middleware.
        \item The Spider processes the Response and returns scraped items and new Requests (to follow) to the Engine, passing through the Spider Middleware.
        \item The Engine sends processed items to Item Pipelines, then sends processed Requests to the Scheduler and asks for possible next Requests to crawl.
        \item The process repeats (from step 1) until there are no more requests from the Scheduler.
\end{enumerate}\par 
%
\begin{figure}[ht!]
	\centering
	\includegraphics[height=0.4\textwidth]{{"Latex/THESIS/Figures/scrapy_architecture"}.png}
	\caption[Scrapy Architecture]{Scrapy Architecture (Source: \cite{Scrapy2018})}
	\label{fig:Scrapy Architecture}
\end{figure}

\noindent
In order to search for the respective keyword on a crawled page as well as retrieve and store the respective information, the Spider was setup as follows:
%
\begin{enumerate}
    \setlength\itemsep{0.001em}
        \item The html file is parsed into a "soup" object by the Beautiful Soup package (\cite{Richardson2007}).
        \item The text of the html file is extracted.
        \item Based on the \textit{CONDITION if the search term agil*} (in Regex: '\textbackslash bagil.?') \textit{is present or not} the following steps are performed:
        \begin{enumerate}
            \item IF the search term \textit{is present}:
            \begin{enumerate}
                \item A random ID is created for the page, based on its URL and stored as an item for later processing in the Pipeline.
                \item The page's URL and domain are stored as an item for later processing in the Pipeline.
                \item Four different (potential) publishing date instances, as well as the document's main heading are extracted with the help of XPath expressions and stored as items for later processing in the Pipeline (\cite{W3schools.com2020}).
                \item The html file is stored in a newly created directory (domain name) under the newly created ID.
                \item The path name is reported as "saved file" in the log file. 
            \end{enumerate}
            \item IF the search term \textit{is NOT present}:
            \begin{enumerate}
                \item The page is being skipped (reported as "skipping" in the log file).
            \end{enumerate}
        \end{enumerate}
        \item The process repeats from step 1. 
\end{enumerate}\par
\noindent
To not get stuck with trying to decipher non-html files, all other common file extensions (e.g. "flv", "css", "pdf") were denied for extraction. After the html files were parsed and analysed in the Spider, the items are written into a CSV file and stored for further processing.\par
\noindent
Once the stored items were passed to the Items Pipeline, the pipeline stores the retrieved information as a new row in a CSV file as the spider crawls along.\par 
%
\subsection{Data Cleaning}
The purpose of the data cleaning process was to strip the text from unnecessary web-script elements, extract further date timestamps and select the earliest one, and finally, delete duplicates and false positives from the data set (e.g. agile related to military), save everything in machine readable CSV- and text-file formats. To reach this goal, two algorithms were designed and adapted for their language / country specific application. \par 
\textbf{First Algorithm:} One task of the first algorithm was to ensure, that the search term would only appear in the text body of a page, and not in other elements such as headers or links to other web pages. For this purpose all non-text-relevant elements were deleted from a parsed copy of the html-files with the help of the Beautiful Soup package (\cite{Richardson2007}). The therefrom extracted text was stored in a text-file, and the search for the respective agile term was iterated, this time with more specific regex expressions\footnote{E.g. the regex expression to catch all English matches was specified to be '\textbackslash bagil\textbackslash b\textbar \textbackslash bagility\textbackslash b'.} Also, the preceding and subsequent context (five words each) of the first appearance of the search term was captured in order to facilitate the later search for false positives. Furthermore, another search for the explicit mentioning of the term "agile [...] method*" was conducted.\par 
Since there were cases in which the during scarping defined date instances have yielded no results, the other major task of this algorithm was to retrieve further manually identified instances for (potential) date entries from the web pages so that the majority of cases (more than 80\%) would be equipped with at least one date entry. To achieve this, the parser package lxml was used besides Beautiful Soup in order to be able to read XPath (\cite{Faassen2006, Clark1999}). Since for most relevant cases the earliest date retrieved matched the date when the content of the website was first published,\footnote{This was tested manually by checking a sample of cases with multiple date entries.} a "final date" variable was created with the earliest date as entry for each case. To correctly parse the dates into python datetime objects, the dateutil package was used (\cite{Niemeyer2003}). Finally, the algorithm stored all the newly generated variables (search term matches, context of first match, date entries, and location of the text files) into a CSV file.\par
\textbf{Second Algorithm:} The task of the second algorithm was to get rid of all the duplicates and false positives in the data set. For both of these tasks the pandas python package was used (\cite{McKinney2010}). To erase the false positives two strategies were used. The first strategy was to erase all cases, where the more restrictive regex match from the first algorithm (see above) did not yield any results. The second strategy was to manually look through the context of the remaining matches to identify and remove all cases that had obviously talked about a different subject matter – for example, "agile" military in the UK context or "agile Senioren" (en: agile seniors) in the case of German websites. Finally, the duplicate cases where identified based on the heading as well as the context of the first match and erased so that the version which dates back the longest remained in the data set.\par 

%htmldate package?

\vspace{3pt}
\begin{center}
    \textbf{Everything above has so far only been deployed for the UK and all now following descriptions are work in progress.}
\end{center}

I plan to have a look into the spacy in order use named entity recognition and word vectors to make the extraction of false positives potentially more automatable. 


\subsection{Data Analysis}






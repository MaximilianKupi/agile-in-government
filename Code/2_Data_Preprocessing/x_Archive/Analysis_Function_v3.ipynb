{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from lxml import etree\n",
    "from lxml import html\n",
    "try:\n",
    "    from urllib2 import urlopen\n",
    "except ImportError:\n",
    "    from urllib.request import urlopen\n",
    "import dateutil\n",
    "from dateutil.parser import *\n",
    "from datetime import datetime\n",
    "import pathlib\n",
    "from htmldate import find_date\n",
    "import trafilatura\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process statewise GERMAN VERSION \n",
    "\n",
    "\n",
    "\n",
    "def process_state_DE(state):\n",
    "\n",
    "    agile_regex = re.compile(r'\\bagility\\b|\\bagil\\w{0,2}\\b|\\bagilitÃ¤t\\b', re.IGNORECASE | re.UNICODE)\n",
    "\n",
    "    agile_method_regex = re.compile(r'\\bagil\\w{0,2}\\b((?![.]|agil\\w{0,2}).)*?\\bmethod\\w*?\\b', re.IGNORECASE | re.UNICODE)\n",
    "\n",
    "    agile_context_regex = re.compile(r'\\s*([^\\s]+?)\\s+([^\\s]+?)\\s+([^\\s]+?)\\s+([^\\s]+?)\\s+agil.*?\\s+([^\\s]+)\\s+([^\\s]+?)\\s+([^\\s]+?)\\s+([^\\s]+?)\\s+', re.IGNORECASE | re.UNICODE)\n",
    "\n",
    "    csv_main = csv.DictReader(open(\"/Users/mxm/Google Drive/Masterstudium/Inhalte/Master Thesis/GitHubRepo/agile-in-government/Analysis/1_Data_Collection/DATA/CSVs/Germany/agile_sites_output_Germany_{}.csv\".format(state)), fieldnames=[\"id\", 'url', 'domain', 'date1', 'date2', 'date3', 'heading'])\n",
    "    array = {}\n",
    "    for line in csv_main: \n",
    "        array[line[\"id\"]] = line\n",
    "    text_dir = \"/Users/mxm/Google Drive/Masterstudium/Inhalte/Master Thesis/GitHubRepo/agile-in-government/Analysis/2_Data_Processing/DATA/TextFiles/Germany/{}\".format(state)\n",
    "    pathlib.Path(text_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "######################## STILL TO BE ADAPTED FROM THE UK ######################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for id, line in array.items():\n",
    "        print(\"processing {}\".format(id))\n",
    "        id = line[\"id\"]\n",
    "        domain = line[\"domain\"]\n",
    "        path = '/Users/mxm/Google Drive/Masterstudium/Inhalte/Master Thesis/GitHubRepo/agile-in-government/Analysis/1_Data_Collection/DATA/HTMLs/Germany/{}/{}/{}.html'.format(state, domain, id)\n",
    "        try:  \n",
    "            soup = BeautifulSoup(open(path), \"html.parser\")\n",
    "        except UnicodeDecodeError:\n",
    "            soup = BeautifulSoup(open(path, encoding='windows-1252'), \"html.parser\")    \n",
    "                \n",
    "        #deleting all the Script and Style Elements\n",
    "        for script in soup([\"script\", \"style\", \"a\"]):\n",
    "            script.decompose()    # rip it out\n",
    "                \n",
    "         # get text\n",
    "        doctext = soup.get_text()\n",
    "\n",
    "        # break into lines and remove leading and trailing space on each\n",
    "        lines = (line.strip() for line in doctext.splitlines())\n",
    "        # break multi-headlines into a line each\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        # drop blank lines\n",
    "        doctext = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "        \n",
    "        #old 'simple' cleaning         \n",
    "        #doctext = soup.get_text().replace(\"\\n\", \" \")\n",
    "        with open(\"{}/{}.txt\".format(text_dir, id), \"w\") as textfile:\n",
    "            textfile.write(doctext)\n",
    "        \n",
    "        #Getting date4\n",
    "        date4_element = soup.select_one(\"span.date\")\n",
    "        date4 = \"\"\n",
    "        if date4_element is not None:\n",
    "            date4 = date4_element.get_text()\n",
    "        \n",
    "        #Getting date5\n",
    "\n",
    "        htmlparser = etree.HTMLParser()    \n",
    "        try:\n",
    "            tree = etree.parse(open(path), htmlparser)\n",
    "        except UnicodeDecodeError:\n",
    "            tree = etree.parse(open(path, encoding='windows-1252'), htmlparser)\n",
    "        date5 = tree.xpath(\"substring(substring-after(/html//script[@type='application/ld+json']/text(), 'datePublished'), 4, 23)\")\n",
    "\n",
    "        # a special cases for date 5\n",
    "        date5_1 = tree.xpath(\"substring(substring-after(/html//script[@type='application/ld+json']/text(), 'datePublished'), 4, 25)\")\n",
    "\n",
    "\n",
    "        # Getting date6\n",
    "\n",
    "        date6_element = soup.select_one(\"h1#page-title + p\")\n",
    "        date6 = \"\"\n",
    "        if date6_element is not None:\n",
    "            date6 = date6_element.get_text()\n",
    "\n",
    "\n",
    "        # Getting date7 from National Archives --> the date the site was archived on --> the date the site was released on would have been even earlier\n",
    "\n",
    "        date7 = tree.xpath(\"substring(substring-after(/html/head/script/text(), 'timestamp'),9,14)\")\n",
    "\n",
    "        #Getting date with the htmldate package\n",
    "        htmldate = find_date(tree)\n",
    "\n",
    "\n",
    "        # assinging the already crawled dates\n",
    "        date1 = line[\"date1\"]\n",
    "        date2 = line[\"date2\"]\n",
    "\n",
    "\n",
    "        # Storing the oldest date as final date variable in python date format\n",
    "        date_vars = [date1, date2, date4, date5, date5_1, date6, date7]\n",
    "        \n",
    "        final_date = None \n",
    "\n",
    "        for date_var in date_vars:\n",
    "            try:\n",
    "                if final_date is None:\n",
    "                    final_date = parse(date_var, ignoretz = True)\n",
    "                elif parse(date_var, ignoretz=True) < final_date:\n",
    "                    final_date = parse(date_var, ignoretz=True)\n",
    "            except dateutil.parser._parser.ParserError:\n",
    "                pass \n",
    "\n",
    "\n",
    "        #finding the matches for agile and agility\n",
    "        agile_term = []\n",
    "        agile_term = agile_regex.findall(doctext)\n",
    "\n",
    "        #removing white space, dot etc. from the end of the term\n",
    "        \n",
    "        signs=[' ', '.',':',';']\n",
    "        \n",
    "       # print(agile_term_match)\n",
    "        agile_term=[]\n",
    "        \n",
    "        if agile_term_match != []:\n",
    "            for item in agile_term_match:\n",
    "                if item[-1] in signs:\n",
    "                   # print(\"item\", item)\n",
    "                    newitem = item[:-1]\n",
    "                   # print(\"newitem\", newitem)\n",
    "                else:\n",
    "                    newitem = item\n",
    "                newitem = newitem.lower()\n",
    "                agile_term.append(newitem)\n",
    "\n",
    "        #print(\"newitem\", newitem)\n",
    "        #print(\"agile_term\",agile_term)\n",
    "\n",
    "        #finding the matches for agile...methods\n",
    "        agile_method = []\n",
    "        agile_method = agile_method_regex.findall(doctext.lower())\n",
    "\n",
    "\n",
    "        # finding the context for the agile\n",
    "        agile_context = []\n",
    "        agile_context = agile_context_regex.search(doctext)\n",
    "\n",
    "        agile_context_pre = \"\"\n",
    "        agile_context_post = \"\"\n",
    "        \n",
    "        if agile_context is not None:\n",
    "            agile_context_pre = \" \".join(agile_context.group(1,2,3,4))\n",
    "            agile_context_post = \" \".join(agile_context.group(5,6,7,8))\n",
    "        \n",
    "\n",
    "        # assigning all the variables to the items\n",
    "\n",
    "        if len(agile_term) == 0:\n",
    "            line[\"agile_term\"] = \"\" \n",
    "        else:\n",
    "            line[\"agile_term\"] = \",\".join(agile_term)\n",
    "        \n",
    "        if len(agile_method) == 0:\n",
    "            line[\"agile_method\"] = \"\" \n",
    "        else:\n",
    "            line[\"agile_method\"] = \",\".join(agile_method)\n",
    "\n",
    "        line[\"agile_context_pre\"] = agile_context_pre\n",
    "        line[\"agile_context_post\"] = agile_context_post \n",
    "        line[\"date4\"] = date4\n",
    "        line[\"date5\"] = date5\n",
    "        line[\"date5_1\"] = date5_1\n",
    "        line[\"date6\"] = date6\n",
    "        line[\"date7\"] = date7\n",
    "        line[\"final_date\"] = final_date\n",
    "        line[\"htmldate\"] = htmldate\n",
    "        line[\"country\"] = \"Germany\"\n",
    "        line[\"level\"] = state\n",
    "        line[\"text_file_loc\"] = \"{}/{}.txt\".format(text_dir, id)\n",
    "    \n",
    "    outputfile = open(\"/Users/mxm/Google Drive/Masterstudium/Inhalte/Master Thesis/GitHubRepo/agile-in-government/Analysis/2_Data_Processing/DATA/CSVs/Germany/{}.csv\".format(state), \"w\")\n",
    "    writer = csv.DictWriter(outputfile, fieldnames=[\"id\", \"country\", \"level\", 'url', 'domain', 'date1', 'date2', 'date3', 'date4', 'date5', 'date5_1', 'date6', 'date7', 'final_date', 'htmldate', 'heading', 'agile_term', 'agile_method', 'agile_context_pre', 'agile_context_post', 'text_file_loc'])\n",
    "    writer.writeheader()\n",
    "    for id, line in array.items():\n",
    "        writer.writerow(line)\n",
    "    outputfile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process statewise UK VERSION --> type \"General\"\n",
    "\n",
    "\n",
    "def process_state_UK(state):\n",
    "\n",
    "    agile_regex = re.compile(r'\\bagility\\b|\\bagile\\b', re.IGNORECASE | re.UNICODE)\n",
    "\n",
    "    agile_method_regex = re.compile(r'\\bagile\\b(?:(?![.]|agile).)*?\\bmethod\\w*?\\b', re.IGNORECASE | re.UNICODE)\n",
    "\n",
    "    agile_context_regex = re.compile(r'\\s*([^\\s]+?)\\s+([^\\s]+?)\\s+([^\\s]+?)\\s+([^\\s]+?)\\s+agil.*?\\s+([^\\s]+)\\s+([^\\s]+?)\\s+([^\\s]+?)\\s+([^\\s]+?)\\s+', re.IGNORECASE | re.UNICODE)\n",
    "\n",
    "    csv_main = csv.DictReader(open(\"/Users/mxm/Google Drive/Masterstudium/Inhalte/Master Thesis/GitHubRepo/agile-in-government/Analysis/1_Data_Collection/DATA/CSVs/UK/SAVEPLACE/agile_sites_output_UK_{}.csv\".format(state)), fieldnames=[\"id\", 'url', 'domain', 'date1', 'date2', 'date3', 'heading'])\n",
    "    array = {}\n",
    "    for line in csv_main: \n",
    "        array[line[\"id\"]] = line\n",
    "    text_dir = \"/Users/mxm/Google Drive/Masterstudium/Inhalte/Master Thesis/GitHubRepo/agile-in-government/Analysis/2_Data_Processing/DATA/TextFiles/UK/{}\".format(state)\n",
    "    pathlib.Path(text_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for id, line in array.items():\n",
    "        print(\"processing {}\".format(id))\n",
    "        id = line[\"id\"]\n",
    "        domain = line[\"domain\"]\n",
    "        url = line[\"url\"]\n",
    "        path = '/Users/mxm/Google Drive/Masterstudium/Inhalte/Master Thesis/GitHubRepo/agile-in-government/Analysis/1_Data_Collection/DATA/HTMLs/UK/SAVEPLACE/{}/{}/{}.html'.format(state, domain, id)\n",
    "        try:  \n",
    "            soup = BeautifulSoup(open(path), \"html.parser\")\n",
    "        except UnicodeDecodeError:\n",
    "            soup = BeautifulSoup(open(path, encoding='windows-1252'), \"html.parser\")    \n",
    "        \n",
    "\n",
    "        #Getting date4\n",
    "        date4_element = soup.select_one(\"span.date\")\n",
    "        date4 = \"\"\n",
    "        if date4_element is not None:\n",
    "            date4 = date4_element.get_text()\n",
    "        \n",
    "\n",
    "        # Getting date6\n",
    "\n",
    "        date6_element = soup.select_one(\"h1#page-title + p\")\n",
    "        date6 = \"\"\n",
    "        if date6_element is not None:\n",
    "            date6 = date6_element.get_text()\n",
    "\n",
    "\n",
    "\n",
    "        #deleting all non body text content from the html\n",
    "        for script in soup(['script', 'style', 'a', 'noscript', 'meta', 'head',  'input', 'form']):\n",
    "            script.decompose()    # rip it out\n",
    "            \n",
    "        # get text\n",
    "        doctext = soup.get_text()\n",
    "\n",
    "        # break into lines and remove leading and trailing space on each\n",
    "        lines = (line.strip() for line in doctext.splitlines())\n",
    "        # break multi-headlines into a line each\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        # drop blank lines\n",
    "        doctext = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "        \n",
    "        \n",
    "        #old 'simple' cleaning \n",
    "        #doctext = soup.get_text().replace(\"\\n\", \" \")\n",
    "\n",
    "        with open(\"{}/{}.txt\".format(text_dir, id), \"w\") as textfile:\n",
    "            textfile.write(doctext)\n",
    "        \n",
    "\n",
    "        #Getting date5\n",
    "\n",
    "        htmlparser = etree.HTMLParser()    \n",
    "        try:\n",
    "            tree = etree.parse(open(path), htmlparser)\n",
    "        except UnicodeDecodeError:\n",
    "            tree = etree.parse(open(path, encoding='windows-1252'), htmlparser)\n",
    "        date5 = tree.xpath(\"substring(substring-after(/html//script[@type='application/ld+json']/text(), 'datePublished'), 4, 23)\")\n",
    "\n",
    "        # a special cases for date 5\n",
    "        date5_1 = tree.xpath(\"substring(substring-after(/html//script[@type='application/ld+json']/text(), 'datePublished'), 4, 25)\")\n",
    "\n",
    "\n",
    "        # Getting date7 from National Archives --> the date the site was archived on --> the date the site was released on would have been even earlier\n",
    "\n",
    "        date7 = tree.xpath(\"substring(substring-after(/html/head/script/text(), 'timestamp'),9,14)\")\n",
    "\n",
    "\n",
    "        mytree = html.parse(open(path), htmlparser)\n",
    "\n",
    "        #Getting date with the htmldate package\n",
    "        htmldate = find_date(mytree)\n",
    "\n",
    "        #Getting the main content of the page with the trafilatura package\n",
    "        main_content = trafilatura.extract(mytree)\n",
    "\n",
    "\n",
    "        # assinging the already crawled dates\n",
    "        date1 = line[\"date1\"]\n",
    "        date2 = line[\"date2\"]\n",
    "\n",
    "\n",
    "        # Storing the oldest date as final date variable in python date format\n",
    "        date_vars = [date1, date2, date4, date5, date5_1, date6, date7]\n",
    "        \n",
    "        final_date = None \n",
    "\n",
    "        for date_var in date_vars:\n",
    "            try:\n",
    "                if final_date is None:\n",
    "                    final_date = parse(date_var, ignoretz = True)\n",
    "                elif parse(date_var, ignoretz=True) < final_date:\n",
    "                    final_date = parse(date_var, ignoretz=True)\n",
    "            except dateutil.parser._parser.ParserError:\n",
    "                pass \n",
    "\n",
    "\n",
    "        #finding the matches for agile and agility\n",
    "        agile_term = []\n",
    "        agile_term = agile_regex.findall(doctext.lower())\n",
    "        \n",
    "\n",
    "        #finding the matches for agile...methods\n",
    "        agile_method = []\n",
    "        agile_method = agile_method_regex.findall(doctext.lower())\n",
    "\n",
    "\n",
    "\n",
    "        # finding the context for the agile\n",
    "        agile_context = []\n",
    "        agile_context = agile_context_regex.search(doctext)\n",
    "\n",
    "        agile_context_pre = \"\"\n",
    "        agile_context_post = \"\"\n",
    "        \n",
    "        if agile_context is not None:\n",
    "            agile_context_pre = \" \".join(agile_context.group(1,2,3,4))\n",
    "            agile_context_post = \" \".join(agile_context.group(5,6,7,8))\n",
    "        \n",
    "\n",
    "        # assigning all the variables to the items\n",
    "\n",
    "        if len(agile_term) == 0:\n",
    "            line[\"agile_term\"] = \"\" \n",
    "        else:\n",
    "            line[\"agile_term\"] = \",\".join(agile_term)\n",
    "        \n",
    "        if len(agile_method) == 0:\n",
    "            line[\"agile_method\"] = \"\" \n",
    "        else:\n",
    "            line[\"agile_method\"] = \",\".join(agile_method)\n",
    "\n",
    "        line[\"agile_context_pre\"] = agile_context_pre\n",
    "        line[\"agile_context_post\"] = agile_context_post \n",
    "        line[\"date4\"] = date4\n",
    "        line[\"date5\"] = date5\n",
    "        line[\"date5_1\"] = date5_1\n",
    "        line[\"date6\"] = date6\n",
    "        line[\"date7\"] = date7\n",
    "        line[\"final_date\"] = final_date\n",
    "        line[\"htmldate\"] = htmldate\n",
    "        line[\"country\"] = \"UK\"\n",
    "        line[\"level\"] = state\n",
    "        line[\"main_content\"] = main_content\n",
    "        line[\"text_file_loc\"] = \"{}/{}.txt\".format(text_dir, id)\n",
    "        \n",
    "    outputfile = open(\"/Users/mxm/Google Drive/Masterstudium/Inhalte/Master Thesis/GitHubRepo/agile-in-government/Analysis/2_Data_Processing/DATA/CSVs/UK/{}.csv\".format(state), \"w\")\n",
    "    writer = csv.DictWriter(outputfile, fieldnames=[\"id\", \"country\", \"level\", 'url', 'domain', 'date1', 'date2', 'date3', 'date4', 'date5', 'date5_1', 'date6', 'date7', 'final_date', 'htmldate', 'heading', 'agile_term', 'agile_method', 'agile_context_pre', 'agile_context_post', 'main_content', 'text_file_loc'])\n",
    "    writer.writeheader()\n",
    "    for id, line in array.items():\n",
    "        writer.writerow(line)\n",
    "    outputfile.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "processing d64b7c1c-cddf-3ef7-ab10-f6c32100fc36\nprocessing 001213a3-6bb1-374b-83ff-6d166d567933\nprocessing c41d71f9-6ced-3c39-a661-dd85dff2175c\nprocessing a36f0ef0-ec1b-33b6-aad4-45cda5defa48\nprocessing 4d89205c-ee61-3d36-bdcc-d9ec9b22a082\nprocessing 19dd6541-fe8a-3edb-835b-ab6d94423537\nprocessing 447443cc-ba1b-3d2b-b0b7-5390f81e6569\nprocessing af9dd497-d2a2-35ca-b48f-33c26d4476de\nprocessing 9e8cc26a-36ba-3046-bf44-5c1484f2f0b3\nprocessing 42f53033-7a47-39bf-ade6-3e3623083fb2\nprocessing 1c69cbda-86f6-348a-ba31-a53f12a9197e\nprocessing 83d20ea8-c770-37d6-b880-c7f329c5ced5\nprocessing 9f67207b-61f1-3521-88c4-b533f74a244b\nprocessing b0f2757b-c084-34a0-a104-e1bcbae826c8\nprocessing 68d31c56-39f1-3753-b29d-ac5842eac9d3\nprocessing 9e925771-d3a3-3b40-add7-fde6e74d97f2\nprocessing f470ba9c-6aa9-39a0-86e5-d560d928cd16\nprocessing 9b2b3bc8-d57f-345b-a4cc-66acb4144e91\nprocessing 2233b421-720a-391b-b517-ae3a5233afe4\nprocessing 242e8fc5-7383-3e83-bd2c-a00b7ee9b877\nprocessing 635cbcf2-0fe2-39cd-91b5-d1ad79a8440d\nprocessing 014e2d2f-779e-3685-aac6-47230e4a7942\nprocessing 57879f15-b1f3-3b02-b989-3a40fce1a6cd\nprocessing 0523e375-81c8-3230-81f6-d770b2147060\nprocessing c2afbd48-eb7e-30cb-b288-efab91085399\nprocessing 8f22648d-3f5f-3a1f-b2cc-8d5b65e966e7\nprocessing 8cb87387-f246-3f70-8c10-9425e4e6567a\nprocessing 0d290d17-f152-3f40-a363-333c7c93507d\nprocessing 774b610b-b621-3085-9216-16124b9d2dc8\nprocessing 717643c9-022f-3140-a6da-031c72efe84d\nprocessing f8ba56d5-fcb4-3041-879a-456c9de497ef\nprocessing 353baedc-edab-321f-af3e-e07015d82913\nprocessing b8903cb8-f48f-3593-9ff4-fac09858532e\nprocessing fed3dfdf-48da-3484-96c5-d67bd6fd3a32\nprocessing 50141327-3a3f-302b-916a-85af2bbefc3c\nprocessing 7118b8c9-6cb6-3975-bb7d-c5ce2de5c8ef\nprocessing a05a3327-ab85-3ab1-b313-da14439fb077\nprocessing 41e40ed8-83cd-31d7-ae03-0457636a977a\nprocessing 8ec9f66d-2c8b-32d8-b22d-bc16c6f0fc01\nprocessing 5614360e-96ac-328c-a992-da9ad31c3065\nprocessing d375d6b8-f36f-393e-8b41-2affc5c7bba2\nprocessing a0d97d33-886d-38a0-95f4-aee869267f4f\nprocessing ca04fc0b-9829-3a2b-95b9-e2c33281ffa1\nprocessing d759b78b-cc3d-3d82-8b3d-e83f2d89c715\nprocessing 37c8a228-ef45-3931-bac5-5019c698c5e9\nprocessing c70a1c5d-fa85-3a88-b25d-4403730a98ed\nprocessing 5f3bad21-3443-3db4-adaf-db20d117ad61\nprocessing 004c8d62-c543-3627-a29d-64283d265a36\nprocessing bca89a20-6803-3c81-a57f-1cd2294ddbf0\nprocessing 1a15c230-5a71-3dd0-a85e-c414c38f19df\nprocessing aeceec9d-2e57-351e-8e7b-c7a3024f94de\nprocessing fda72704-4748-32bd-84b9-50a66e5e726c\nprocessing 7b144d58-265d-333f-bec9-74c850942e07\nprocessing 75e9e979-c6ad-33ca-80b4-c1986f2468c8\nprocessing 6af013d4-b660-34b8-9d75-315d8089b521\nprocessing 823ca35d-0056-38c3-9480-bb95ffc2c515\nprocessing cea9b7f1-bbb7-308e-9a1c-68cc5da8876c\nprocessing 8c9f2897-84b0-3102-8ff0-bcdba0849043\nprocessing 0cb6e732-9f51-3d98-a98e-ab11e45d42a3\nprocessing 6fb7c733-bc60-3b01-9402-859441e70cfe\nprocessing 820d52be-59b5-3933-bf25-fda25dcb8aba\nprocessing a97e7c6e-5c74-3823-96d7-a84ce7489587\nprocessing 42e9bc3a-4628-3264-a973-2b4da99f012c\nprocessing 99bc860a-50fb-32a6-a8fc-9ecffc2494c1\nprocessing 85e74b27-e18f-339d-92fa-fed474397689\nprocessing 46f4e547-c4fe-3f86-9516-cfb69d092f07\nprocessing af71c7e9-5440-393b-8279-10c6aebe922d\nprocessing d2c9b90d-401d-3ca4-be67-3336588eec41\nprocessing de7752ec-b795-3463-b5c0-f94e7866b3ad\nprocessing 4586b102-357e-3e82-a162-02342a4e1b18\nprocessing 0be65cb5-4194-3615-b837-a6ef3a1b6db0\nprocessing 3827dfc9-64a5-3ed5-a1a2-3cb02a420aad\nprocessing b3eef5a9-4e9b-387b-a400-acdcec17b896\nprocessing 8d5d0fe5-73e9-39a5-ad8b-98b3f677089f\nprocessing 789c2001-6392-30ee-876c-95a72631857e\nprocessing 0db575da-5559-37c5-ad16-7c3a31f1e956\nprocessing a43b869b-8312-3556-8e3c-365e7e32a958\nprocessing f081e111-cc58-327f-9ab7-b658ce84cc37\nprocessing e366dd34-1af3-39fe-9a10-2ec296bf2b39\nprocessing cb56cf8b-8416-346a-ac67-588b5f1b33ec\nprocessing aca70065-8bbc-3d67-8de4-35012d4ecdec\nprocessing 105fbd0e-33b3-3a5c-8999-da9d53d83698\nprocessing e1c4209a-ee53-390d-b1a7-a3de728fd4ab\nprocessing 29b71149-fb61-3f3b-b9ec-4266b61e4fe6\nprocessing 50728431-5223-3fb2-b8e3-9ebc64f34fd6\nprocessing 47d261ab-e3f3-3881-ad20-5ecd1988734b\nprocessing da743fbd-3936-35a8-91b8-a9ead2c414cb\nprocessing 5bea9c36-7f15-3c2e-80c8-50b3c2ab1b51\nprocessing 22bacf7e-df5a-39db-9887-9a0d8aed26ff\nprocessing f96dc7dc-50de-3f16-8ccb-0f239ff2f9c4\nprocessing 43c0fbba-8838-3ba3-8b5e-9ea1a9eba180\nprocessing 031f2acd-ec89-31d8-9949-c4536f112028\nprocessing e70cb3c1-8b70-382d-9c9d-759d64aeae72\nprocessing e280cca3-7f41-351f-aaf6-fc0f91c3e9b5\nprocessing 49ddef4c-a71e-3ac5-8dc0-65ad596025b6\nprocessing 841aeae8-d85a-3839-a62b-bfecb23871ba\nprocessing 0c0005f9-69e3-3065-930c-f57d70713588\nprocessing 2f34a50f-c817-3d8a-9d4d-25a601afac6e\nprocessing ca861dbc-961b-381f-9d2d-b0f60e6d57dc\n"
    }
   ],
   "source": [
    "#Running processing for Germany\n",
    "process_state_DE(\"Federal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "processing 74ef9f72-3612-3d2b-a14c-e080bb3567eb\n"
    },
    {
     "ename": "AttributeError",
     "evalue": "'lxml.etree._Element' object has no attribute 'text_content'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-25491357b9ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Running processing for UK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprocess_state_UK\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"General\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-44-e18f4670b2f5>\u001b[0m in \u001b[0;36mprocess_state_UK\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;31m#Getting date with the htmldate package\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mhtmldate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_date\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmytree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;31m#Getting the main content of the page with the trafilatura package\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/Scraper_v1/lib/python3.7/site-packages/htmldate/core.py\u001b[0m in \u001b[0;36mfind_date\u001b[0;34m(htmlobject, extensive_search, original_date, outputformat, url, verbose, max_date)\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mexpr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mDATE_EXPRESSIONS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m         dateresult = examine_date_elements(\n\u001b[0;32m--> 639\u001b[0;31m             \u001b[0msearch_tree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensive_search\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_date\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    640\u001b[0m         )\n\u001b[1;32m    641\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdateresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/Scraper_v1/lib/python3.7/site-packages/htmldate/core.py\u001b[0m in \u001b[0;36mexamine_date_elements\u001b[0;34m(tree, expression, outputformat, extensive_search, max_date)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0melements\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# trim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mtemptext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mtemptext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'[\\n\\r\\s\\t]+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemptext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMULTILINE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mtextcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemptext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'lxml.etree._Element' object has no attribute 'text_content'"
     ]
    }
   ],
   "source": [
    "#Running processing for UK\n",
    "process_state_UK(\"General\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Cleaning State Germany\n",
    "def cleaning_state_DE(state):\n",
    "    inputFileName = \"/Users/mxm/Google Drive/Masterstudium/Inhalte/Master Thesis/GitHubRepo/agile-in-government/Analysis/2_Data_Processing/DATA/CSVs/Germany/{}.csv\".format(state)\n",
    "    outputFileName = \"/Users/mxm/Google Drive/Masterstudium/Inhalte/Master Thesis/GitHubRepo/agile-in-government/Analysis/2_Data_Processing/DATA/CSVs/Germany/{}_v1.csv\".format(state)\n",
    "\n",
    "    with open(inputFileName, newline='') as inFile, open(outputFileName, 'w', newline='') as outFile:\n",
    "        df = pd.read_csv(inFile)\n",
    "        df.sort_values(by=['final_date'])\n",
    "        df.dropna(axis=0, how='any', thresh=None, subset=(['agile_term', 'agile_context_pre', 'agile_context_post']), inplace=True)\n",
    "        df.drop_duplicates(subset=(['heading', 'agile_context_pre', 'agile_context_post']), inplace=True)\n",
    "        df['heading'] = df['heading'].astype(str)\n",
    "        #deleting false positives\n",
    "        df = df[~df.agile_context_post.str.startswith(\"(\")] # dropdown menu entry\n",
    "        df = df[~df.agile_context_pre.str.endswith(\")\")] # dropdown menu entry\n",
    "\n",
    "        df.to_csv(outFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Cleaning State UK\n",
    "def cleaning_state_UK(state):\n",
    "    inputFileName = \"/Users/mxm/Google Drive/Masterstudium/Inhalte/Master Thesis/GitHubRepo/agile-in-government/Analysis/2_Data_Processing/DATA/CSVs/UK/{}.csv\".format(state)\n",
    "    outputFileName = \"/Users/mxm/Google Drive/Masterstudium/Inhalte/Master Thesis/GitHubRepo/agile-in-government/Analysis/2_Data_Processing/DATA/CSVs/UK/{}_v1.csv\".format(state)\n",
    "\n",
    "    with open(inputFileName, newline='') as inFile, open(outputFileName, 'w', newline='') as outFile:\n",
    "        df = pd.read_csv(inFile)\n",
    "        df.sort_values(by=['final_date'])\n",
    "        df.dropna(axis=0, how='any', thresh=None, subset=(['agile_term']), inplace=True)\n",
    "        df.drop_duplicates(subset=(['heading', 'agile_context_pre', 'agile_context_post']), inplace=True)\n",
    "        df['agile_context_pre'] = df['agile_context_pre'].astype(str)\n",
    "        df['agile_context_post'] = df['agile_context_post'].astype(str)\n",
    "        #deleting false positives\n",
    "        df = df[~df.agile_context_post.str.startswith(\"(\")] # dropdown menu entry\n",
    "        df = df[~df.agile_context_pre.str.endswith(\")\")] # dropdown menu entry\n",
    "        df = df[~df.agile_context_pre.str.endswith(\"EB\")] # supplier list\n",
    "        df = df[~df.agile_context_post.str.match('and battle-winning armed forces')] #army related agility\n",
    "        df = df[~df.agile_context_pre.str.match('Atom CategoriesCategories Select Category')] # dopdown menu entry\n",
    "        df = df[~df.agile_context_pre.str.match('RAF to Force Generate')] # Entry on Air Commanders\n",
    "        df = df[~df.agile_context_post.str.startswith(\"Trains\")] # name of a company \"Agility Trains\"\n",
    "        df = df[~df.agile_context_post.str.startswith(\"trains\")] # name of a company \"Agility Trains\"\n",
    "        df = df[~df.agile_context_post.str.match('People Services Limited Independent')] #company name\n",
    "        df.to_csv(outFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cleaning_state_DE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-ada40f4a060c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Cleaning State Germany\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcleaning_state_DE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'insert_state_name'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'cleaning_state_DE' is not defined"
     ]
    }
   ],
   "source": [
    "#Cleaning State Germany\n",
    "cleaning_state_DE('insert_state_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning State UK\n",
    "cleaning_state_UK('General')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### END OF ANALYSIS ###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Combining all the Data Frames into one with pandas ########\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "2019-05-29T12:44:02+00:00\n"
    }
   ],
   "source": [
    "#tester for getting dates with Xpath\n",
    "\n",
    "htmlparser = etree.HTMLParser()    \n",
    "try:\n",
    "    tree = etree.parse(open(\"/Users/mxm/Google Drive/Masterstudium/Inhalte/Master Thesis/GithubRepository/Analysis/ALL_CRAWLS/Germany/Federal/www.westmidlands-pcc.gov.uk/765598f5-dc1c-3af2-94a6-96bd59ad0c44.html\", \"r\"), htmlparser)\n",
    "except UnicodeDecodeError:\n",
    "    tree = etree.parse(open(path, encoding='windows-1252'), htmlparser)\n",
    "date8 = tree.xpath(\"substring(substring-after(/html//script[@type='application/ld+json']/text(), 'datePublished'), 4, 25)\")\n",
    "print(date8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[' ']\n"
    }
   ],
   "source": [
    "#testing regex expressions\n",
    "agile_regex = re.compile(r'\\bagile\\b(?:(?![.]|agile).)*?\\bmethod\\w*?\\b', re.IGNORECASE | re.UNICODE)\n",
    "text = 'agile teams            To help people in policy or related areas understand what digital means in government, exploring the methodology'\n",
    "print(agile_regex.findall(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/anaconda3/envs/Scraper_v1/lib/python3.7/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mtype_pprinters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_printers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                 deferred_pprinters=self.deferred_printers)\n\u001b[0;32m--> 702\u001b[0;31m             \u001b[0mprinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m             \u001b[0mprinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/Scraper_v1/lib/python3.7/site-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36mpretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    403\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m                                 \u001b[0;32mand\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'__repr__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m                             \u001b[0;32mreturn\u001b[0m \u001b[0m_repr_pprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_default_pprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/Scraper_v1/lib/python3.7/site-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36m_repr_pprint\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    693\u001b[0m     \u001b[0;34m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m     \u001b[0;31m# Find newlines and replace them with p.break_()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 695\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    696\u001b[0m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/Scraper_v1/lib/python3.7/site-packages/bs4/element.py\u001b[0m in \u001b[0;36m__unicode__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__unicode__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1410\u001b[0m         \u001b[0;34m\"\"\"Renders this PageElement as a Unicode string.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1411\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/Scraper_v1/lib/python3.7/site-packages/bs4/element.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, indent_level, eventual_encoding, formatter)\u001b[0m\n\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformatter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFormatter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m             \u001b[0mformatter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformatter_for_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformatter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1475\u001b[0;31m         \u001b[0mattributes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattributes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1476\u001b[0m         \u001b[0mattrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1477\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/Scraper_v1/lib/python3.7/site-packages/bs4/formatter.py\u001b[0m in \u001b[0;36mattributes\u001b[0;34m(self, tag)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mbackwards\u001b[0m \u001b[0mcompatibility\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0molder\u001b[0m \u001b[0mversions\u001b[0m \u001b[0mof\u001b[0m \u001b[0mBeautiful\u001b[0m \u001b[0mSoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \"\"\"\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "#testing deletion of elements from HTML\n",
    "\n",
    "markup = '<li class=\"gem-c-related-navigation__link\"><a class=\"gem-c-related-navigation__section-link gem-c-related-navigation__section-link--sidebar  gem-c-related-navigation__section-link--other\" data-track-category=\"relatedLinkClicked\" data-track-action=\"1.4 Related content\" data-track-label=\"/government/publications/assurance-for-agile-delivery-of-digital-services\" href=\"/government/publications/assurance-for-agile-delivery-of-digital-services\">Assurance for agile delivery of digital services</a></li>'\n",
    "soup = BeautifulSoup(markup)\n",
    "a_tag = soup.a\n",
    "\n",
    "soup.i.decompose()\n",
    "\n",
    "a_tag\n",
    "# <a href=\"http://example.com/\">I linked to</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('Scraper_v1': conda)",
   "language": "python",
   "name": "python37664bitscraperv1condac67eee00092f451cb06b7dadff488d14"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
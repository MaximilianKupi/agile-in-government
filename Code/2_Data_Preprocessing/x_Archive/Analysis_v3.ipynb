{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from lxml import etree\n",
    "try:\n",
    "    # Python 2\n",
    "    from urllib2 import urlopen\n",
    "except ImportError:\n",
    "    from urllib.request import urlopen\n",
    "import dateutil\n",
    "from dateutil.parser import *\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_main = csv.DictReader(open(\"/Users/mxm/Google Drive/Masterstudium/Inhalte/Master Thesis/GithubRepository/Analysis/PYTHON/Scraper_v1/AgileScraper/AgileScraper/agile_sites_output.csv\"), fieldnames=[\"id\", 'url', 'domain', 'date1', 'date2', 'date3', 'heading'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in csv_main: \n",
    "    array[line[\"id\"]] = line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "processing 74ef9f72-3612-3d2b-a14c-e080bb3567eb\n"
    },
    {
     "ename": "AttributeError",
     "evalue": "type object 'datetime.datetime' has no attribute 'parse'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-c4dfa77ad842>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfinal_date\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2050\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0mfinal_date\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate_var\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mfinal_date\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mfinal_date\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'datetime.datetime' has no attribute 'parse'"
     ]
    }
   ],
   "source": [
    "agile_regex = re.compile(r'\\bagile|\\bagility', re.IGNORECASE | re.UNICODE)\n",
    "agile_context_regex = re.compile(r'\\s*([^\\s]+?)\\s+([^\\s]+?)\\s+([^\\s]+?)\\s+([^\\s]+?)\\s+agil.*?\\s+([^\\s]+)\\s+([^\\s]+?)\\s+([^\\s]+?)\\s+([^\\s]+?)\\s+', re.IGNORECASE | re.UNICODE)\n",
    "text_dir = \"/Users/mxm/Google Drive/Masterstudium/Inhalte/Master Thesis/GithubRepository/Analysis/PYTHON/Analysis/DATA/TextFiles\"\n",
    "\n",
    "date5_xpath = \"substring(substring-after(/html//script[@type='application/ld+json']/text(), 'datePublished'), 4, 24)\"\n",
    "\n",
    "for id, line in array.items():\n",
    "    print(\"processing {}\".format(id))\n",
    "    id = line[\"id\"]\n",
    "    domain = line[\"domain\"]\n",
    "    path = '/Users/mxm/Google Drive/Masterstudium/Inhalte/Master Thesis/GithubRepository/Analysis/ALL_CRAWLS/Germany/Federal/{}/{}.html'.format(domain, id)\n",
    "    try:  \n",
    "        soup = BeautifulSoup(open(path), \"html.parser\")\n",
    "    except UnicodeDecodeError:\n",
    "        soup = BeautifulSoup(open(path, encoding='windows-1252'), \"html.parser\")    \n",
    "    doctext = soup.get_text().replace(\"\\n\", \" \")\n",
    "    with open(\"{}/{}.txt\".format(text_dir, id), \"w\") as textfile:\n",
    "        textfile.write(doctext)\n",
    "    \n",
    "    date4_element = soup.select_one(\"span.date\")\n",
    "    date4 = \"\"\n",
    "    if date4_element is not None:\n",
    "        date4 = date4_element.get_text()\n",
    "    \n",
    "    #Getting date5\n",
    "\n",
    "    htmlparser = etree.HTMLParser()    \n",
    "    try:\n",
    "        tree = etree.parse(open(path), htmlparser)\n",
    "    except UnicodeDecodeError:\n",
    "        tree = etree.parse(open(path, encoding='windows-1252'), htmlparser)\n",
    "    date5 = tree.xpath(date5_xpath)\n",
    "\n",
    "    # Getting date6\n",
    "\n",
    "    date6_element = soup.select_one(\"h1#page-title + p\")\n",
    "    date6 = \"\"\n",
    "    if date6_element is not None:\n",
    "        date6 = date6_element.get_text()\n",
    "\n",
    "\n",
    "    # Storing the oldest date as final date variable in python date format\n",
    "    date_vars = ['date1', 'date2', 'date4', 'date5', 'date6']\n",
    "    \n",
    "\n",
    "    final_date = datetime(2050, 1, 1, 0, 0)\n",
    "\n",
    "    for date_var in date_vars:\n",
    "        try:\n",
    "            if final_date == datetime(2050, 1, 1, 0, 0):\n",
    "                final_date = parse(date_var)\n",
    "            elif parse(date_var) < final_date:\n",
    "                final_date = parse(date_var)\n",
    "        except dateutil.parser._parser.ParserError:\n",
    "            pass \n",
    "\n",
    "\n",
    "\n",
    "    agil_term = agile_regex.findall(doctext)\n",
    "    agil_context = agile_context_regex.search(doctext)\n",
    "    agil_context_pre = \"\"\n",
    "    agil_context_post = \"\"\n",
    "    if agil_context is not None:\n",
    "        agil_context_pre = \" \".join(agil_context.group(1,2,3,4))\n",
    "        agil_context_post = \" \".join(agil_context.group(5,6,7,8))\n",
    "    if len(agil_term) == 0:\n",
    "        line[\"agil_term\"] = \"\" \n",
    "    else:\n",
    "        line[\"agil_term\"] = \",\".join(agil_term)\n",
    "    line[\"agil_context_pre\"] = agil_context_pre\n",
    "    line[\"agil_context_post\"] = agil_context_post \n",
    "    line[\"date4\"] = date4\n",
    "    line[\"date5\"] = date5\n",
    "    line[\"date6\"] = date6\n",
    "    line[\"final_date\"] = final_date\n",
    "    line[\"country\"] = \"Britain\"\n",
    "    line[\"level\"] = \"general\"\n",
    "    line[\"text_file_loc\"] = \"{}/{}.txt\".format(text_dir, id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputfile = open(\"/Users/mxm/Google Drive/Masterstudium/Inhalte/Master Thesis/GithubRepository/Analysis/PYTHON/Analysis/DATA/UK.csv\", \"w\")\n",
    "writer = csv.DictWriter(outputfile, fieldnames=[\"id\", \"country\", \"level\", 'url', 'domain', 'date1', 'date2', 'date3', 'date4', 'date5', 'date6', 'final_date', 'heading', 'agil_term', 'agil_context_pre', 'agil_context_post', 'text_file_loc'])\n",
    "writer.writeheader()\n",
    "for id, line in array.items():\n",
    "    writer.writerow(line)\n",
    "outputfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deleting double rows (if any) and rows where agil_term is empty hence not \"agile\" or \"agility\"\n",
    "\n",
    "inputFileName = \"/Users/mxm/Google Drive/Masterstudium/Inhalte/Master Thesis/GithubRepository/Analysis/PYTHON/Analysis/DATA/UK.csv\"\n",
    "outputFileName = \"/Users/mxm/Google Drive/Masterstudium/Inhalte/Master Thesis/GithubRepository/Analysis/PYTHON/Analysis/DATA/UK_v1.csv\"\n",
    "\n",
    "\n",
    "with open(inputFileName, newline='') as inFile, open(outputFileName, 'w', newline='') as outFile:\n",
    "    df = pd.read_csv(inFile)\n",
    "    df.dropna(axis=0, how='any', thresh=None, subset=(['agil_term', 'agil_context_pre', 'agil_context_post']), inplace=True)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df.to_csv(outFile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('Scraper_v1': conda)",
   "language": "python",
   "name": "python37664bitscraperv1condac67eee00092f451cb06b7dadff488d14"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitscraperv1condac67eee00092f451cb06b7dadff488d14",
   "display_name": "Python 3.7.6 64-bit ('Scraper_v1': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the required packages\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from lxml import etree\n",
    "from lxml import html\n",
    "try:\n",
    "    from urllib2 import urlopen\n",
    "except ImportError:\n",
    "    from urllib.request import urlopen\n",
    "import dateutil\n",
    "from dateutil.parser import *\n",
    "from datetime import datetime\n",
    "import pathlib\n",
    "from htmldate import find_date\n",
    "import justext\n",
    "import requests\n",
    "from newspaper import fulltext\n",
    "import codecs\n",
    "import html2text\n",
    "import trafilatura\n",
    "import spacy\n",
    "import dframcy\n",
    "from dframcy import DframCy\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### ENGLAND ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the function to read in all the html files, clean the text, get further publishing dates and select the earliest one, and store everything into csv\n",
    "# process statewise UK VERSION --> type \"General\"\n",
    "\n",
    "def process_state_UK(state):\n",
    "\n",
    "    # compiling the regex search terms\n",
    "    agile_regex = re.compile(r'\\bagility\\b|\\bagile\\b', re.IGNORECASE | re.UNICODE) # for agile\n",
    "    agile_context_regex = re.compile(r'\\s*([^\\s]+?)\\s+([^\\s]+?)\\s+([^\\s]+?)\\s+([^\\s]+?)\\s+agil.*?\\s+([^\\s]+)\\s+([^\\s]+?)\\s+([^\\s]+?)\\s+([^\\s]+?)\\s+', re.IGNORECASE | re.UNICODE) # for context of the first mention\n",
    "    digital_regex = re.compile(r'\\bdigital\\b|\\bditialization\\b|\\bdigitalisation\\b\\bdigital transformation\\b', re.IGNORECASE | re.UNICODE) # for anything related to digital\n",
    "\n",
    "    # getting the csv which was produced by scrapy as input file\n",
    "    csv_main = csv.DictReader(open(\"/Users/mxm/Google Drive/Masterstudium/Inhalte/Master Thesis/GitHubRepo/agile-in-government/Analysis/1_Data_Collection/DATA/CSVs/UK/SAVEPLACE/agile_sites_output_UK_{}.csv\".format(state)), fieldnames=[\"id\", 'url', 'domain', 'date1', 'date2', 'date3', 'heading']) \n",
    "\n",
    "    # creating an empty array to write into in the processing loop\n",
    "    array = {}\n",
    "\n",
    "    # getting every line of the input csv into the array\n",
    "    for line in csv_main: \n",
    "        array[line[\"id\"]] = line\n",
    "    \n",
    "    # specfifying the the directory where the text files should be stored\n",
    "    text_dir = \"/Users/mxm/Google Drive/Masterstudium/Inhalte/Master Thesis/GitHubRepo/agile-in-government/Analysis/2_Data_Preprocessing/DATA/TextFiles/UK/{}\".format(state)\n",
    "    \n",
    "    # making the directory if it doesn't exist\n",
    "    pathlib.Path(text_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # specifying variables to keep track of the progress while processing\n",
    "    total_count = len(array)\n",
    "    processed = 0\n",
    "\n",
    "    # this is the main working loop\n",
    "    for id, line in array.items():   \n",
    "        # print function to keep track of which files are being processed     \n",
    "        print(\"processing {}\".format(id))\n",
    "        # getting the id from the array\n",
    "        id = line[\"id\"]\n",
    "        # getting the domain from the array\n",
    "        domain = line[\"domain\"]\n",
    "        # getting the url from the array\n",
    "        url = line[\"url\"]\n",
    "        # specifying the path where the html to process is located\n",
    "        path = '/Users/mxm/Google Drive/Masterstudium/Inhalte/Master Thesis/GitHubRepo/agile-in-government/Analysis/1_Data_Collection/DATA/HTMLs/UK/SAVEPLACE/{}/{}/{}.html'.format(state, domain, id)\n",
    "        # opening the html file while catching an encoding error     \n",
    "        try:  \n",
    "            soup = BeautifulSoup(open(path), \"html.parser\")\n",
    "        except UnicodeDecodeError:\n",
    "            soup = BeautifulSoup(open(path, encoding='windows-1252'), \"html.parser\")    \n",
    "        \n",
    "        ### In the following further potential publication dates are parsed and the oldest one is select as \"final date\" ###\n",
    "\n",
    "        # Getting date4\n",
    "        date4_element = soup.select_one(\"span.date\")\n",
    "        date4 = \"\"\n",
    "        if date4_element is not None:\n",
    "            date4 = date4_element.get_text()\n",
    "        \n",
    "        # Getting date6\n",
    "        date6_element = soup.select_one(\"h1#page-title + p\")\n",
    "        date6 = \"\"\n",
    "        if date6_element is not None:\n",
    "            date6 = date6_element.get_text()\n",
    "\n",
    "       # Getting date5\n",
    "        htmlparser = etree.HTMLParser()    \n",
    "        try:\n",
    "            tree = etree.parse(open(path), htmlparser)\n",
    "        except UnicodeDecodeError:\n",
    "            tree = etree.parse(open(path, encoding='windows-1252'), htmlparser)\n",
    "        \n",
    "        date5 = tree.xpath(\"substring(substring-after(/html//script[@type='application/ld+json']/text(), 'datePublished'), 4, 23)\")\n",
    "\n",
    "        # a special cases for date 5\n",
    "        date5_1 = tree.xpath(\"substring(substring-after(/html//script[@type='application/ld+json']/text(), 'datePublished'), 4, 25)\")\n",
    "\n",
    "\n",
    "        # Getting date7 from National Archives --> the date the site was archived on --> the date the site was released on would have been even earlier\n",
    "        date7 = tree.xpath(\"substring(substring-after(/html/head/script/text(), 'timestamp'),9,14)\")\n",
    "\n",
    "  \n",
    "        # different parsing for for htmldate\n",
    "        try:\n",
    "            html_tree = html.parse(open(path))\n",
    "        except UnicodeDecodeError:\n",
    "            html_tree = html.parse(open(path, encoding='windows-1252'))\n",
    "            \n",
    "\n",
    "        # Getting date with the htmldate package\n",
    "        htmldate = find_date(html_tree)\n",
    "\n",
    "\n",
    "        # since html date gives out nonetype when it doesn't find anything, it has to be respecified to empty string so that it works with dateparser later on\n",
    "        if htmldate is None:\n",
    "            htmldate = ''\n",
    "\n",
    "        #htmldate = htmldate.astype('str')\n",
    "\n",
    "\n",
    "\n",
    "        # assinging the already succesfully parsed dates during crawling stage\n",
    "        date1 = line[\"date1\"]\n",
    "        date2 = line[\"date2\"]\n",
    "\n",
    "\n",
    "        # Storing the oldest date as final date variable in python date format\n",
    "        date_vars = [date1, date2, date4, date5, date5_1, date6, date7, htmldate]\n",
    "        \n",
    "        final_date = None \n",
    "\n",
    "        for date_var in date_vars:\n",
    "            try:\n",
    "                if final_date is None:\n",
    "                    final_date = parse(date_var, ignoretz = True)\n",
    "                elif parse(date_var, ignoretz=True) < final_date:\n",
    "                    final_date = parse(date_var, ignoretz=True)\n",
    "            except dateutil.parser._parser.ParserError:\n",
    "                pass \n",
    "\n",
    "\n",
    "\n",
    "        #deleting all non-body text content from the html\n",
    "        \n",
    "        # defining the html elements to delete\n",
    "        for script in soup(['script', 'style', 'meta', 'a', 'head', 'footer', 'navbar', 'header', 'search-box', 'global-cookie-message', 'id=\"global-cookie-message\"', 'global-bar', 'menu', 'noscript', 'global-cookie-message', 'search']):\n",
    "            script.decompose()    # deleting those elements\n",
    "            \n",
    "        # geting text\n",
    "        doctext = soup.get_text()\n",
    "\n",
    "        # break into lines and remove leading and trailing space on each\n",
    "        lines = (line.strip() for line in doctext.splitlines())\n",
    "        # break multi-headlines into a line each\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        # drop blank lines\n",
    "        doctext = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "        \n",
    "  \n",
    "\n",
    "        # saving the cleaned text into the directory\n",
    "        with open(\"{}/{}.txt\".format(text_dir, id), \"w\") as textfile:\n",
    "            textfile.write(doctext)\n",
    "        \n",
    "\n",
    " \n",
    "        #### In the following the matches for the search terms (agile, digital) are gathered ####\n",
    "        \n",
    "        # finding the matches for agile and agility\n",
    "        agile_term = []\n",
    "        agile_term = agile_regex.findall(doctext.lower())\n",
    "    \n",
    "\n",
    "        # finding the matches for digital\n",
    "        digital_term = []\n",
    "        digital_term = digital_regex.findall(doctext.lower())\n",
    "\n",
    "\n",
    "        # finding the context for agile\n",
    "        agile_context = []\n",
    "        agile_context = agile_context_regex.search(doctext)\n",
    "\n",
    "        agile_context_pre = \"\"\n",
    "        agile_context_post = \"\"\n",
    "        \n",
    "        if agile_context is not None:\n",
    "            agile_context_pre = \" \".join(agile_context.group(1,2,3,4))\n",
    "            agile_context_post = \" \".join(agile_context.group(5,6,7,8))\n",
    "        \n",
    "\n",
    "        # assigning all generated variables as line items to the array\n",
    "\n",
    "        if len(agile_term) == 0:\n",
    "            line[\"agile_term\"] = \"\" \n",
    "        else:\n",
    "            line[\"agile_term\"] = \",\".join(agile_term)\n",
    "\n",
    "        if len(digital_term) == 0:\n",
    "            line[\"digital_term\"] = \"\" \n",
    "        else:\n",
    "            line[\"digital_term\"] = \",\".join(digital_term)\n",
    "\n",
    "\n",
    "        line[\"agile_context_pre\"] = agile_context_pre\n",
    "        line[\"agile_context_post\"] = agile_context_post \n",
    "        line[\"date4\"] = date4\n",
    "        line[\"date5\"] = date5\n",
    "        line[\"date5_1\"] = date5_1\n",
    "        line[\"date6\"] = date6\n",
    "        line[\"date7\"] = date7\n",
    "        line[\"final_date\"] = final_date\n",
    "        line[\"htmldate\"] = htmldate\n",
    "        line[\"country\"] = \"UK\"\n",
    "        line[\"level\"] = state\n",
    "        line[\"text_file_loc\"] = \"{}/{}.txt\".format(text_dir, id)\n",
    "        line['doctext'] = doctext\n",
    "\n",
    "        \n",
    "        # Printing processing status\n",
    "        processed += 1\n",
    "        print(\"processed: {}%\".format((processed/total_count)*100))\n",
    "        print(\"Current Time: \", datetime.now())\n",
    "\n",
    "    # opening the output csv\n",
    "    outputfile = open(\"/Users/mxm/Google Drive/Masterstudium/Inhalte/Master Thesis/GitHubRepo/agile-in-government/Analysis/2_Data_Preprocessing/DATA/CSVs/UK/{}.csv\".format(state), \"w\")\n",
    "    \n",
    "    # writing all the array lines into the csv file\n",
    "    writer = csv.DictWriter(outputfile, fieldnames=[\"id\", \"country\", \"level\", 'url', 'domain', 'date1', 'date2', 'date3', 'date4', 'date5', 'date5_1', 'date6', 'date7', 'htmldate', 'final_date', 'heading', 'agile_term', 'agile_context_pre', 'agile_context_post', 'digital_term', 'text_file_loc', 'doctext',])\n",
    "    writer.writeheader()\n",
    "    for id, line in array.items():\n",
    "        writer.writerow(line)\n",
    "    outputfile.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nprocess_state_UK(\"General\")\\n'"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Running processing for UK\n",
    "\"\"\"\n",
    "process_state_UK(\"General\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Praparing the cleaning algorithm\n",
    "\n",
    "# Writing Spacy Lemmatizer for lists function\n",
    "def spacy_list_lemmatizer(text, nlp):\n",
    "    nlp.disable_pipes('tagger', 'ner')\n",
    "    doclist = list(nlp.pipe(text, n_threads=3,  batch_size=50))\n",
    "    docs=[]\n",
    "    for i, doc in enumerate(doclist):\n",
    "        docs.append(' '.join([listitem.lemma_ for listitem in doc]))\n",
    "    return docs\n",
    "\n",
    "\n",
    "# creating the lemmas of all english co-searchterms\n",
    "searchterms_eng = [\"'agile' capability\", \"'agile' ceremony\", \"'agile' certificate\", \"'agile' certification\", \"'agile' coach\", \"'agile' coaching\", \"'agile' delivery\", \"'agile' development\", \"'agile' digital capability \", \"'agile' environment\", \"'agile' innovation\", \"'agile' management\", \"'agile' manifesto\", \"'agile' method\", \"'agile' methodology\", \"'agile' mindset\", \"'agile' operating model\", \"'agile' organisation\", \"'agile' organization unit\", \"'agile' principles\", \"'agile' processes\", \"'agile' programming\", \"'agile' project\", \"'agile' project management\", \"'agile' software development\", \"'agile' stages\", \"'agile' structures\", \"'agile' team\", \"'agile' techniques\", \"'agile' transformation\", \"'agile' transition\", \"'agile' Value\", \"'agile' walls\", \"'agile' way\", \"'agile' way of working\", \"'agile' work\", \"'agile' work practices\", \"'agile' working and management methods\", \"'agile' working culture\", \"'agile' working methods\", \"agile capability\", \"agile ceremony\", \"agile certificate\", \"agile certification\", \"agile coach\", \"agile coaching\", \"agile delivery\", \"agile development\", \"agile digital capability \", \"agile environment\", \"agile innovation\", \"agile management\", \"agile manifesto\", \"agile method\", \"agile methodology\", \"agile mindset\", \"agile operating model\", \"agile organisation\", \"agile organization unit\", \"agile principles\", \"agile processes\", \"agile programming\", \"agile project\", \"agile project management\", \"agile software development\", \"agile stages\", \"agile structures\", \"agile team\", \"agile techniques\", \"agile transformation\", \"agile transition\", \"agile Value\", \"agile walls\", \"agile way\", \"agile way of working\", \"agile work\", \"agile work practices\", \"agile working and management methods\", \"agile working culture\", \"agile working methods\", \"'agile' capability\", \"'agile' ceremony\", \"'agile' certificate\", \"'agile' certification\", \"'agile' coach\", \"'agile' coaching\", \"'agile' delivery\", \"'agile' development\", \"'agile' digital capability \", \"'agile' environment\", \"'agile' innovation\", \"'agile' management\", \"'agile' manifesto\", \"'agile' method\", \"'agile' methodology\", \"'agile' mindset\", \"'agile' operating model\", \"'agile' organisation\", \"'agile' organization unit\", \"'agile' principles\", \"'agile' processes\", \"'agile' programming\", \"'agile' project\", \"'agile' project management\", \"'agile' software development\", \"'agile' stages\", \"'agile' structures\", \"'agile' team\", \"'agile' techniques\", \"'agile' transformation\", \"'agile' transition\", \"'agile' Value\", \"'agile' walls\", \"'agile' way\", \"'agile' way of working\", \"'agile' work\", \"'agile' work practices\", \"'agile' working and management methods\", \"'agile' working culture\", \"'agile' working methods\", \"backlog\", \"co-design\", \"collaboration spaces\", \"collaborative design\", \"cross-departmental approach\", \"daily scrum\", \"definition of done\", \"definition of ready\", \"design thinking\", \"development team\", \"empathy map\", \"extreme user\", \"fail fast\", \"fail forward\", \"free of hierarchy\", \"hierarchy free space\", \"innovation culture\", \"innovation management\", \"iteration\", \"iterative\", \"Kanban\", \"kanban board\", \"lean software development\", \"lean start up\", \"lean start-up\", \"lean startup\", \"minimum viable product\", \"MVP\", \"new team and management structures\", \"new work\", \"new working methods\", \"new working models\", \"planning poker\", \"product backlog\", \"product owner\", \"project canvas\", \"project work\", \"project-based\", \"scrum\", \"scrum coach\", \"scrum master\", \"scrum team\", \"service design\", \"short-cycle results\", \"software development\", \"sprint\", \"sprint backlog\", \"sprint planning\", \"sprint retrospective\", \"sprint review\", \"time box\", \"timeboxing\", \"user needs\", \"user requirements\", \"user research\", \"user story\", \"user testing\", \"user-centred\", \"user-centred design\", \"user-friendly solutions\", \"user-oriented\", \"visual thinking\", \"whiteboard\", \"wicked problem\", \"wireframe\"]\n",
    "\n",
    "nlp_eng = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "searchterms_eng_lemma = spacy_list_lemmatizer(searchterms_eng, nlp_eng)\n",
    "\n",
    "searchterms_eng_lemma = [each_string.lower() for each_string in searchterms_eng_lemma]\n",
    "\n",
    "# creating the lemmas of all German and English co-searchterms for German sites, since English terms might also appear on German sites\n",
    "searchterms_de_eng = [\"abteilungsübergreifender Ansatz\", \"agil arbeitende Organisationseinheit\", \"agil arbeitende Teams\", \"agile Arbeitsformen\", \"agile Arbeitskultur\", \"agile Arbeitsmethoden\", \"agile Arbeitspraktiken\", \"agile Arbeitsumgebung\", \"agile Arbeitsweise\", \"agile Bereitstellung\", \"agile digitale Fähigkeit \", \"agile Entwicklung\", \"agile Fähigkeiten \", \"agile Innovation\", \"agile Methode\", \"agile Methodik\", \"agile Organisation\", \"agile Phasen\", \"agile Prinzipien\", \"agile Prozesse\", \"agile Softwareentwicklung\", \"agile Softwareweiterentwicklung\", \"agile Strukturen\", \"agile Techniken\", \"agile Transformation\", \"Agile Transition\", \"agile Wände\", \"agile Weise\", \"agile Zeremonie\", \"agile Zertifizierung\", \"agilen Arbeits- und Managementmethoden\", \"agiler Ansatz\", \"agiler Coach\", \"agiler Wert\", \"agiles Arbeiten\", \"agiles Betriebsmodell\", \"agiles Coaching\", \"agiles Management\", \"Agiles Manifest\", \"agiles Mindset\", \"agiles Programmieren\", \"agiles Projekt\", \"Agiles Projektmanagement\", \"agiles Team\", \"agiles Training \", \"agiles Zertifikat\", '\"agil\" arbeitende Organisationseinheit', '\"agil\" arbeitende Teams', '\"agile\" Arbeitsformen', '\"agile\" Arbeitskultur', '\"agile\" Arbeitsmethoden', '\"agile\" Arbeitspraktiken', '\"agile\" Arbeitsumgebung', '\"agile\" Arbeitsweise', '\"agile\" Bereitstellung', '\"agile\" digitale Fähigkeit ', '\"agile\" Entwicklung', '\"agile\" Fähigkeiten ', '\"agile\" Innovation', '\"agile\" Methode', '\"agile\" Methodik', '\"agile\" Organisation', '\"agile\" Phasen', '\"agile\" Prinzipien', '\"agile\" Prozesse', '\"agile\" Softwareentwicklung', '\"agile\" Softwareweiterentwicklung', '\"agile\" Strukturen', '\"agile\" Techniken', '\"agile\" Transformation', '\"agile\" Transition', '\"agile\" Wände', '\"agile\" Weise', '\"agile\" Zeremonie', '\"agile\" Zertifizierung', '\"agilen\" Arbeits- und Managementmethoden', '\"agiler\" Ansatz', '\"agiler\" Coach', '\"agiler\" Wert', '\"agiles\" Arbeiten', '\"agiles\" Betriebsmodell', '\"agiles\" Coaching', '\"agiles\" Management', '\"agiles\" Manifest', '\"agiles\" Mindset', '\"agiles\" Programmieren', '\"agiles\" Projekt', '\"agiles\" Projektmanagement', '\"agiles\" Team', '\"agiles\" Training ', '\"agiles\" Zertifikat', \"'agil' arbeitende Organisationseinheit\", \"'agil' arbeitende Teams\", \"'agile' Arbeitsformen\", \"'agile' Arbeitskultur\", \"'agile' Arbeitsmethoden\", \"'agile' Arbeitspraktiken\", \"'agile' Arbeitsumgebung\", \"'agile' Arbeitsweise\", \"'agile' Bereitstellung\", \"'agile' digitale Fähigkeit \", \"'agile' Entwicklung\", \"'agile' Fähigkeiten \", \"'agile' Innovation\", \"'agile' Methode\", \"'agile' Methodik\", \"'agile' Organisation\", \"'agile' Phasen\", \"'agile' Prinzipien\", \"'agile' Prozesse\", \"'agile' Softwareentwicklung\", \"'agile' Softwareweiterentwicklung\", \"'agile' Strukturen\", \"'agile' Techniken\", \"'agile' Transformation\", \"'agile' Transition\", \"'agile' Wände\", \"'agile' Weise\", \"'agile' Zeremonie\", \"'agile' Zertifizierung\", \"'agilen' Arbeits- und Managementmethoden\", \"'agiler' Ansatz\", \"'agiler' Coach\", \"'agiler' Wert\", \"'agiles' Arbeiten\", \"'agiles' Betriebsmodell\", \"'agiles' Coaching\", \"'agiles' Management\", \"'agiles' Manifest\", \"'agiles' Mindset\", \"'agiles' Programmieren\", \"'agiles' Projekt\", \"'agiles' Projektmanagement\", \"'agiles' Team\", \"'agiles' Training \", \"'agiles' Zertifikat\", \"Backlog\", \"Co-Design\", \"Daily Scrum\", \"Definition of Done\", \"Definition of Ready\", \"Design Thinking\", \"Empathy Map\", \"Entwicklungsteam\", \"Extremnutzer Nutzer\", \"fail fast\", \"fail forward\", \"hierarchiefrei\", \"hierarchiefreien Raum\", \"Innovationskultur\", \"Innovationsmanagement\", \"Iteration\", \"iterativ \", \"Kanban\", \"Kanbanboard\", \"Kollaborationsräume\", \"kollaboratives Design\", \"kurzzyklische Ergebnisse\", \"Lean Start up\", \"Lean Start-up\", \"Lean Startup\",  \"Minimum Viable Product\", \"MVP\", \"neue Arbeitsmodelle\", \"neue Arbeitsweisen\", \"neue Team- und Führungsstrukturen\", \"New Work\", \"Nutzeranforderungen\", \"Nutzerbedürfnisse\", \"Nutzerforschung\", \"nutzerorientiert\", \"nutzerorientierte Lösungen\", \"Nutzerstory\", \"Nutzertest\", \"nutzerzentriert\", \"nutzerzentriertes Design\", \"Planning Poker\", \"Product Backlog\", \"Product Owner\", \"Project Canvas\", \"Projektarbeiten\", \"Projektbasiert\", \"schlanke Softwareentwicklung\", \"Scrum\", \"Scrum Coach\", \"Scrum Master\", \"Scrum Team\", \"Service Design\", \"Softwareentwicklung\", \"Sprint\", \"Sprint Backlog\", \"Sprint Planning\", \"Sprint Retrospective\", \"Sprint Review\", \"Timebox\", \"Timeboxing\", \"visuelles Denken\", \"Whiteboard\", \"Wicked Problem\", \"Wireframe\", \"'agile' capability\", \"'agile' ceremony\", \"'agile' certificate\", \"'agile' certification\", \"'agile' coach\", \"'agile' coaching\", \"'agile' delivery\", \"'agile' development\", \"'agile' digital capability \", \"'agile' environment\", \"'agile' innovation\", \"'agile' management\", \"'agile' manifesto\", \"'agile' method\", \"'agile' methodology\", \"'agile' mindset\", \"'agile' operating model\", \"'agile' organisation\", \"'agile' organization unit\", \"'agile' principles\", \"'agile' processes\", \"'agile' programming\", \"'agile' project\", \"'agile' project management\", \"'agile' software development\", \"'agile' stages\", \"'agile' structures\", \"'agile' team\", \"'agile' techniques\", \"'agile' transformation\", \"'agile' transition\", \"'agile' Value\", \"'agile' walls\", \"'agile' way\", \"'agile' way of working\", \"'agile' work\", \"'agile' work practices\", \"'agile' working and management methods\", \"'agile' working culture\", \"'agile' working methods\", \"agile capability\", \"agile ceremony\", \"agile certificate\", \"agile certification\", \"agile coach\", \"agile coaching\", \"agile delivery\", \"agile development\", \"agile digital capability \", \"agile environment\", \"agile innovation\", \"agile management\", \"agile manifesto\", \"agile method\", \"agile methodology\", \"agile mindset\", \"agile operating model\", \"agile organisation\", \"agile organization unit\", \"agile principles\", \"agile processes\", \"agile programming\", \"agile project\", \"agile project management\", \"agile software development\", \"agile stages\", \"agile structures\", \"agile team\", \"agile techniques\", \"agile transformation\", \"agile transition\", \"agile Value\", \"agile walls\", \"agile way\", \"agile way of working\", \"agile work\", \"agile work practices\", \"agile working and management methods\", \"agile working culture\", \"agile working methods\", \"'agile' capability\", \"'agile' ceremony\", \"'agile' certificate\", \"'agile' certification\", \"'agile' coach\", \"'agile' coaching\", \"'agile' delivery\", \"'agile' development\", \"'agile' digital capability \", \"'agile' environment\", \"'agile' innovation\", \"'agile' management\", \"'agile' manifesto\", \"'agile' method\", \"'agile' methodology\", \"'agile' mindset\", \"'agile' operating model\", \"'agile' organisation\", \"'agile' organization unit\", \"'agile' principles\", \"'agile' processes\", \"'agile' programming\", \"'agile' project\", \"'agile' project management\", \"'agile' software development\", \"'agile' stages\", \"'agile' structures\", \"'agile' team\", \"'agile' techniques\", \"'agile' transformation\", \"'agile' transition\", \"'agile' Value\", \"'agile' walls\", \"'agile' way\", \"'agile' way of working\", \"'agile' work\", \"'agile' work practices\", \"'agile' working and management methods\", \"'agile' working culture\", \"'agile' working methods\", \"backlog\", \"co-design\", \"collaboration spaces\", \"collaborative design\", \"cross-departmental approach\", \"daily scrum\", \"definition of done\", \"definition of ready\", \"design thinking\", \"development team\", \"empathy map\", \"extreme user\", \"fail fast\", \"fail forward\", \"free of hierarchy\", \"hierarchy free space\", \"innovation culture\", \"innovation management\", \"iteration\", \"iterative\", \"Kanban\", \"kanban board\", \"lean software development\", \"lean start up\", \"lean start-up\", \"lean startup\", \"minimum viable product\", \"MVP\", \"new team and management structures\", \"new work\", \"new working methods\", \"new working models\", \"planning poker\", \"product backlog\", \"product owner\", \"project canvas\", \"project work\", \"project-based\", \"scrum\", \"scrum coach\", \"scrum master\", \"scrum team\", \"service design\", \"short-cycle results\", \"software development\", \"sprint\", \"sprint backlog\", \"sprint planning\", \"sprint retrospective\", \"sprint review\", \"time box\", \"timeboxing\", \"user needs\", \"user requirements\", \"user research\", \"user story\", \"user testing\", \"user-centred\", \"user-centred design\", \"user-friendly solutions\", \"user-oriented\", \"visual thinking\", \"whiteboard\", \"wicked problem\", \"wireframe\"]\n",
    "\n",
    "nlp_de = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "# lemmatizing\n",
    "searchterms_de_eng_lemma = spacy_list_lemmatizer(searchterms_de_eng, nlp_de)\n",
    "\n",
    "# making lowercase\n",
    "searchterms_de_eng_lemma = [each_string.lower() for each_string in searchterms_de_eng_lemma]\n",
    "\n",
    "# deleting duplicates\n",
    "searchterms_de_eng_lemma = list(set(searchterms_de_eng_lemma)) \n",
    "\n",
    "with open(\"searchterms_de_eng_lemma.pickle\", \"wb\") as file:\n",
    "    pickle.dump(searchterms_de_eng_lemma, file)\n",
    "\n",
    "with open(\"searchterms_eng_lemma.pickle\", \"wb\") as file:\n",
    "    pickle.dump(searchterms_eng_lemma, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['\" agil \" organisation',\n 'agil prinzip',\n 'agil training',\n \"' agil ' innovation\",\n \"' agil ' technik\",\n '\" agil \" winden',\n 'backlog',\n \"' agil ' projektmanagement\",\n '\" agil \" entwicklung',\n 'agil structures',\n 'collaborative design',\n \"' agil ' programmieren\",\n 'agil wert',\n \"' agil ' arbeits- und managementmethoden\",\n 'mvp',\n '\" agil \" programmieren',\n '\" agil \" zeremonie',\n \"' agil ' projekt\",\n 'daily scrum',\n \"' agil ' entwicklung\",\n 'sprint retrospective',\n 'agil environment',\n \"' agil ' arbeitsformen\",\n \"' agil ' arbeitsweise\",\n 'agil digital capability',\n 'agil weise',\n '\" agil \" phase',\n \"' agil ' arbeitspraktiken\",\n 'agil bereitstellung',\n '\" agil \" ansatz',\n 'agil value',\n \"' agil ' project management\",\n 'agil methodology',\n 'agil ansatz',\n 'kanban',\n 'agil way of working',\n \"' agil ' fähigkeit\",\n 'agil arbeitend organisationseinheit',\n '\" agil \" arbeitend organisationseinheit',\n 'visuell denken',\n \"' agil ' winden\",\n 'user needs',\n \"' agil ' stages\",\n \"' agil ' walls\",\n 'agil arbeitspraktiken',\n 'kanbanboard',\n '\" agil \" arbeitsweise',\n '\" agil \" arbeitskultur',\n \"' agil ' digitale fähigkeit\",\n 'hierarchy free space',\n \"' agil ' ceremony\",\n 'agil working methods',\n 'agil development',\n \"' agil ' work\",\n 'agil prozeß',\n 'nutzerbedürfnisse',\n 'nutzerzentriertes design',\n \"' agil ' techniques\",\n 'agil techniques',\n \"' agil ' arbeitsumgebung\",\n 'agil delivery',\n 'definition of ready',\n 'extreme user',\n 'sprint planning',\n 'iterativ',\n '\" agil \" projektmanagement',\n \"' agil ' project\",\n '\" agil \" prinzip',\n 'agil coaching',\n 'agil project management',\n 'user testing',\n 'agil arbeits- und managementmethoden',\n '\" agil \" prozeß',\n 'agil entwicklung',\n 'agil team',\n 'agil certificate',\n 'free of hierarchy',\n 'user-oriented',\n 'scrum',\n \"' agil ' arbeit\",\n 'agil stages',\n 'short-cycle results',\n '\" agil \" methode',\n '\" agil \" innovation',\n 'agil zertifizierung',\n \"' agil ' methodik\",\n \"' agil ' betriebsmodell\",\n \"' agil ' principles\",\n \"' agil ' methodology\",\n '\" agil \" struktur',\n \"' agil ' mindset\",\n 'user research',\n \"' agil ' programming\",\n 'entwicklungsteam',\n \"' agil ' methode\",\n '\" agil \" bereitstellung',\n 'agile transition',\n 'agil working culture',\n 'nutzerorientierte lösung',\n 'hierarchiefreien raum',\n 'agil winden',\n 'agil methodik',\n 'nutzerorientiert',\n 'scrum team',\n 'agil arbeitskultur',\n \"' agil ' prozeß\",\n 'agil zeremonie',\n 'agil method',\n \"' agil ' arbeitend team\",\n 'agil work',\n 'service design',\n \"' agil ' working culture\",\n 'agil softwareentwicklung',\n \"' agil ' manifesto\",\n '\" agil \" methodik',\n 'agil arbeit',\n 'iteration',\n 'new working models',\n \"' agil ' working methods\",\n \"' agil ' way of working\",\n \"' agil ' zeremonie\",\n 'user-friendly solutions',\n \"' agil ' bereitstellung\",\n \"' agil ' organisation\",\n \"' agil ' coach\",\n 'lean startup',\n '\" agil \" zertifikat',\n \"' agil ' environment\",\n \"' agil ' team\",\n 'projektarbeiten',\n 'schlank softwareentwicklung',\n 'cross-departmental approach',\n 'new work',\n \"' agil ' ansatz\",\n 'minimum viable product',\n 'agil digitale fähigkeit',\n \"' agil ' prinzip\",\n \"' agil ' arbeitskultur\",\n 'innovationsmanagement',\n '\" agil \" fähigkeit',\n \"' agil ' development\",\n '\" agil \" weise',\n 'agil phase',\n 'agil organisation',\n 'agil processes',\n 'agil zertifikat',\n \"' agil ' zertifikat\",\n '\" agil \" team',\n 'lean start-up',\n 'planning poker',\n 'product backlog',\n 'agil work practices',\n \"' agil ' way\",\n '\" agil \" arbeits- und managementmethoden',\n 'kanban board',\n 'user-centred design',\n 'definition of done',\n 'agil coach',\n 'agil methode',\n 'product owner',\n \"' agil ' work practices\",\n 'agil principles',\n '\" agil \" training',\n 'iterative',\n \"' agil ' certification\",\n '\" agil \" manifest',\n \"' agil ' management\",\n \"' agil ' training\",\n '\" agil \" projekt',\n 'agil technik',\n 'timeboxing',\n '\" agil \" softwareweiterentwicklung',\n 'agil arbeitsformen',\n '\" agil \" arbeit',\n 'timebox',\n 'agil transition',\n 'lean start up',\n '\" agil \" arbeitsformen',\n 'sprint',\n 'innovation management',\n \"' agil ' manifest\",\n 'new team and management structures',\n 'abteilungsübergreifender ansatz',\n '\" agil \" zertifizierung',\n 'agiles manifest',\n \"' agil ' phase\",\n 'user story',\n '\" agil \" mindset',\n 'development team',\n 'visual thinking',\n \"' agil ' coaching\",\n 'agil programming',\n '\" agil \" coach',\n '\" agil \" betriebsmodell',\n \"' agil ' softwareweiterentwicklung\",\n 'hierarchiefrei',\n 'wicked problem',\n 'fail forward',\n 'whiteboard',\n \"' agil ' value\",\n 'neue arbeitsmodelle',\n '\" agil \" management',\n \"' agil ' processes\",\n \"' agil ' organization unit\",\n \"' agil ' weise\",\n 'agil fähigkeit',\n 'project canvas',\n \"' agil ' arbeitend organisationseinheit\",\n 'neue team- und führungsstrukturen',\n 'new working methods',\n '\" agil \" technik',\n \"' agil ' digital capability\",\n 'innovationskultur',\n 'empathy map',\n \"' agil ' method\",\n 'user requirements',\n 'extremnutzer nutzer',\n 'agil betriebsmodell',\n '\" agil \" arbeitsumgebung',\n 'kollaboratives design',\n 'user-centred',\n 'nutzerforschung',\n '\" agil \" arbeitspraktiken',\n 'kollaborationsräume',\n 'softwareentwicklung',\n '\" agil \" coaching',\n 'time box',\n 'agil operating model',\n '\" agil \" arbeitend team',\n \"' agil ' operating model\",\n \"' agil ' working and management methods\",\n 'agil transformation',\n '\" agil \" transition',\n 'collaboration spaces',\n 'agiles projektmanagement',\n 'agil innovation',\n 'agil softwareweiterentwicklung',\n 'sprint backlog',\n 'agil certification',\n 'innovation culture',\n \"' agil ' transition\",\n 'scrum coach',\n 'agil capability',\n 'software development',\n \"' agil ' softwareentwicklung\",\n 'co-design',\n 'agil projekt',\n 'sprint review',\n 'agil organization unit',\n 'agil ceremony',\n \"' agil ' struktur\",\n 'agil arbeitend team',\n 'agil arbeitsweise',\n 'agil arbeitsmethoden',\n \"' agil ' software development\",\n \"' agil ' wert\",\n 'agil management',\n 'agil project',\n '\" agil \" digitale fähigkeit',\n 'wireframe',\n '\" agil \" transformation',\n 'neue arbeitsweisen',\n 'projektbasiert',\n \"' agil ' certificate\",\n \"' agil ' delivery\",\n 'agil walls',\n '\" agil \" softwareentwicklung',\n 'agil programmieren',\n 'design thinking',\n 'agil working and management methods',\n 'nutzeranforderungen',\n 'fail fast',\n 'scrum master',\n \"' agil ' arbeitsmethoden\",\n 'nutzerstory',\n \"' agil ' transformation\",\n 'agil manifesto',\n 'nutzerzentriert',\n 'agil way',\n 'lean software development',\n 'nutzertest',\n 'project-based',\n '\" agil \" wert',\n 'agil arbeitsumgebung',\n '\" agil \" arbeitsmethoden',\n 'agil struktur',\n \"' agil ' zertifizierung\",\n 'kurzzyklische ergebnis',\n 'project work',\n 'agil mindset',\n \"' agil ' capability\",\n \"' agil ' structures\",\n 'agil software development']"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "searchterms_de_eng_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the data and selecting only relevant pages based on the co-searchterms dictionary\n",
    "\n",
    "inputFileName = \"/Users/mxm/Google Drive/Masterstudium/Inhalte/Master Thesis/GitHubRepo/agile-in-government/Analysis/2_Data_Preprocessing/DATA/CSVs/UK/General.csv\"\n",
    "outputFileName = \"/Users/mxm/Google Drive/Masterstudium/Inhalte/Master Thesis/GitHubRepo/agile-in-government/Analysis/2_Data_Preprocessing/DATA/CSVs/UK/General_cleaned.csv\"\n",
    "\n",
    "# loading the spacy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# switching off irrelevant spacy functions\n",
    "nlp.disable_pipes('tagger', 'ner')\n",
    "\n",
    "# defining the keywords search function\n",
    "def searchfunction(row):\n",
    "    matchstring = \"\"\n",
    "    count = 0\n",
    "    for term in searchterms_eng_lemma:\n",
    "        if term.lower() in row['doctext_lemma']:\n",
    "            count += 1\n",
    "            matchstring = matchstring + term.lower() + \", \"\n",
    "    row['co_searchterms_nr'] = count\n",
    "    row['co_searchterms_matches'] = matchstring\n",
    "    return row\n",
    "\n",
    "\n",
    "# opening and cleaning / preprocessing dataframe\n",
    "with open(inputFileName, newline='') as inFile, open(outputFileName, 'w', newline='') as outFile:\n",
    "    # reading the csv\n",
    "    df = pd.read_csv(inFile)\n",
    "    # sorting the values by final date\n",
    "    df.sort_values(by=['final_date'])\n",
    "    # dropping all sites which didn't yield any match for the agile term\n",
    "    df.dropna(axis=0, how='any', thresh=None, subset=(['agile_term']), inplace=True)\n",
    "    # dropping potential duplicates based on the heading and the context of the first agile term mention\n",
    "    df.drop_duplicates(subset=(['heading', 'agile_context_pre', 'agile_context_post']), inplace=True)\n",
    "    # deleting sites with csv previews\n",
    "    df = df[~df.url.str.endswith(\"csv/preview\")]\n",
    "    # deleting search sites\n",
    "    df = df[~df.url.str.contains('/search/')]\n",
    "    # deleting agile metioned in a tweet that occurs on multiple sites \n",
    "    df['doctext'] = df['doctext'].map(lambda x: x.strip('#Agile Development lessons learned from Gov.uk'))\n",
    "    #deleting cases where no date has been found (htmldate attributes 2020-01-01 for no dates)\n",
    "    df = df[df['final_date'] != '2020-01-01 00:00:00']\n",
    "    #deleting all cases from 2020 because the analysis focuses on everything up until 2019\n",
    "    df = df[df['final_date'] < '2020-01-01 00:00:00']\n",
    "    # changing the final_date column to a pandas datetime object\n",
    "    df['final_date'] = pd.to_datetime(df['final_date'])\n",
    "\n",
    "    # lemmatizing the text\n",
    "    lemma = []\n",
    "    for doc in nlp.pipe(df['doctext'].astype('unicode').values, batch_size=50, n_threads=3):\n",
    "        if doc.is_parsed:\n",
    "            lemma.append([n.lemma_ for n in doc])\n",
    "        else:\n",
    "            lemma.append(None)\n",
    "\n",
    "    # storing the lemmas in a new column \n",
    "    df['doctext_lemma'] = lemma\n",
    "    \n",
    "    #making lemmas lowercase\n",
    "    df['doctext_lemma'] = df['doctext_lemma'].map(lambda lemmas: [x.lower() for x in lemmas])\n",
    "    \n",
    "    #joining list of lemmas into one string agein, so it becomes matchable by the searchfunction\n",
    "    df['doctext_lemma'] = df['doctext_lemma'].map(lambda lemmas: \" \".join(lemmas))\n",
    " \n",
    "    #applying the keyword search function \n",
    "    df = df.apply(searchfunction, axis = 1)\n",
    "\n",
    "    # delete everything that didn't yield a co-searchterm match\n",
    "    df = df[df['co_searchterms_nr'] != 0 ]\n",
    "    df.to_csv(outFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Organisation Names based on domain\n",
    "\n",
    "# specifying input and output file and paths\n",
    "inputFileName = \"/Users/mxm/Google Drive/Masterstudium/Inhalte/Master Thesis/GitHubRepo/agile-in-government/Analysis/2_Data_Preprocessing/DATA/CSVs/UK/General_cleaned.csv\"\n",
    "outputFileName = \"/Users/mxm/Google Drive/Masterstudium/Inhalte/Master Thesis/GitHubRepo/agile-in-government/Analysis/2_Data_Preprocessing/DATA/CSVs/UK/General_cleaned_org.csv\"\n",
    "\n",
    "# file with all the uk subdomains (manual eddited to include subsubdomains)\n",
    "domains_dict = pd.read_csv(\"/Users/mxm/Google Drive/Masterstudium/Inhalte/Master Thesis/GitHubRepo/agile-in-government/Analysis/2_Data_Preprocessing/DATA/CSVs/UK/List_of_gov.uk_domain_names_as_at_28_Oct_2019_manualEdit.csv\")\n",
    "\n",
    "# compiling regex to extract domains from url (also in the case of a site on national archives)\n",
    "domain_regex = re.compile(r'http(s?)://(?!.*http)(www\\.)?([a-zA-Z0-9\\-.]+)', re.IGNORECASE | re.UNICODE)\n",
    "\n",
    "# defining the url extraction function\n",
    "def extract(url):\n",
    "    match = domain_regex.search(url)\n",
    "    if match:\n",
    "        match_domain = match.group(3)\n",
    "        return '.'.join(match_domain.split('.')[-4:])\n",
    "    else:\n",
    "        print(url)\n",
    "        return ''\n",
    "\n",
    "# adding the domain / organisation names to the file\n",
    "with open(inputFileName, newline='') as inFile, open(outputFileName, 'w', newline='') as outFile:\n",
    "    df = pd.read_csv(inFile)\n",
    "    # getting the subdomain from \n",
    "    df['sub_domain'] = df.apply(lambda row: extract(row['url']), axis=1)\n",
    "    # merging the file with the dataframe that has all the subdomains and organisation names\n",
    "    df2 = pd.merge(df, domains_dict, left_on = 'sub_domain', right_on='Domain: Domain Name', how='left')\n",
    "    # saving the file\n",
    "    df2.to_csv(outFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### GERMANY ############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the function to read in all the html files, clean the text, get further publishing dates and select the earliest one, and store everything into csv\n",
    "def process_state_DE(state):\n",
    "\n",
    "    # compiling the regex search terms\n",
    "    agile_regex = re.compile(r'\\bagility\\b|\\bagil\\w{0,2}\\b|\\bagilität\\b', re.IGNORECASE | re.UNICODE) # for agil keyword\n",
    "    agile_context_regex = re.compile(r'\\s*([^\\s]+?)\\s+([^\\s]+?)\\s+([^\\s]+?)\\s+([^\\s]+?)\\s+agil.*?\\s+([^\\s]+)\\s+([^\\s]+?)\\s+([^\\s]+?)\\s+([^\\s]+?)\\s+', re.IGNORECASE | re.UNICODE) # to get the context\n",
    "    digital_regex = re.compile(r'\\bdigital\\w{0,2}\\b|\\bdigitalisierung\\b|\\bdigitale transformation\\b', re.IGNORECASE | re.UNICODE) # to get everything related digitalisation\n",
    "\n",
    "    # getting the csv which was produced by scrapy as input file\n",
    "    csv_main = csv.DictReader(open(\"/Users/mxm/Google Drive/Masterstudium/Inhalte/Master Thesis/GitHubRepo/agile-in-government/Analysis/1_Data_Collection/DATA/CSVs/Germany/agile_sites_output_Germany_{}.csv\".format(state)), fieldnames=[\"id\", 'url', 'domain', 'date1', 'date2', 'date3', 'heading'])\n",
    "\n",
    "    # creating an empty array to write into in the processing loop\n",
    "    array = {}\n",
    "\n",
    "    # getting every line of the input csv into the array\n",
    "    for line in csv_main: \n",
    "        array[line[\"id\"]] = line\n",
    "    \n",
    "    # specfifying the the directory where the text files should be stored\n",
    "    text_dir = \"/Users/mxm/Google Drive/Masterstudium/Inhalte/Master Thesis/GitHubRepo/agile-in-government/Analysis/2_Data_Preprocessing/DATA/TextFiles/Germany/{}\".format(state)\n",
    "    \n",
    "    # making the directory if it doesn't exist\n",
    "    pathlib.Path(text_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # specifying variables to keep track of the progress while processing\n",
    "    total_count = len(array)\n",
    "    processed = 0\n",
    "\n",
    "    # this is the main working loop\n",
    "    for id, line in array.items():   \n",
    "        # print function to keep track of which files are being processed     \n",
    "        print(\"processing {}\".format(id))\n",
    "        # getting the id from the array\n",
    "        id = line[\"id\"]\n",
    "        # getting the domain from the array\n",
    "        domain = line[\"domain\"]\n",
    "        # getting the url from the array\n",
    "        url = line[\"url\"]\n",
    "\n",
    "        #excluding hits from berlin jobs which are a lot and are de facto a germany wide job portal, which is of no relevance for this research \n",
    "        if domain == 'jobs.berlin.de':\n",
    "            continue\n",
    "            \n",
    "        # specifying the path where the html to process is located\n",
    "        path = '/Users/mxm/Google Drive/Masterstudium/Inhalte/Master Thesis/GitHubRepo/agile-in-government/Analysis/1_Data_Collection/DATA/HTMLs/Germany/{}/{}/{}.html'.format(state, domain, id)\n",
    "        # opening the html file while catching an encoding error     \n",
    "        try:  \n",
    "            soup = BeautifulSoup(open(path), \"html.parser\")\n",
    "        except UnicodeDecodeError:\n",
    "            soup = BeautifulSoup(open(path, encoding='windows-1252'), \"html.parser\")    \n",
    "        \n",
    "        ### In the following further potential publication dates are parsed and the oldest one is select as \"final date\" ###\n",
    "\n",
    "        # Getting date4\n",
    "        date4_element = soup.select_one(\"span.date\")\n",
    "        date4 = \"\"\n",
    "        if date4_element is not None:\n",
    "            date4 = date4_element.get_text()\n",
    "        \n",
    "        # Getting date6\n",
    "        date6_element = soup.select_one(\"h1#page-title + p\")\n",
    "        date6 = \"\"\n",
    "        if date6_element is not None:\n",
    "            date6 = date6_element.get_text()\n",
    "\n",
    "       # Getting date5\n",
    "        htmlparser = etree.HTMLParser()    \n",
    "        try:\n",
    "            tree = etree.parse(open(path), htmlparser)\n",
    "        except UnicodeDecodeError:\n",
    "            tree = etree.parse(open(path, encoding='windows-1252'), htmlparser)\n",
    "        \n",
    "        date5 = tree.xpath(\"substring(substring-after(/html//script[@type='application/ld+json']/text(), 'datePublished'), 4, 23)\")\n",
    "\n",
    "        # a special cases for date 5\n",
    "        date5_1 = tree.xpath(\"substring(substring-after(/html//script[@type='application/ld+json']/text(), 'datePublished'), 4, 25)\")\n",
    "\n",
    "\n",
    "        # Getting date7 from National Archives --> the date the site was archived on --> the date the site was released on would have been even earlier\n",
    "        date7 = tree.xpath(\"substring(substring-after(/html/head/script/text(), 'timestamp'),9,14)\")\n",
    "\n",
    "  \n",
    "        # different parsing for for htmldate\n",
    "        try:\n",
    "            html_tree = html.parse(open(path))\n",
    "        except UnicodeDecodeError:\n",
    "            html_tree = html.parse(open(path, encoding='windows-1252'))\n",
    "            \n",
    "\n",
    "        # Getting date with the htmldate package\n",
    "        htmldate = find_date(html_tree)\n",
    "\n",
    "\n",
    "        # since html date gives out nonetype when it doesn't find anything, it has to be respecified to empty string so that it works with dateparser later on\n",
    "        if htmldate is None:\n",
    "            htmldate = ''\n",
    "\n",
    "        #htmldate = htmldate.astype('str')\n",
    "\n",
    "\n",
    "\n",
    "        # assinging the already succesfully parsed dates during crawling stage\n",
    "        date1 = line[\"date1\"]\n",
    "        date2 = line[\"date2\"]\n",
    "\n",
    "\n",
    "        # Storing the oldest date as final date variable in python date format\n",
    "        date_vars = [date1, date2, date4, date5, date5_1, date6, date7, htmldate]\n",
    "        \n",
    "        final_date = None \n",
    "\n",
    "        for date_var in date_vars:\n",
    "            try:\n",
    "                if final_date is None:\n",
    "                    final_date = parse(date_var, ignoretz = True)\n",
    "                elif parse(date_var, ignoretz=True) < final_date:\n",
    "                    final_date = parse(date_var, ignoretz=True)\n",
    "            except dateutil.parser._parser.ParserError:\n",
    "                pass \n",
    "\n",
    "\n",
    "\n",
    "        #deleting all non-body text content from the html\n",
    "        \n",
    "        # defining the html elements to delete\n",
    "        for script in soup(['script', 'style', 'meta', 'a', 'head', 'footer', 'navbar', 'header', 'search-box', 'global-cookie-message', 'id=\"global-cookie-message\"', 'global-bar', 'menu', 'noscript', 'global-cookie-message', 'search']):\n",
    "            script.decompose()    # deleting those elements\n",
    "            \n",
    "        # geting text\n",
    "        doctext = soup.get_text()\n",
    "\n",
    "        # break into lines and remove leading and trailing space on each\n",
    "        lines = (line.strip() for line in doctext.splitlines())\n",
    "        # break multi-headlines into a line each\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        # drop blank lines\n",
    "        doctext = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "        \n",
    "  \n",
    "\n",
    "        # saving the cleaned text into the directory\n",
    "        with open(\"{}/{}.txt\".format(text_dir, id), \"w\") as textfile:\n",
    "            textfile.write(doctext)\n",
    "        \n",
    "\n",
    " \n",
    "        #### In the following the matches for the search terms (agile, digital) are gathered ####\n",
    "        \n",
    "        # finding the matches for agile and agility\n",
    "        agile_term = []\n",
    "        agile_term = agile_regex.findall(doctext.lower())\n",
    "    \n",
    "\n",
    "        # finding the matches for digital\n",
    "        digital_term = []\n",
    "        digital_term = digital_regex.findall(doctext.lower())\n",
    "\n",
    "\n",
    "        # finding the context for agile\n",
    "        agile_context = []\n",
    "        agile_context = agile_context_regex.search(doctext)\n",
    "\n",
    "        agile_context_pre = \"\"\n",
    "        agile_context_post = \"\"\n",
    "        \n",
    "        if agile_context is not None:\n",
    "            agile_context_pre = \" \".join(agile_context.group(1,2,3,4))\n",
    "            agile_context_post = \" \".join(agile_context.group(5,6,7,8))\n",
    "        \n",
    "\n",
    "        # assigning all generated variables as line items to the array\n",
    "\n",
    "        if len(agile_term) == 0:\n",
    "            line[\"agile_term\"] = \"\" \n",
    "        else:\n",
    "            line[\"agile_term\"] = \",\".join(agile_term)\n",
    "\n",
    "        if len(digital_term) == 0:\n",
    "            line[\"digital_term\"] = \"\" \n",
    "        else:\n",
    "            line[\"digital_term\"] = \",\".join(digital_term)\n",
    "\n",
    "\n",
    "        line[\"agile_context_pre\"] = agile_context_pre\n",
    "        line[\"agile_context_post\"] = agile_context_post \n",
    "        line[\"date4\"] = date4\n",
    "        line[\"date5\"] = date5\n",
    "        line[\"date5_1\"] = date5_1\n",
    "        line[\"date6\"] = date6\n",
    "        line[\"date7\"] = date7\n",
    "        line[\"final_date\"] = final_date\n",
    "        line[\"htmldate\"] = htmldate\n",
    "        line[\"country\"] = \"Germany\"\n",
    "        line[\"level\"] = state\n",
    "        line[\"text_file_loc\"] = \"{}/{}.txt\".format(text_dir, id)\n",
    "        line['doctext'] = doctext\n",
    "\n",
    "        \n",
    "        # Printing processing status\n",
    "        processed += 1\n",
    "        print(\"processed: {}%\".format((processed/total_count)*100))\n",
    "        print(\"Current Time: \", datetime.now())\n",
    "\n",
    "    # opening the output csv\n",
    "    outputfile = open(\"/Users/mxm/Google Drive/Masterstudium/Inhalte/Master Thesis/GitHubRepo/agile-in-government/Analysis/2_Data_Preprocessing/DATA/CSVs/Germany/{}.csv\".format(state), \"w\")\n",
    "    \n",
    "    # writing all the array lines into the csv file\n",
    "    writer = csv.DictWriter(outputfile, fieldnames=[\"id\", \"country\", \"level\", 'url', 'domain', 'date1', 'date2', 'date3', 'date4', 'date5', 'date5_1', 'date6', 'date7', 'htmldate', 'final_date', 'heading', 'agile_term', 'agile_context_pre', 'agile_context_post', 'digital_term', 'text_file_loc', 'doctext',])\n",
    "    writer.writeheader()\n",
    "    for id, line in array.items():\n",
    "        writer.writerow(line)\n",
    "    outputfile.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nstate_list = [\"Federal\", \"Schleswig-Holstein\",\"Bremen\",\"Hamburg\",\"Lower Saxony\",\"Thuringia\",\"Saxony-Anhalt\",\"Saxony\",\"Mecklenburg-Vorpommern\",\"Berlin\",\"Brandenburg\",\"North Rhine-Westphalia\",\"Rhineland-Palatinate\",\"Saarland\", \"Hesse\",\"Bavaria\",\"Baden-Wuerttemberg\"]\\n\\nfor state in state_list:\\n    print(\\'processing \\', state)\\n    process_state_DE(state)\\n'"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#######Processing Statewise Germany\n",
    "\"\"\"\n",
    "state_list = [\"Federal\", \"Schleswig-Holstein\",\"Bremen\",\"Hamburg\",\"Lower Saxony\",\"Thuringia\",\"Saxony-Anhalt\",\"Saxony\",\"Mecklenburg-Vorpommern\",\"Berlin\",\"Brandenburg\",\"North Rhine-Westphalia\",\"Rhineland-Palatinate\",\"Saarland\", \"Hesse\",\"Bavaria\",\"Baden-Wuerttemberg\"]\n",
    "\n",
    "for state in state_list:\n",
    "    print('processing ', state)\n",
    "    process_state_DE(state)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####Merging all German States\n",
    "state_list = [\"Federal\", \"Schleswig-Holstein\",\"Bremen\",\"Hamburg\",\"Lower Saxony\",\"Thuringia\",\"Saxony-Anhalt\",\"Saxony\",\"Mecklenburg-Vorpommern\",\"Berlin\",\"Brandenburg\",\"North Rhine-Westphalia\",\"Rhineland-Palatinate\",\"Saarland\" ,\"Hesse\" ,\"Bavaria\",\"Baden-Wuerttemberg\"]\n",
    "\n",
    "state_df = {}\n",
    "\n",
    "def reading_csv(state):\n",
    "    inputFileName = \"/Users/mxm/Google Drive/Masterstudium/Inhalte/Master Thesis/GitHubRepo/agile-in-government/Analysis/2_Data_Preprocessing/DATA/CSVs/Germany/{}.csv\".format(state)\n",
    "    outputFileName = \"/Users/mxm/Google Drive/Masterstudium/Inhalte/Master Thesis/GitHubRepo/agile-in-government/Analysis/2_Data_Preprocessing/DATA/CSVs/Germany/Combined.csv\"\n",
    "    with open(inputFileName, newline='') as inFile, open(outputFileName, 'w', newline='') as outFile:\n",
    "        df = pd.read_csv(inFile)\n",
    "    return df\n",
    "\n",
    "combined_dataframe_Germany = None \n",
    "\n",
    "for state in state_list:\n",
    "    temp_df = reading_csv(state)\n",
    "    if combined_dataframe_Germany is None:\n",
    "       combined_dataframe_Germany = temp_df\n",
    "    else: \n",
    "        combined_dataframe_Germany = pd.concat([combined_dataframe_Germany,temp_df])\n",
    "\n",
    "    combined_dataframe_Germany.to_csv(\"/Users/mxm/Google Drive/Masterstudium/Inhalte/Master Thesis/GitHubRepo/agile-in-government/Analysis/2_Data_Preprocessing/DATA/CSVs/Germany/Combined.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the data and selecting only relevant pages based on the co-searchterms dictionary\n",
    "\n",
    "inputFileName = \"/Users/mxm/Google Drive/Masterstudium/Inhalte/Master Thesis/GitHubRepo/agile-in-government/Analysis/2_Data_Preprocessing/DATA/CSVs/Germany/Combined.csv\"\n",
    "outputFileName = \"/Users/mxm/Google Drive/Masterstudium/Inhalte/Master Thesis/GitHubRepo/agile-in-government/Analysis/2_Data_Preprocessing/DATA/CSVs/Germany/Combined_cleaned.csv\"\n",
    "\n",
    "# loading the spacy model\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "# switching off irrelevant spacy functions\n",
    "nlp.disable_pipes('tagger', 'ner')\n",
    "\n",
    "# defining the keywords search function\n",
    "def searchfunction(row):\n",
    "    matchstring = \"\"\n",
    "    count = 0\n",
    "    for term in searchterms_de_eng_lemma:\n",
    "        if term.lower() in row['doctext_lemma']:\n",
    "            count += 1\n",
    "            matchstring = matchstring + term.lower() + \", \"\n",
    "    row['co_searchterms_nr'] = count\n",
    "    row['co_searchterms_matches'] = matchstring\n",
    "    return row\n",
    "\n",
    "\n",
    "# opening and cleaning / preprocessing dataframe\n",
    "with open(inputFileName, newline='') as inFile, open(outputFileName, 'w', newline='') as outFile:\n",
    "    # reading the csv\n",
    "    df = pd.read_csv(inFile)\n",
    "    # sorting the values by final date\n",
    "    df.sort_values(by=['final_date'])\n",
    "    # dropping all sites which didn't yield any match for the agile term\n",
    "    df.dropna(axis=0, how='any', thresh=None, subset=(['agile_term']), inplace=True)\n",
    "    # dropping potential duplicates based on the heading and the context of the first agile term mention\n",
    "    df.drop_duplicates(subset=(['heading', 'agile_context_pre', 'agile_context_post']), inplace=True)\n",
    "    # changing htmldate to string for later processing\n",
    "    df['htmldate'] = df['htmldate'].astype(str)\n",
    "    # deleting sites with csv previews\n",
    "    df = df[~df.url.str.endswith(\"csv/preview\")]\n",
    "    # deleting job search site entries\n",
    "    df = df[~df.domain.str.contains(\"www.yojo.de\")]\n",
    "    # deleting search sites\n",
    "    df = df[~df.url.str.contains('/search/')] \n",
    "    # deleting search results for sachsen\n",
    "    df = df[~df.domain.str.contains('search.sachsen.de')] \n",
    "    # deleting terms related to agilis which is a lizard\n",
    "    df = df[~df.agile_term.str.contains('agilis')]\n",
    "    # deleting ticket counts\n",
    "    df = df[~df.url.str.contains('/tickets/')] \n",
    "    # deleting content where date has been wrongly classified and true date is none-attainable\n",
    "    df = df[~df.htmldate.str.contains('1999-01-01')] \n",
    "    # deleting non-german content\n",
    "    df = df[~df.url.str.contains('/FR/')] # deleting non-german content\n",
    "    df = df[~df.url.str.contains('/fr/')] # deleting non-german content\n",
    "    df = df[~df.url.str.contains( '/EN/'  )] # deleting non-german content\n",
    "    df = df[~df.url.str.contains('/en/' )] # deleting non-german content\n",
    "    df = df[~df.url.str.contains('/breg-en/' )] # deleting non-german content\n",
    "    df = df[~df.url.str.contains('/breg-fr/' )] # deleting non-german content\n",
    "    # deleting agile metioned in a tweet that occurs on multiple sites \n",
    "    df['doctext'] = df['doctext'].map(lambda x: x.strip('#Agile Development lessons learned from Gov.uk'))\n",
    "    #deleting cases where no date has been found (htmldate attributes 2020-01-01 for no dates)\n",
    "    df = df[df['final_date'] != '2020-01-01 00:00:00']\n",
    "    #deleting all cases from 2020 because the analysis focuses on everything up until 2019\n",
    "    df = df[df['final_date'] < '2020-01-01 00:00:00']\n",
    "    # changing the final_date column to a pandas datetime object\n",
    "    df['final_date'] = pd.to_datetime(df['final_date'])\n",
    "\n",
    "    # lemmatizing the text\n",
    "    lemma = []\n",
    "    for doc in nlp.pipe(df['doctext'].astype('unicode').values, batch_size=50, n_threads=3):\n",
    "        if doc.is_parsed:\n",
    "            lemma.append([n.lemma_ for n in doc])\n",
    "        else:\n",
    "            lemma.append(None)\n",
    "\n",
    "    # storing the lemmas in a new column \n",
    "    df['doctext_lemma'] = lemma\n",
    "    \n",
    "    #making lemmas lowercase\n",
    "    df['doctext_lemma'] = df['doctext_lemma'].map(lambda lemmas: [x.lower() for x in lemmas])\n",
    "    \n",
    "    #joining list of lemmas into one string agein, so it becomes matchable by the searchfunction\n",
    "    df['doctext_lemma'] = df['doctext_lemma'].map(lambda lemmas: \" \".join(lemmas))\n",
    " \n",
    "    #applying the keyword search function \n",
    "    df = df.apply(searchfunction, axis = 1)\n",
    "\n",
    "    # delete everything that didn't yield a co-searchterm match\n",
    "    df = df[df['co_searchterms_nr'] != 0 ]\n",
    "    df.to_csv(outFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "https://de.m.wikipedia.org/w/index.php?title=PRINCE2\nhttps://de.m.wikipedia.org/w/index.php?title=PRINCE2\n"
    }
   ],
   "source": [
    "# Adding subdomain Names based on domain\n",
    "\n",
    "# specifying input and output file and paths\n",
    "inputFileName = \"/Users/mxm/Google Drive/Masterstudium/Inhalte/Master Thesis/GitHubRepo/agile-in-government/Analysis/2_Data_Preprocessing/DATA/CSVs/Germany/Combined_cleaned.csv\"\n",
    "outputFileName = \"/Users/mxm/Google Drive/Masterstudium/Inhalte/Master Thesis/GitHubRepo/agile-in-government/Analysis/2_Data_Preprocessing/DATA/CSVs/Germany/Combined_cleaned_subdomains.csv\"\n",
    "\n",
    "\n",
    "# compiling regex to extract domains from url (also in the case of a site on national archives)\n",
    "domain_regex = re.compile(r'http(s?)://(?!.*http)(www\\.)?([a-zA-Z0-9\\-.]+)', re.IGNORECASE | re.UNICODE)\n",
    "alternate_url_regex = re.compile(r'\\.de/([\\w%-]+)(/)?([\\w%-]+)?(/)?', re.IGNORECASE | re.UNICODE)\n",
    "#alternate_sub_regex = re.compile()\n",
    "\n",
    "# defining the url extraction function\n",
    "def extract1(url):\n",
    "    match = domain_regex.search(url)\n",
    "    if match:\n",
    "        match_domain = match.group(3)\n",
    "        return '.'.join(match_domain.split('.')[-4:])\n",
    "    else:\n",
    "        print(url)\n",
    "        return ''\n",
    "#defining second extraction function\n",
    "def extract2(url):\n",
    "    match = alternate_url_regex.search(url)\n",
    "    if match:\n",
    "        match= match.group(1)\n",
    "        return match\n",
    "    else:\n",
    "        print(url)\n",
    "        return \"\"\n",
    "#defining third extraction function\n",
    "def extract3(url):\n",
    "    match = alternate_url_regex.search(url)\n",
    "    if match:\n",
    "        match = match.group(3)\n",
    "        return match\n",
    "    else:\n",
    "        print(url)\n",
    "        return \"\"\n",
    "# defining fourth url extraction function\n",
    "def extract4(url):\n",
    "    match = domain_regex.search(url)\n",
    "    if match:\n",
    "        match_domain = match.group(3)\n",
    "        return match_domain.split('.')[0]\n",
    "    else:\n",
    "        print(url)\n",
    "        return ''\n",
    "\n",
    "# adding the domain / organisation names to the file\n",
    "with open(inputFileName, newline='') as inFile, open(outputFileName, 'w', newline='') as outFile:\n",
    "    df = pd.read_csv(inFile)\n",
    "    # getting the subdomain from \n",
    "    df['sub_domain'] = df.apply(lambda row: extract1(row['url']), axis=1)\n",
    "    df['sub_domain_name'] = df.apply(lambda row: extract4(row['url']), axis=1)\n",
    "    df['url_match_1'] = df.apply(lambda row: extract2(row['url']), axis=1)\n",
    "    df['url_match_2'] = df.apply(lambda row: extract3(row['url']), axis=1)\n",
    "    # merging the file with the dataframe that has all the subdomains and organisation names\n",
    "    # saving the file\n",
    "    df.to_csv(outFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### COMBINING ALL DATAFRAMES ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_germany =  pd.read_csv(\"/Users/mxm/Google Drive/Masterstudium/Inhalte/Master Thesis/GitHubRepo/agile-in-government/Analysis/2_Data_Preprocessing/DATA/CSVs/Germany/Combined_cleaned.csv\")\n",
    "df_uk = pd.read_csv('/Users/mxm/Google Drive/Masterstudium/Inhalte/Master Thesis/GitHubRepo/agile-in-government/Analysis/2_Data_Preprocessing/DATA/CSVs/UK/General_cleaned_org.csv')\n",
    "df_master = pd.concat([df_germany,df_uk])\n",
    "\n",
    "df_master.to_csv('/Users/mxm/Google Drive/Masterstudium/Inhalte/Master Thesis/GitHubRepo/agile-in-government/Analysis/2_Data_Preprocessing/DATA/CSVs/Master_File.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
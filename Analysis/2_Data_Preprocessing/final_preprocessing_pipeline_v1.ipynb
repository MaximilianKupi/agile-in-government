{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitscraperv1condac67eee00092f451cb06b7dadff488d14",
   "display_name": "Python 3.7.6 64-bit ('Scraper_v1': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the required packages\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from lxml import etree\n",
    "from lxml import html\n",
    "try:\n",
    "    from urllib2 import urlopen\n",
    "except ImportError:\n",
    "    from urllib.request import urlopen\n",
    "import dateutil\n",
    "from dateutil.parser import *\n",
    "from datetime import datetime\n",
    "import pathlib\n",
    "from htmldate import find_date\n",
    "import justext\n",
    "import requests\n",
    "from newspaper import fulltext\n",
    "import codecs\n",
    "import html2text\n",
    "import trafilatura\n",
    "import spacy\n",
    "import dframcy\n",
    "from dframcy import DframCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### ENGLAND ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the function to read in all the html files, clean the text, get further publishing dates and select the earliest one, and store everything into csv\n",
    "# process statewise UK VERSION --> type \"General\"\n",
    "\n",
    "def process_state_UK(state):\n",
    "\n",
    "    # compiling the regex search terms\n",
    "    agile_regex = re.compile(r'\\bagility\\b|\\bagile\\b', re.IGNORECASE | re.UNICODE) # for agile\n",
    "    agile_context_regex = re.compile(r'\\s*([^\\s]+?)\\s+([^\\s]+?)\\s+([^\\s]+?)\\s+([^\\s]+?)\\s+agil.*?\\s+([^\\s]+)\\s+([^\\s]+?)\\s+([^\\s]+?)\\s+([^\\s]+?)\\s+', re.IGNORECASE | re.UNICODE) # for context of the first mention\n",
    "    digital_regex = re.compile(r'\\bdigital\\b|\\bditialization\\b|\\bdigitalisation\\b\\bdigital transformation\\b', re.IGNORECASE | re.UNICODE) # for anything related to digital\n",
    "\n",
    "    # getting the csv which was produced by scrapy as input file\n",
    "    csv_main = csv.DictReader(open(\"/Users/mxm/Google Drive/Masterstudium/Inhalte/Master Thesis/GitHubRepo/agile-in-government/Analysis/1_Data_Collection/DATA/CSVs/UK/SAVEPLACE/agile_sites_output_UK_{}.csv\".format(state)), fieldnames=[\"id\", 'url', 'domain', 'date1', 'date2', 'date3', 'heading']) \n",
    "\n",
    "    # creating an empty array to write into in the processing loop\n",
    "    array = {}\n",
    "\n",
    "    # getting every line of the input csv into the array\n",
    "    for line in csv_main: \n",
    "        array[line[\"id\"]] = line\n",
    "    \n",
    "    # specfifying the the directory where the text files should be stored\n",
    "    text_dir = \"/Users/mxm/Google Drive/Masterstudium/Inhalte/Master Thesis/GitHubRepo/agile-in-government/Analysis/2_Data_Preprocessing/DATA/TextFiles/UK/{}\".format(state)\n",
    "    \n",
    "    # making the directory if it doesn't exist\n",
    "    pathlib.Path(text_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # specifying variables to keep track of the progress while processing\n",
    "    total_count = len(array)\n",
    "    processed = 0\n",
    "\n",
    "    # this is the main working loop\n",
    "    for id, line in array.items():   \n",
    "        # print function to keep track of which files are being processed     \n",
    "        print(\"processing {}\".format(id))\n",
    "        # getting the id from the array\n",
    "        id = line[\"id\"]\n",
    "        # getting the domain from the array\n",
    "        domain = line[\"domain\"]\n",
    "        # getting the url from the array\n",
    "        url = line[\"url\"]\n",
    "        # specifying the path where the html to process is located\n",
    "        path = '/Users/mxm/Google Drive/Masterstudium/Inhalte/Master Thesis/GitHubRepo/agile-in-government/Analysis/1_Data_Collection/DATA/HTMLs/UK/SAVEPLACE/{}/{}/{}.html'.format(state, domain, id)\n",
    "        # opening the html file while catching an encoding error     \n",
    "        try:  \n",
    "            soup = BeautifulSoup(open(path), \"html.parser\")\n",
    "        except UnicodeDecodeError:\n",
    "            soup = BeautifulSoup(open(path, encoding='windows-1252'), \"html.parser\")    \n",
    "        \n",
    "        ### In the following further potential publication dates are parsed and the oldest one is select as \"final date\" ###\n",
    "\n",
    "        # Getting date4\n",
    "        date4_element = soup.select_one(\"span.date\")\n",
    "        date4 = \"\"\n",
    "        if date4_element is not None:\n",
    "            date4 = date4_element.get_text()\n",
    "        \n",
    "        # Getting date6\n",
    "        date6_element = soup.select_one(\"h1#page-title + p\")\n",
    "        date6 = \"\"\n",
    "        if date6_element is not None:\n",
    "            date6 = date6_element.get_text()\n",
    "\n",
    "       # Getting date5\n",
    "        htmlparser = etree.HTMLParser()    \n",
    "        try:\n",
    "            tree = etree.parse(open(path), htmlparser)\n",
    "        except UnicodeDecodeError:\n",
    "            tree = etree.parse(open(path, encoding='windows-1252'), htmlparser)\n",
    "        \n",
    "        date5 = tree.xpath(\"substring(substring-after(/html//script[@type='application/ld+json']/text(), 'datePublished'), 4, 23)\")\n",
    "\n",
    "        # a special cases for date 5\n",
    "        date5_1 = tree.xpath(\"substring(substring-after(/html//script[@type='application/ld+json']/text(), 'datePublished'), 4, 25)\")\n",
    "\n",
    "\n",
    "        # Getting date7 from National Archives --> the date the site was archived on --> the date the site was released on would have been even earlier\n",
    "        date7 = tree.xpath(\"substring(substring-after(/html/head/script/text(), 'timestamp'),9,14)\")\n",
    "\n",
    "  \n",
    "        # different parsing for for htmldate\n",
    "        try:\n",
    "            html_tree = html.parse(open(path))\n",
    "        except UnicodeDecodeError:\n",
    "            html_tree = html.parse(open(path, encoding='windows-1252'))\n",
    "            \n",
    "\n",
    "        # Getting date with the htmldate package\n",
    "        htmldate = find_date(html_tree)\n",
    "\n",
    "\n",
    "        # since html date gives out nonetype when it doesn't find anything, it has to be respecified to empty string so that it works with dateparser later on\n",
    "        if htmldate is None:\n",
    "            htmldate = ''\n",
    "\n",
    "        #htmldate = htmldate.astype('str')\n",
    "\n",
    "\n",
    "\n",
    "        # assinging the already succesfully parsed dates during crawling stage\n",
    "        date1 = line[\"date1\"]\n",
    "        date2 = line[\"date2\"]\n",
    "\n",
    "\n",
    "        # Storing the oldest date as final date variable in python date format\n",
    "        date_vars = [date1, date2, date4, date5, date5_1, date6, date7, htmldate]\n",
    "        \n",
    "        final_date = None \n",
    "\n",
    "        for date_var in date_vars:\n",
    "            try:\n",
    "                if final_date is None:\n",
    "                    final_date = parse(date_var, ignoretz = True)\n",
    "                elif parse(date_var, ignoretz=True) < final_date:\n",
    "                    final_date = parse(date_var, ignoretz=True)\n",
    "            except dateutil.parser._parser.ParserError:\n",
    "                pass \n",
    "\n",
    "\n",
    "\n",
    "        #deleting all non-body text content from the html\n",
    "        \n",
    "        # defining the html elements to delete\n",
    "        for script in soup(['script', 'style', 'meta', 'a', 'head', 'footer', 'navbar', 'header', 'search-box', 'global-cookie-message', 'id=\"global-cookie-message\"', 'global-bar', 'menu', 'noscript', 'global-cookie-message', 'search']):\n",
    "            script.decompose()    # deleting those elements\n",
    "            \n",
    "        # geting text\n",
    "        doctext = soup.get_text()\n",
    "\n",
    "        # break into lines and remove leading and trailing space on each\n",
    "        lines = (line.strip() for line in doctext.splitlines())\n",
    "        # break multi-headlines into a line each\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        # drop blank lines\n",
    "        doctext = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "        \n",
    "  \n",
    "\n",
    "        # saving the cleaned text into the directory\n",
    "        with open(\"{}/{}.txt\".format(text_dir, id), \"w\") as textfile:\n",
    "            textfile.write(doctext)\n",
    "        \n",
    "\n",
    " \n",
    "        #### In the following the matches for the search terms (agile, digital) are gathered ####\n",
    "        \n",
    "        # finding the matches for agile and agility\n",
    "        agile_term = []\n",
    "        agile_term = agile_regex.findall(doctext.lower())\n",
    "    \n",
    "\n",
    "        # finding the matches for digital\n",
    "        digital_term = []\n",
    "        digital_term = digital_regex.findall(doctext.lower())\n",
    "\n",
    "\n",
    "        # finding the context for agile\n",
    "        agile_context = []\n",
    "        agile_context = agile_context_regex.search(doctext)\n",
    "\n",
    "        agile_context_pre = \"\"\n",
    "        agile_context_post = \"\"\n",
    "        \n",
    "        if agile_context is not None:\n",
    "            agile_context_pre = \" \".join(agile_context.group(1,2,3,4))\n",
    "            agile_context_post = \" \".join(agile_context.group(5,6,7,8))\n",
    "        \n",
    "\n",
    "        # assigning all generated variables as line items to the array\n",
    "\n",
    "        if len(agile_term) == 0:\n",
    "            line[\"agile_term\"] = \"\" \n",
    "        else:\n",
    "            line[\"agile_term\"] = \",\".join(agile_term)\n",
    "\n",
    "        if len(digital_term) == 0:\n",
    "            line[\"digital_term\"] = \"\" \n",
    "        else:\n",
    "            line[\"digital_term\"] = \",\".join(digital_term)\n",
    "\n",
    "\n",
    "        line[\"agile_context_pre\"] = agile_context_pre\n",
    "        line[\"agile_context_post\"] = agile_context_post \n",
    "        line[\"date4\"] = date4\n",
    "        line[\"date5\"] = date5\n",
    "        line[\"date5_1\"] = date5_1\n",
    "        line[\"date6\"] = date6\n",
    "        line[\"date7\"] = date7\n",
    "        line[\"final_date\"] = final_date\n",
    "        line[\"htmldate\"] = htmldate\n",
    "        line[\"country\"] = \"UK\"\n",
    "        line[\"level\"] = state\n",
    "        line[\"text_file_loc\"] = \"{}/{}.txt\".format(text_dir, id)\n",
    "        line['doctext'] = doctext\n",
    "\n",
    "        \n",
    "        # Printing processing status\n",
    "        processed += 1\n",
    "        print(\"processed: {}%\".format((processed/total_count)*100))\n",
    "        print(\"Current Time: \", datetime.now())\n",
    "\n",
    "    # opening the output csv\n",
    "    outputfile = open(\"/Users/mxm/Google Drive/Masterstudium/Inhalte/Master Thesis/GitHubRepo/agile-in-government/Analysis/2_Data_Preprocessing/DATA/CSVs/UK/{}.csv\".format(state), \"w\")\n",
    "    \n",
    "    # writing all the array lines into the csv file\n",
    "    writer = csv.DictWriter(outputfile, fieldnames=[\"id\", \"country\", \"level\", 'url', 'domain', 'date1', 'date2', 'date3', 'date4', 'date5', 'date5_1', 'date6', 'date7', 'htmldate', 'final_date', 'heading', 'agile_term', 'agile_context_pre', 'agile_context_post', 'digital_term', 'text_file_loc', 'doctext',])\n",
    "    writer.writeheader()\n",
    "    for id, line in array.items():\n",
    "        writer.writerow(line)\n",
    "    outputfile.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0\nprocessing fc9a9bd8-630a-3993-b72f-fff668ac4d59\nprocessed: 93.03106633081444%\nCurrent Time:  2020-03-18 15:14:32.121765\nprocessing 04cf18e9-6dad-3ddb-bb76-acbc6e8dde53\nprocessed: 93.07304785894208%\nCurrent Time:  2020-03-18 15:14:32.203108\nprocessing c49d43ab-19c9-36c6-a9aa-d6f04a06b8a0\nprocessed: 93.1150293870697%\nCurrent Time:  2020-03-18 15:14:32.293491\nprocessing 7482effb-3e0b-3e53-b7a6-c58e31387805\nprocessed: 93.15701091519732%\nCurrent Time:  2020-03-18 15:14:32.375718\nprocessing 2d6b2163-83e2-336c-85c4-10c95c604827\nprocessed: 93.19899244332494%\nCurrent Time:  2020-03-18 15:14:32.460693\nprocessing 2cd55f9c-0e80-3ba4-828c-07782788b882\nprocessed: 93.24097397145256%\nCurrent Time:  2020-03-18 15:14:32.545923\nprocessing c58d5ebf-4a54-3162-a745-b497d94d2c00\nprocessed: 93.28295549958018%\nCurrent Time:  2020-03-18 15:14:32.631278\nprocessing 7d4fb21a-3383-32fd-965f-832f4de993ce\nprocessed: 93.3249370277078%\nCurrent Time:  2020-03-18 15:14:32.715460\nprocessing 3f95d1d2-1602-3225-a413-1e2fba710215\nprocessed: 93.36691855583543%\nCurrent Time:  2020-03-18 15:14:32.798600\nprocessing b5925260-424f-3b47-b452-743277ceeb32\nprocessed: 93.40890008396305%\nCurrent Time:  2020-03-18 15:14:32.881743\nprocessing 4e5ef193-89ad-3a69-bbdc-af070d5b745f\nprocessed: 93.45088161209067%\nCurrent Time:  2020-03-18 15:14:32.968898\nprocessing ad3bbcd3-d6f2-317e-9580-557e1108b98e\nprocessed: 93.4928631402183%\nCurrent Time:  2020-03-18 15:14:33.048620\nprocessing a0beb806-0e03-3b0a-8876-900ead2729cb\nprocessed: 93.53484466834593%\nCurrent Time:  2020-03-18 15:14:33.137481\nprocessing 7e9d8572-02a9-34dc-899d-e444b504b46f\nprocessed: 93.57682619647355%\nCurrent Time:  2020-03-18 15:14:33.224369\nprocessing 95622706-37ae-38ed-9e57-7fe6bb2bd6b1\nprocessed: 93.61880772460117%\nCurrent Time:  2020-03-18 15:14:33.309889\nprocessing 41da7db6-382b-34ba-aaef-c251255734d7\nprocessed: 93.6607892527288%\nCurrent Time:  2020-03-18 15:14:33.406408\nprocessing 311ae518-fefc-32c7-9320-088f45dcc854\nprocessed: 93.70277078085643%\nCurrent Time:  2020-03-18 15:14:33.505892\nprocessing f7885494-fc41-388f-badc-e0bac4c5581b\nprocessed: 93.74475230898405%\nCurrent Time:  2020-03-18 15:14:33.594737\nprocessing 77f3446a-c41e-3d5f-9040-e582c2bfe573\nprocessed: 93.78673383711167%\nCurrent Time:  2020-03-18 15:14:33.683970\nprocessing dd52b21b-6faf-3d25-8096-c6d2bc7c19ff\nprocessed: 93.82871536523929%\nCurrent Time:  2020-03-18 15:14:33.763635\nprocessing b40c6853-5b71-3cee-a542-a08a76e5f31f\nprocessed: 93.87069689336693%\nCurrent Time:  2020-03-18 15:14:33.857300\nprocessing 366140f4-c8cd-3c01-bb7b-01797483523a\nprocessed: 93.91267842149455%\nCurrent Time:  2020-03-18 15:14:34.276112\nprocessing 054531d5-61b1-3575-8ea2-ed24dbcfd2ba\nprocessed: 93.95465994962217%\nCurrent Time:  2020-03-18 15:14:34.360105\nprocessing 843eaadf-2779-3b38-b071-87671185edbe\nprocessed: 93.99664147774979%\nCurrent Time:  2020-03-18 15:14:34.446193\nprocessing e93345e3-90d4-33f9-95bb-6b1ddceb2925\nprocessed: 94.03862300587741%\nCurrent Time:  2020-03-18 15:14:34.532618\nprocessing eb516f4c-9f67-3d05-b437-1a8971db855f\nprocessed: 94.08060453400505%\nCurrent Time:  2020-03-18 15:14:34.618153\nprocessing 4830ecfe-daf3-3e96-af8c-7672160eb981\nprocessed: 94.12258606213267%\nCurrent Time:  2020-03-18 15:14:34.711138\nprocessing c2836459-b476-3f97-bcf4-f17626c32223\nprocessed: 94.16456759026029%\nCurrent Time:  2020-03-18 15:14:34.795866\nprocessing d81f909a-a7be-3593-8c73-e133baeea01a\nprocessed: 94.20654911838791%\nCurrent Time:  2020-03-18 15:14:34.877070\nprocessing 1ec6f645-cc97-3a60-aec2-bd382139bbc7\nprocessed: 94.24853064651553%\nCurrent Time:  2020-03-18 15:14:34.956528\nprocessing b0a3dec6-4484-3eca-af5d-657f6a0dbf85\nprocessed: 94.29051217464315%\nCurrent Time:  2020-03-18 15:14:35.040847\nprocessing 8aa9ebd5-db25-3c30-b791-767f2e23695d\nprocessed: 94.33249370277078%\nCurrent Time:  2020-03-18 15:14:35.129669\nprocessing 0f8abafe-490a-35e4-a2f0-ef46e8c4d531\nprocessed: 94.3744752308984%\nCurrent Time:  2020-03-18 15:14:35.221827\nprocessing b435d967-0be9-3ac9-aab0-63358b156262\nprocessed: 94.41645675902602%\nCurrent Time:  2020-03-18 15:14:35.323422\nprocessing 8f42bb70-8e92-3b1a-80e3-b11c79608edb\nprocessed: 94.45843828715365%\nCurrent Time:  2020-03-18 15:14:35.424717\nprocessing 87548b02-99e0-3db6-8ff4-2327a207db59\nprocessed: 94.50041981528128%\nCurrent Time:  2020-03-18 15:14:35.513343\nprocessing 7bee96da-efd4-3117-b294-ba68627c5074\nprocessed: 94.5424013434089%\nCurrent Time:  2020-03-18 15:14:35.598967\nprocessing 8ec12f41-7ea1-3877-9920-2c0b9764091c\nprocessed: 94.58438287153652%\nCurrent Time:  2020-03-18 15:14:35.688354\nprocessing 883f782f-316f-3e75-a3fe-f0f4dbdfefdc\nprocessed: 94.62636439966414%\nCurrent Time:  2020-03-18 15:14:35.770073\nprocessing 07cd0e2c-ba41-3f1e-99e3-536f35c7dc7c\nprocessed: 94.66834592779178%\nCurrent Time:  2020-03-18 15:14:35.856639\nprocessing b0febf7f-8fca-3555-9e74-f3b202c64479\nprocessed: 94.7103274559194%\nCurrent Time:  2020-03-18 15:14:35.933727\nprocessing add9e4be-64c6-323d-858a-b79cdd0bb23e\nprocessed: 94.75230898404702%\nCurrent Time:  2020-03-18 15:14:36.014324\nprocessing a91fbe8d-665b-3c9a-bfbe-61a3b96def7f\nprocessed: 94.79429051217464%\nCurrent Time:  2020-03-18 15:14:36.097574\nprocessing e80a9ab4-1bce-3da1-be09-5e8a09cb6057\nprocessed: 94.83627204030228%\nCurrent Time:  2020-03-18 15:14:36.179910\nprocessing 391bdbda-7709-30b2-9fde-88a27b6b7771\nprocessed: 94.8782535684299%\nCurrent Time:  2020-03-18 15:14:36.267083\nprocessing dd9cb9c2-4437-3028-bd5c-aa4eb98d93c6\nprocessed: 94.92023509655752%\nCurrent Time:  2020-03-18 15:14:36.345698\nprocessing 0faeab5e-18ad-34b9-bb31-69ea83e22c68\nprocessed: 94.96221662468514%\nCurrent Time:  2020-03-18 15:14:36.429726\nprocessing 6d51b063-8b9b-34cc-8ff7-425671be7903\nprocessed: 95.00419815281276%\nCurrent Time:  2020-03-18 15:14:36.512234\nprocessing da026dd4-d1e3-305a-8ba0-113306b6703d\nprocessed: 95.04617968094038%\nCurrent Time:  2020-03-18 15:14:36.596102\nprocessing ddd67f9c-24c2-3bb0-8a38-cb634f4db312\nprocessed: 95.088161209068%\nCurrent Time:  2020-03-18 15:14:36.680137\nprocessing 1bce006b-5046-3f95-8a3c-6efcc9dc77ce\nprocessed: 95.13014273719563%\nCurrent Time:  2020-03-18 15:14:36.752374\nprocessing 4c82f2e0-fe6d-3b30-836b-48e79da3b306\nprocessed: 95.17212426532325%\nCurrent Time:  2020-03-18 15:14:36.831152\nprocessing 54fdcbdb-7b8d-3102-9538-9edacf3e4e9d\nprocessed: 95.21410579345087%\nCurrent Time:  2020-03-18 15:14:36.915745\nprocessing 4244da25-0b1b-386a-bde2-d198ce1bfceb\nprocessed: 95.2560873215785%\nCurrent Time:  2020-03-18 15:14:37.002322\nprocessing ead46699-e9fe-3981-8202-70e1f3d4b096\nprocessed: 95.29806884970613%\nCurrent Time:  2020-03-18 15:14:37.091491\nprocessing b37b2e4e-591e-3241-bff1-91a6c230ba59\nprocessed: 95.34005037783375%\nCurrent Time:  2020-03-18 15:14:37.179426\nprocessing 852f586f-a628-3151-ae5c-57e3301677d7\nprocessed: 95.38203190596137%\nCurrent Time:  2020-03-18 15:14:37.261517\nprocessing 99caacca-fa51-3367-a30a-8e19fec806f0\nprocessed: 95.424013434089%\nCurrent Time:  2020-03-18 15:14:37.349467\nprocessing efbb28f5-a14b-36c0-9c20-73a98e5ffb0c\nprocessed: 95.46599496221663%\nCurrent Time:  2020-03-18 15:14:37.439978\nprocessing 03a644a8-817e-31ee-8a50-57127e54bbcb\nprocessed: 95.50797649034425%\nCurrent Time:  2020-03-18 15:14:37.523021\nprocessing aaebd192-67e7-3768-ad35-27c09c9cb5ff\nprocessed: 95.54995801847187%\nCurrent Time:  2020-03-18 15:14:37.609570\nprocessing 2df02b57-352e-3814-b899-8fe7f385e62a\nprocessed: 95.59193954659949%\nCurrent Time:  2020-03-18 15:14:37.697671\nprocessing 37dec61a-67f6-336a-99eb-eb44e5360a38\nprocessed: 95.63392107472713%\nCurrent Time:  2020-03-18 15:14:37.786490\nprocessing 0015858e-eb45-3ab6-a7f6-99f319b57e64\nprocessed: 95.67590260285475%\nCurrent Time:  2020-03-18 15:14:37.873476\nprocessing 62f52822-9502-39e7-9ebb-16d4a9b5bfe2\nprocessed: 95.71788413098237%\nCurrent Time:  2020-03-18 15:14:37.962207\nprocessing 1b0dc6f7-f919-3a62-8d87-a816de465c9e\nprocessed: 95.75986565910999%\nCurrent Time:  2020-03-18 15:14:38.040820\nprocessing db1bc3e7-ae5d-3277-aeec-a2ff6fb2726c\nprocessed: 95.80184718723763%\nCurrent Time:  2020-03-18 15:14:38.121450\nprocessing 2df0a28c-9892-3b36-9fea-d371990e0ab2\nprocessed: 95.84382871536525%\nCurrent Time:  2020-03-18 15:14:38.208859\nprocessing ed99b582-ae3a-3e8e-a21d-b5862e7bc37c\nprocessed: 95.88581024349287%\nCurrent Time:  2020-03-18 15:14:38.289982\nprocessing 3a560600-d34c-3388-b216-03ddbe4e16a4\nprocessed: 95.92779177162049%\nCurrent Time:  2020-03-18 15:14:38.367238\nprocessing 852e0c10-8fd1-376e-969b-bb2e2d9cb097\nprocessed: 95.96977329974811%\nCurrent Time:  2020-03-18 15:14:38.457613\nprocessing 36702d30-64ed-3538-8959-4af9c1590f70\nprocessed: 96.01175482787573%\nCurrent Time:  2020-03-18 15:14:38.542792\nprocessing f55393db-9cb2-3dee-b926-6da77e24840e\nprocessed: 96.05373635600336%\nCurrent Time:  2020-03-18 15:14:38.661652\nprocessing 9b05c156-dd54-3046-b27c-88b7825fb51c\nprocessed: 96.09571788413098%\nCurrent Time:  2020-03-18 15:14:38.754071\nprocessing c32f0e93-6a84-398f-993d-f589c10674b1\nprocessed: 96.1376994122586%\nCurrent Time:  2020-03-18 15:14:38.836966\nprocessing 2e581bce-ee99-3839-8388-847df9fec7a5\nprocessed: 96.17968094038622%\nCurrent Time:  2020-03-18 15:14:38.913638\nprocessing 89bf5122-a66c-35ec-a5ab-50e7f13131b5\nprocessed: 96.22166246851386%\nCurrent Time:  2020-03-18 15:14:39.005477\nprocessing aab15b12-1414-32c3-9e2d-89fc38195272\nprocessed: 96.26364399664148%\nCurrent Time:  2020-03-18 15:14:39.091021\nprocessing 1cee52b5-3c88-3c8d-9a88-d2d6c8f31020\nprocessed: 96.3056255247691%\nCurrent Time:  2020-03-18 15:14:39.176680\nprocessing 5f0d0a4e-a457-3a63-9c51-69ad42aac08f\nprocessed: 96.34760705289672%\nCurrent Time:  2020-03-18 15:14:39.260654\nprocessing 3a3fccc7-a597-3685-affb-677bd1be88de\nprocessed: 96.38958858102436%\nCurrent Time:  2020-03-18 15:14:39.349769\nprocessing 31a0230c-bab8-3ff2-8300-7ea12af79490\nprocessed: 96.43157010915198%\nCurrent Time:  2020-03-18 15:14:39.439799\nprocessing 1dfb7dcc-2927-3921-8332-bc091f57e165\nprocessed: 96.4735516372796%\nCurrent Time:  2020-03-18 15:14:39.528590\nprocessing c3f8f06e-810f-3d0c-9f63-d40c3358c7b2\nprocessed: 96.51553316540722%\nCurrent Time:  2020-03-18 15:14:39.614296\nprocessing 60c9907f-dc02-3bce-98f4-b576c29fc09f\nprocessed: 96.55751469353484%\nCurrent Time:  2020-03-18 15:14:39.700887\nprocessing d6b17ca6-bbd4-31a6-943a-43be5a8706ed\nprocessed: 96.59949622166248%\nCurrent Time:  2020-03-18 15:14:39.790395\nprocessing 6c80d77a-86da-3e2c-9fe3-06b52221a52d\nprocessed: 96.6414777497901%\nCurrent Time:  2020-03-18 15:14:39.894279\nprocessing f3a18ba2-6e4e-347c-a601-e41a399f62eb\nprocessed: 96.68345927791772%\nCurrent Time:  2020-03-18 15:14:39.997229\nprocessing ae7c66b3-8f2b-33d8-aef9-fbb44ee8539e\nprocessed: 96.72544080604534%\nCurrent Time:  2020-03-18 15:14:40.084706\nprocessing 6714d561-4943-3b30-94c1-afcdac842825\nprocessed: 96.76742233417296%\nCurrent Time:  2020-03-18 15:14:40.168019\nprocessing ef138304-8f7b-3750-8149-d203dc95b12b\nprocessed: 96.8094038623006%\nCurrent Time:  2020-03-18 15:14:40.251157\nprocessing eceb6031-df2e-3d61-b67e-a611ac07abb6\nprocessed: 96.85138539042822%\nCurrent Time:  2020-03-18 15:14:40.344946\nprocessing 13c58f89-9454-3880-862c-a64f87f44b00\nprocessed: 96.89336691855584%\nCurrent Time:  2020-03-18 15:14:40.429023\nprocessing 12e5d6f1-b73d-31d1-a918-1b894b554156\nprocessed: 96.93534844668346%\nCurrent Time:  2020-03-18 15:14:40.534851\nprocessing 96b0fceb-723c-3e9c-84ef-cc732eb3a6ad\nprocessed: 96.97732997481108%\nCurrent Time:  2020-03-18 15:14:40.628941\nprocessing 31bbea36-4f06-371a-a2f0-adfc4bb4fa2b\nprocessed: 97.0193115029387%\nCurrent Time:  2020-03-18 15:14:40.706713\nprocessing 27fd05f2-135d-3ec9-ad0a-c102df0ad541\nprocessed: 97.06129303106633%\nCurrent Time:  2020-03-18 15:14:40.806415\nprocessing d5547c9b-92d2-3e91-95b7-44853b39d277\nprocessed: 97.10327455919395%\nCurrent Time:  2020-03-18 15:14:40.897071\nprocessing 7162de85-f19c-3306-a6f6-0e7c7f4389b4\nprocessed: 97.14525608732157%\nCurrent Time:  2020-03-18 15:14:40.986550\nprocessing 9275806f-005b-3ed7-9904-926abbb8072c\nprocessed: 97.1872376154492%\nCurrent Time:  2020-03-18 15:14:41.070416\nprocessing 23e8f1e5-a666-3e05-bb5b-9ce50b55a088\nprocessed: 97.22921914357683%\nCurrent Time:  2020-03-18 15:14:41.151641\nprocessing 4ae75663-d968-3d3d-bf7e-386cb6132a9b\nprocessed: 97.27120067170445%\nCurrent Time:  2020-03-18 15:14:41.232496\nprocessing 19d31e70-1ea2-3fe4-ad6e-2cb9bfa46daf\nprocessed: 97.31318219983207%\nCurrent Time:  2020-03-18 15:14:41.321151\nprocessing fb1e1778-7639-385d-bc54-e9fa5e6c3329\nprocessed: 97.35516372795969%\nCurrent Time:  2020-03-18 15:14:41.406983\nprocessing 2a2d0d37-1e7e-3d31-915a-c47942a24264\nprocessed: 97.39714525608733%\nCurrent Time:  2020-03-18 15:14:41.491921\nprocessing 146d59cb-7dd3-3101-b51c-6d994516ac73\nprocessed: 97.43912678421495%\nCurrent Time:  2020-03-18 15:14:41.578016\nprocessing b1bee9c5-5922-36b2-b5e1-1a04c997c7bd\nprocessed: 97.48110831234257%\nCurrent Time:  2020-03-18 15:14:41.659507\nprocessing 4107e0d3-fdfe-3692-88e8-aad31d347acf\nprocessed: 97.52308984047019%\nCurrent Time:  2020-03-18 15:14:41.739153\nprocessing f7a1d726-e8bf-3344-a0cd-e2d329a29ac8\nprocessed: 97.56507136859783%\nCurrent Time:  2020-03-18 15:14:41.814991\nprocessing b393e9ad-e62b-3808-97fb-9bb33a9310c2\nprocessed: 97.60705289672545%\nCurrent Time:  2020-03-18 15:14:41.895700\nprocessing 8011bd2d-71fb-3255-9d43-f4cf202744f3\nprocessed: 97.64903442485307%\nCurrent Time:  2020-03-18 15:14:41.974823\nprocessing ad15fb28-372e-364a-974f-70386c6e64d6\nprocessed: 97.69101595298069%\nCurrent Time:  2020-03-18 15:14:42.056351\nprocessing 82ed3b11-4911-304b-bfec-f56eb7cab6cb\nprocessed: 97.73299748110831%\nCurrent Time:  2020-03-18 15:14:42.140657\nprocessing ddaf73fc-e1b1-3a08-b934-8129762dba5f\nprocessed: 97.77497900923593%\nCurrent Time:  2020-03-18 15:14:42.215564\nprocessing fa4b751f-4b6c-30c9-ae01-ef960daef2ae\nprocessed: 97.81696053736356%\nCurrent Time:  2020-03-18 15:14:42.299793\nprocessing 46970c87-f928-3dbb-9290-29155ed3c8c9\nprocessed: 97.85894206549118%\nCurrent Time:  2020-03-18 15:14:42.387385\nprocessing e22d01e8-915e-338b-9441-8c45648529f0\nprocessed: 97.9009235936188%\nCurrent Time:  2020-03-18 15:14:42.470920\nprocessing 12a6f03f-7290-34f3-a77d-a3a2fc7c4fdc\nprocessed: 97.94290512174642%\nCurrent Time:  2020-03-18 15:14:42.818273\nprocessing f790c7a4-57ae-31e5-b52a-4b0335b7e71f\nprocessed: 97.98488664987406%\nCurrent Time:  2020-03-18 15:14:42.919007\nprocessing a950a20b-29d8-30db-aae6-8d66165672ce\nprocessed: 98.02686817800168%\nCurrent Time:  2020-03-18 15:14:43.074315\nprocessing f9f62e29-95fe-3ea6-b566-6f0ecc606dfc\nprocessed: 98.0688497061293%\nCurrent Time:  2020-03-18 15:14:43.259171\nprocessing 5549d07c-2468-3299-acf1-74f91ac839d2\nprocessed: 98.11083123425692%\nCurrent Time:  2020-03-18 15:14:43.296485\nprocessing 0d7079ae-c8c5-3717-be73-2279b3a5a34c\nprocessed: 98.15281276238456%\nCurrent Time:  2020-03-18 15:14:44.596827\nprocessing a6dbeb74-7b9a-316b-8775-97a9d1992f5b\nprocessed: 98.19479429051218%\nCurrent Time:  2020-03-18 15:14:44.669578\nprocessing 1d1ab5ca-9a17-39f8-ba5c-85ec2847fb40\nprocessed: 98.2367758186398%\nCurrent Time:  2020-03-18 15:14:46.221318\nprocessing 63a39a44-3034-376d-8d9c-151064e3802d\nprocessed: 98.27875734676742%\nCurrent Time:  2020-03-18 15:14:47.601859\nprocessing 73541b94-2bbc-3326-ba73-1903cf12a1ae\nprocessed: 98.32073887489504%\nCurrent Time:  2020-03-18 15:14:47.769233\nprocessing 53e4c396-8d56-3d87-8e41-181ff8e55b0f\nprocessed: 98.36272040302268%\nCurrent Time:  2020-03-18 15:14:47.894986\nprocessing c9a3bdda-5402-3125-a351-cad2391dd17f\nprocessed: 98.4047019311503%\nCurrent Time:  2020-03-18 15:14:47.924710\nprocessing 1dab1245-b31e-3eec-9ac9-7596c8fcdcaf\nprocessed: 98.44668345927792%\nCurrent Time:  2020-03-18 15:14:48.057556\nprocessing 4b8cccaf-d9da-3526-9c35-c0f4c9687186\nprocessed: 98.48866498740554%\nCurrent Time:  2020-03-18 15:14:48.117527\nprocessing 9d62d413-2498-3a33-b322-d6316a626354\nprocessed: 98.53064651553316%\nCurrent Time:  2020-03-18 15:14:48.173999\nprocessing 8e680286-eaa7-3035-b4e8-c9e12ef14d6e\nprocessed: 98.5726280436608%\nCurrent Time:  2020-03-18 15:14:48.219794\nprocessing da9af499-150d-301a-b748-62229d0afb48\nprocessed: 98.61460957178842%\nCurrent Time:  2020-03-18 15:14:48.268860\nprocessing a53a2d10-6fcb-34b6-9ed3-984bf967051f\nprocessed: 98.65659109991604%\nCurrent Time:  2020-03-18 15:14:48.312559\nprocessing 5e95e196-dc45-3752-8202-fd35b753ae65\nprocessed: 98.69857262804366%\nCurrent Time:  2020-03-18 15:14:49.455007\nprocessing e8452df1-52a2-32b4-83eb-d7ad5925dd95\nprocessed: 98.74055415617129%\nCurrent Time:  2020-03-18 15:14:50.708650\nprocessing 7af310d7-bfda-38d0-ab44-ca61609cf4f9\nprocessed: 98.7825356842989%\nCurrent Time:  2020-03-18 15:14:50.795113\nprocessing bd1de351-93e2-36bc-b7dd-4cf303949659\nprocessed: 98.82451721242653%\nCurrent Time:  2020-03-18 15:14:51.022009\nprocessing b252e189-d15d-304f-869b-c3ea57b5c858\nprocessed: 98.86649874055415%\nCurrent Time:  2020-03-18 15:14:51.100618\nprocessing 4ae13f60-9e14-36f8-baac-35a73b6a573b\nprocessed: 98.90848026868177%\nCurrent Time:  2020-03-18 15:14:51.170884\nprocessing 7850a7ab-01c0-3f2e-b8a5-c15379f93e50\nprocessed: 98.9504617968094%\nCurrent Time:  2020-03-18 15:14:51.220570\nprocessing 2ca46f2e-d1e9-3265-8b08-e020a7a9ca1a\nprocessed: 98.99244332493703%\nCurrent Time:  2020-03-18 15:14:51.294277\nprocessing 0921236d-a358-3ed2-8030-5f6768925849\nprocessed: 99.03442485306465%\nCurrent Time:  2020-03-18 15:14:52.173912\nprocessing eb083253-8b91-32de-8cf5-c7b0a138dab6\nprocessed: 99.07640638119227%\nCurrent Time:  2020-03-18 15:14:52.231036\nprocessing 9e7a1efe-34dd-3621-a984-c0475a98251d\nprocessed: 99.11838790931989%\nCurrent Time:  2020-03-18 15:14:52.324277\nprocessing e388d426-9c60-3260-8561-8cffb47c7fb6\nprocessed: 99.16036943744753%\nCurrent Time:  2020-03-18 15:14:52.390286\nprocessing fe883b70-932d-3591-b22d-de837a3a4a47\nprocessed: 99.20235096557515%\nCurrent Time:  2020-03-18 15:14:52.444510\nprocessing e5901c84-2ca9-3a97-8723-9d60d573c491\nprocessed: 99.24433249370277%\nCurrent Time:  2020-03-18 15:14:52.515162\nprocessing 84e852d3-e6b0-3796-8e4f-dd79df6ac60b\nprocessed: 99.28631402183039%\nCurrent Time:  2020-03-18 15:14:52.584790\nprocessing 871c28e9-798e-3196-abd1-9262f43655c8\nprocessed: 99.32829554995803%\nCurrent Time:  2020-03-18 15:14:52.650836\nprocessing 76987ad2-62a9-31b9-85be-83531438a4aa\nprocessed: 99.37027707808565%\nCurrent Time:  2020-03-18 15:14:52.709198\nprocessing 6854153f-ca0a-3332-8852-0256031bfbde\nprocessed: 99.41225860621327%\nCurrent Time:  2020-03-18 15:14:52.764564\nprocessing b6ea474c-de97-3d4f-89b0-ee4e67e3e6ca\nprocessed: 99.45424013434089%\nCurrent Time:  2020-03-18 15:14:52.821772\nprocessing 69d46b3f-17a9-3792-9db9-9acf64a17443\nprocessed: 99.49622166246851%\nCurrent Time:  2020-03-18 15:14:52.888075\nprocessing 453f7ead-f091-3576-97db-8316df2825a6\nprocessed: 99.53820319059614%\nCurrent Time:  2020-03-18 15:14:52.951404\nprocessing 8706907d-3b1a-3f3a-a2f2-2b2cdc179bbf\nprocessed: 99.58018471872376%\nCurrent Time:  2020-03-18 15:14:53.002503\nprocessing 13848e40-25e5-34b6-8b3c-32381234a360\nprocessed: 99.62216624685138%\nCurrent Time:  2020-03-18 15:14:53.074188\nprocessing 7b8f367f-4a0f-39b5-8971-794b83e180ff\nprocessed: 99.664147774979%\nCurrent Time:  2020-03-18 15:14:53.114448\nprocessing c2684060-e94d-3f3a-8437-cd013eaa030b\nprocessed: 99.70612930310662%\nCurrent Time:  2020-03-18 15:14:53.160223\nprocessing d16f7937-fdb2-391b-868e-864ffe10067e\nprocessed: 99.74811083123426%\nCurrent Time:  2020-03-18 15:14:53.213581\nprocessing ebefeb60-844f-3c4f-bc35-9380fb5286fb\nprocessed: 99.79009235936188%\nCurrent Time:  2020-03-18 15:14:53.259240\nprocessing a6e770c0-5cb5-303a-b443-00cc958f3807\nprocessed: 99.8320738874895%\nCurrent Time:  2020-03-18 15:14:53.315297\nprocessing 3c111193-9f3e-3be6-8d5e-a6ee3056dda1\nprocessed: 99.87405541561712%\nCurrent Time:  2020-03-18 15:14:53.380146\nprocessing b1bacb13-a7e0-34f8-9bf9-2e4202c511cc\nprocessed: 99.91603694374476%\nCurrent Time:  2020-03-18 15:14:53.448478\nprocessing 4b00f33c-876b-3d78-9dcc-97b98a3e1c2d\nprocessed: 99.95801847187238%\nCurrent Time:  2020-03-18 15:14:53.500459\nprocessing 16fef62e-2898-3431-b0c6-3ea138618812\nprocessed: 100.0%\nCurrent Time:  2020-03-18 15:14:53.546406\n"
    }
   ],
   "source": [
    "#Running processing for UK\n",
    "process_state_UK(\"General\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Praparing the cleaning algorithm\n",
    "\n",
    "# Writing Spacy Lemmatizer for lists function\n",
    "def spacy_list_lemmatizer(text, nlp):\n",
    "    nlp.disable_pipes('tagger', 'ner')\n",
    "    doclist = list(nlp.pipe(text, n_threads=3,  batch_size=50))\n",
    "    docs=[]\n",
    "    for i, doc in enumerate(doclist):\n",
    "        docs.append(' '.join([listitem.lemma_ for listitem in doc]))\n",
    "    return docs\n",
    "\n",
    "\n",
    "# creating the lemmas of all english co-searchterms\n",
    "searchterms_eng = ['\"agile\" capability', '\"agile\" ceremony', '\"agile\" certificate', '\"agile\" certification', '\"agile\" coach', '\"agile\" coaching', '\"agile\" delivery', '\"agile\" development', '\"agile\" digital capability ', '\"agile\" environment', '\"agile\" innovation', '\"agile\" management', '\"agile\" manifesto', '\"agile\" method', '\"agile\" methodology', '\"agile\" mindset', '\"agile\" operating model', '\"agile\" organisation', '\"agile\" organization unit', '\"agile\" principle', '\"agile\" processes', '\"agile\" programming', '\"agile\" project', '\"agile\" project management', '\"agile\" software development', '\"agile\" stages', '\"agile\" structures', '\"agile\" team', '\"agile\" techniques', '\"agile\" transformation', '\"agile\" transition', '\"agile\" Value', '\"agile\" walls', '\"agile\" way', '\"agile\" way of working', '\"agile\" work', '\"agile\" work practices', '\"agile\" working and management methods', '\"agile\" working culture', \"'agile' capability\", \"'agile' ceremony\", \"'agile' certificate\", \"'agile' certification\", \"'agile' coach\", \"'agile' coaching\", \"'agile' delivery\", \"'agile' development\", \"'agile' digital capability \", \"'agile' environment\", \"'agile' innovation\", \"'agile' management\", \"'Agile' Manifesto\", \"'agile' method\", \"'agile' methodology\", \"'agile' mindset\", \"'agile' operating model\", \"'agile' organisation\", \"'agile' organization unit\", \"'agile' principle\", \"'agile' processes\", \"'agile' programming\", \"'agile' project\", \"'agile' project management\", \"'agile' software development\", \"'agile' stages\", \"'agile' structures\", \"'agile' team\", \"'agile' techniques\", \"'agile' transformation\", \"'agile' transition\", \"'agile' Value\", \"'agile' walls\", \"'agile' way\", \"'agile' way of working\", \"'agile' work\", \"'agile' work practices\", \"'agile' working and management methods\", \"'agile' working culture\", \"backlog\", \"co-design\", \"collaboration spaces\", \"collaborative design\", \"cross-departmental approach\", \"daily scrum\", \"definition of done\", \"definition of ready\", \"design thinking\", \"development team\", \"empathy map\", \"extreme user\", \"fail fast\", \"fail forward\", \"free of hierarchy\", \"hierarchy free space\", \"innovation culture\", \"innovation management\", \"iteration\", \"iterative\", \"kanban board\", \"lean software development\", \"lean start up\", \"lean start-up\", \"lean startup\", \"minimum viable product\", \"MVP\", \"new team and management structures\", \"new work\", \"new working methods\", \"planning poker\", \"product backlog\", \"product owner\", \"project canvas\", \"project-based\", \"scrum\", \"scrum coach\", \"scrum master\", \"scrum team\", \"service design\", \"short-cycle results\", \"software development\", \"sprint\", \"sprint backlog\", \"sprint planning\", \"sprint retrospective\", \"sprint review\", \"time box\", \"timeboxing\", \"user needs\", \"user requirements\", \"user research\", \"user story\", \"user testing\", \"user-centred design\", \"user-friendly solutions\", \"user-oriented\", \"visual thinking\", \"whiteboard\", \"wicked problem\", \"wireframe\"]\n",
    "\n",
    "nlp_eng = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "searchterms_eng_lemma = spacy_list_lemmatizer(searchterms_eng, nlp_eng)\n",
    "\n",
    "# creating the lemmas of all german co-searchterms\n",
    "searchterms_de = [\"abteilungsübergreifender Ansatz\", \"'agil' arbeitende Organisationseinheit\", \"'agil' arbeitende Teams\", \"'agile' Arbeitsformen\", \"'agile' arbeitskultur\", \"'agile' Arbeitspraktiken\", \"'agile' arbeitsumgebung\", \"'agile' Arbeitsweise\", \"'agile' Bereitstellung\", \"'agile' digitale Fähigkeit \", \"'agile' Entwicklung\", \"'agile' Fähigkeiten \", \"'agile' Innovation\", \"'agile' Methode\", \"'agile' Methodik\", \"'agile' Organisation\", \"'agile' Phasen\", \"'agile' Prozesse\", \"'agile' Softwareentwicklung\", \"'agile' Softwareweiterentwicklung\", \"'agile' Strukturen\", \"'agile' Techniken\", \"'agile' Transformation\", \"'agile' Transition\", \"'agile' Wände\", \"'agile' Weise\", \"'agile' Zeremonie\", \"'agile' Zertifizierung\", \"'agilen' Arbeits- und Managementmethoden\", \"'agiler' Ansatz\", \"'agiler' Coach\", \"'agiler' Wert\", \"'agiles' Arbeiten\", \"'agiles' Betriebsmodell\", \"'agiles' Coaching\", \"'agiles' Management\", \"'agiles' Manifest\", \"'agiles' Mindset\", \"'agiles' Prinzip\", \"'agiles' Programmieren\", \"'agiles' Projekt\", \"'agiles' Projektmanagement\", \"'agiles' Team\", \"'agiles' Training \", \"'agiles' Zertifikat\", '\"agil\" arbeitende Organisationseinheit', '\"agil\" arbeitende Teams', '\"agile\" Arbeitsformen', '\"agile\" arbeitskultur', '\"agile\" Arbeitspraktiken', '\"agile\" arbeitsumgebung', '\"agile\" Arbeitsweise', '\"agile\" Bereitstellung', '\"agile\" digitale Fähigkeit ', '\"agile\" Entwicklung', '\"agile\" Fähigkeiten ', '\"agile\" Innovation', '\"agile\" Methode', '\"agile\" Methodik', '\"agile\" Organisation', '\"agile\" Phasen', '\"agile\" Prozesse', '\"agile\" Softwareentwicklung', '\"agile\" Softwareweiterentwicklung', '\"agile\" Strukturen', '\"agile\" Techniken', '\"agile\" Transformation', '\"agile\" Transition', '\"agile\" Wände', '\"agile\" Weise', '\"agile\" Zeremonie', '\"agile\" Zertifizierung', '\"agilen\" Arbeits- und Managementmethoden', '\"agiler\" Ansatz', '\"agiler\" Coach', '\"agiler\" Wert', '\"agiles\" Arbeiten', '\"agiles\" Betriebsmodell', '\"agiles\" Coaching', '\"agiles\" Management', '\"agiles\" Manifest', '\"agiles\" Mindset', '\"agiles\" Prinzip', '\"agiles\" Programmieren', '\"agiles\" Projekt', '\"agiles\" Projektmanagement', '\"agiles\" Team', '\"agiles\" Training ', '\"agiles\" Zertifikat', \"Backlog\", \"Co-Design\", \"Co-Kreation\", \"Daily Scrum\", \"Definition of Done\", \"Definition of Ready\", \"Design Thinking\", \"Empathy Map\", \"Entwicklungsteam\", \"Extremnutzer Nutzer\", \"fail fast\", \"fail forward\", \"hierarchiefrei\", \"hierarchiefreien Raum\", \"Innovationskultur\", \"Innovationsmanagement\", \"Iteration\", \"iterativ \", \"Kanbanboard\", \"Kollaborationsräume\", \"kollaboratives Design\", \"kurzzyklische Ergebnisse\", \"Lean Start up\", \"Lean Start-up\", \"Lean Startup\", \"Minimum Viable Product\", \"MVP\", \"neue Arbeitsweisen\", \"neue Team- und Führungsstrukturen\", \"New Work\",  \"Nutzeranforderungen\", \"Nutzerbedürfnisse\", \"Nutzerforschung\", \"nutzerorientiert\", \"nutzerorientierte Lösungen\", \"Nutzerstory\", \"Nutzertest\", \"nutzerzentriert\", \"nutzerzentriertes Design\", \"Planning Poker\", \"Product Backlog\", \"Product Owner\", \"Project Canvas\", \"Projektbasiert\", \"schlanke Softwareentwicklung\", \"Scrum\", \"Scrum Coach\", \"Scrum Master\", \"Scrum Team\", \"Service Design\", \"software entwicklung\", \"Sprint\", \"Sprint Backlog\", \"Sprint Planning\", \"Sprint Retrospective\", \"Sprint Review\", \"Timebox\", \"Timeboxing\", \"visuelles Denken\", \"Whiteboard\", \"Wicked Problem\", \"Wireframe\"]\n",
    "\n",
    "nlp_de = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "searchterms_de_lemma = spacy_list_lemmatizer(searchterms_de, nlp_de)\n",
    "\n",
    "searchterms_de_eng_lemma = searchterms_de_lemma + searchterms_eng_lemma\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the data and selecting only relevant pages based on the co-searchterms dictionary\n",
    "\n",
    "inputFileName = \"/Users/mxm/Google Drive/Masterstudium/Inhalte/Master Thesis/GitHubRepo/agile-in-government/Analysis/2_Data_Preprocessing/DATA/CSVs/UK/General.csv\"\n",
    "outputFileName = \"/Users/mxm/Google Drive/Masterstudium/Inhalte/Master Thesis/GitHubRepo/agile-in-government/Analysis/2_Data_Preprocessing/DATA/CSVs/UK/General_cleaned.csv\"\n",
    "\n",
    "# loading the spacy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# switching off irrelevant spacy functions\n",
    "nlp.disable_pipes('tagger', 'ner')\n",
    "\n",
    "# defining the keywords search function\n",
    "def searchfunction(row):\n",
    "    matchstring = \"\"\n",
    "    count = 0\n",
    "    for term in searchterms_eng_lemma:\n",
    "        if term.lower() in row['doctext_lemma']:\n",
    "            count += 1\n",
    "            matchstring = matchstring + term.lower() + \", \"\n",
    "    row['co_searchterms_nr'] = count\n",
    "    row['co_searchterms_matches'] = matchstring\n",
    "    return row\n",
    "\n",
    "\n",
    "# opening and cleaning / preprocessing dataframe\n",
    "with open(inputFileName, newline='') as inFile, open(outputFileName, 'w', newline='') as outFile:\n",
    "    # reading the csv\n",
    "    df = pd.read_csv(inFile)\n",
    "    # sorting the values by final date\n",
    "    df.sort_values(by=['final_date'])\n",
    "    # dropping all sites which didn't yield any match for the agile term\n",
    "    df.dropna(axis=0, how='any', thresh=None, subset=(['agile_term']), inplace=True)\n",
    "    # dropping potential duplicates based on the heading and the context of the first agile term mention\n",
    "    df.drop_duplicates(subset=(['heading', 'agile_context_pre', 'agile_context_post']), inplace=True)\n",
    "    # deleting sites with csv previews\n",
    "    df = df[~df.url.str.endswith(\"csv/preview\")]\n",
    "    # deleting search sites\n",
    "    df = df[~df.url.str.contains('/search/')]\n",
    "    # deleting agile metioned in a tweet that occurs on multiple sites \n",
    "    df['doctext'] = df['doctext'].map(lambda x: x.strip('#Agile Development lessons learned from Gov.uk'))\n",
    "    #deleting cases where no date has been found (htmldate attributes 2020-01-01 for no dates)\n",
    "    df = df[df['final_date'] != '2020-01-01 00:00:00']\n",
    "    #deleting all cases from 2020 because the analysis focuses on everything up until 2019\n",
    "    df = df[df['final_date'] < '2020-01-01 00:00:00']\n",
    "    # changing the final_date column to a pandas datetime object\n",
    "    df['final_date'] = pd.to_datetime(df['final_date'])\n",
    "\n",
    "    # lemmatizing the text\n",
    "    lemma = []\n",
    "    for doc in nlp.pipe(df['doctext'].astype('unicode').values, batch_size=50, n_threads=3):\n",
    "        if doc.is_parsed:\n",
    "            lemma.append([n.lemma_ for n in doc])\n",
    "        else:\n",
    "            lemma.append(None)\n",
    "\n",
    "    # storing the lemmas in a new column \n",
    "    df['doctext_lemma'] = lemma\n",
    "    \n",
    "    #making lemmas lowercase\n",
    "    df['doctext_lemma'] = df['doctext_lemma'].map(lambda lemmas: [x.lower() for x in lemmas])\n",
    "    \n",
    "    #joining list of lemmas into one string agein, so it becomes matchable by the searchfunction\n",
    "    df['doctext_lemma'] = df['doctext_lemma'].map(lambda lemmas: \" \".join(lemmas))\n",
    " \n",
    "    #applying the keyword search function \n",
    "    df = df.apply(searchfunction, axis = 1)\n",
    "\n",
    "    # delete everything that didn't yield a co-searchterm match\n",
    "    df = df[df['co_searchterms_nr'] != 0 ]\n",
    "    df.to_csv(outFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Organisation Names based on domain\n",
    "\n",
    "# specifying input and output file and paths\n",
    "inputFileName = \"/Users/mxm/Google Drive/Masterstudium/Inhalte/Master Thesis/GitHubRepo/agile-in-government/Analysis/2_Data_Preprocessing/DATA/CSVs/UK/General_cleaned.csv\"\n",
    "outputFileName = \"/Users/mxm/Google Drive/Masterstudium/Inhalte/Master Thesis/GitHubRepo/agile-in-government/Analysis/2_Data_Preprocessing/DATA/CSVs/UK/General_cleaned_org.csv\"\n",
    "\n",
    "# file with all the uk subdomains (manual eddited to include subsubdomains)\n",
    "domains_dict = pd.read_csv(\"/Users/mxm/Google Drive/Masterstudium/Inhalte/Master Thesis/GitHubRepo/agile-in-government/Analysis/2_Data_Preprocessing/DATA/CSVs/UK/List_of_gov.uk_domain_names_as_at_28_Oct_2019_manualEdit.csv\")\n",
    "\n",
    "# compiling regex to extract domains from url (also in the case of a site on national archives)\n",
    "domain_regex = re.compile(r'http(s?)://(?!.*http)(www\\.)?([a-zA-Z0-9\\-.]+)', re.IGNORECASE | re.UNICODE)\n",
    "\n",
    "# defining the url extraction function\n",
    "def extract(url):\n",
    "    match = domain_regex.search(url)\n",
    "    if match:\n",
    "        match_domain = match.group(3)\n",
    "        return '.'.join(match_domain.split('.')[-4:])\n",
    "    else:\n",
    "        print(url)\n",
    "        return ''\n",
    "\n",
    "# adding the domain / organisation names to the file\n",
    "with open(inputFileName, newline='') as inFile, open(outputFileName, 'w', newline='') as outFile:\n",
    "    df = pd.read_csv(inFile)\n",
    "    # getting the subdomain from \n",
    "    df['sub_domain'] = df.apply(lambda row: extract(row['url']), axis=1)\n",
    "    # merging the file with the dataframe that has all the subdomains and organisation names\n",
    "    df2 = pd.merge(df, domains_dict, left_on = 'sub_domain', right_on='Domain: Domain Name', how='left')\n",
    "    # saving the file\n",
    "    df2.to_csv(outFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### GERMANY ############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the function to read in all the html files, clean the text, get further publishing dates and select the earliest one, and store everything into csv\n",
    "def process_state_DE(state):\n",
    "\n",
    "    # compiling the regex search terms\n",
    "    agile_regex = re.compile(r'\\bagility\\b|\\bagil\\w{0,2}\\b|\\bagilität\\b', re.IGNORECASE | re.UNICODE) # for agil keyword\n",
    "    agile_context_regex = re.compile(r'\\s*([^\\s]+?)\\s+([^\\s]+?)\\s+([^\\s]+?)\\s+([^\\s]+?)\\s+agil.*?\\s+([^\\s]+)\\s+([^\\s]+?)\\s+([^\\s]+?)\\s+([^\\s]+?)\\s+', re.IGNORECASE | re.UNICODE) # to get the context\n",
    "    digital_regex = re.compile(r'\\bdigital\\w{0,2}\\b|\\bdigitalisierung\\b|\\bdigitale transformation\\b', re.IGNORECASE | re.UNICODE) # to get everything related digitalisation\n",
    "\n",
    "    # getting the csv which was produced by scrapy as input file\n",
    "    csv_main = csv.DictReader(open(\"/Users/mxm/Google Drive/Masterstudium/Inhalte/Master Thesis/GitHubRepo/agile-in-government/Analysis/1_Data_Collection/DATA/CSVs/Germany/agile_sites_output_Germany_{}.csv\".format(state)), fieldnames=[\"id\", 'url', 'domain', 'date1', 'date2', 'date3', 'heading'])\n",
    "\n",
    "    # creating an empty array to write into in the processing loop\n",
    "    array = {}\n",
    "\n",
    "    # getting every line of the input csv into the array\n",
    "    for line in csv_main: \n",
    "        array[line[\"id\"]] = line\n",
    "    \n",
    "    # specfifying the the directory where the text files should be stored\n",
    "    text_dir = \"/Users/mxm/Google Drive/Masterstudium/Inhalte/Master Thesis/GitHubRepo/agile-in-government/Analysis/2_Data_Preprocessing/DATA/TextFiles/Germany/{}\".format(state)\n",
    "    \n",
    "    # making the directory if it doesn't exist\n",
    "    pathlib.Path(text_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # specifying variables to keep track of the progress while processing\n",
    "    total_count = len(array)\n",
    "    processed = 0\n",
    "\n",
    "    # this is the main working loop\n",
    "    for id, line in array.items():   \n",
    "        # print function to keep track of which files are being processed     \n",
    "        print(\"processing {}\".format(id))\n",
    "        # getting the id from the array\n",
    "        id = line[\"id\"]\n",
    "        # getting the domain from the array\n",
    "        domain = line[\"domain\"]\n",
    "        # getting the url from the array\n",
    "        url = line[\"url\"]\n",
    "\n",
    "        #excluding hits from berlin jobs which are a lot and are de facto a germany wide job portal, which is of no relevance for this research \n",
    "        if domain == 'jobs.berlin.de':\n",
    "            continue\n",
    "            \n",
    "        # specifying the path where the html to process is located\n",
    "        path = '/Users/mxm/Google Drive/Masterstudium/Inhalte/Master Thesis/GitHubRepo/agile-in-government/Analysis/1_Data_Collection/DATA/HTMLs/Germany/{}/{}/{}.html'.format(state, domain, id)\n",
    "        # opening the html file while catching an encoding error     \n",
    "        try:  \n",
    "            soup = BeautifulSoup(open(path), \"html.parser\")\n",
    "        except UnicodeDecodeError:\n",
    "            soup = BeautifulSoup(open(path, encoding='windows-1252'), \"html.parser\")    \n",
    "        \n",
    "        ### In the following further potential publication dates are parsed and the oldest one is select as \"final date\" ###\n",
    "\n",
    "        # Getting date4\n",
    "        date4_element = soup.select_one(\"span.date\")\n",
    "        date4 = \"\"\n",
    "        if date4_element is not None:\n",
    "            date4 = date4_element.get_text()\n",
    "        \n",
    "        # Getting date6\n",
    "        date6_element = soup.select_one(\"h1#page-title + p\")\n",
    "        date6 = \"\"\n",
    "        if date6_element is not None:\n",
    "            date6 = date6_element.get_text()\n",
    "\n",
    "       # Getting date5\n",
    "        htmlparser = etree.HTMLParser()    \n",
    "        try:\n",
    "            tree = etree.parse(open(path), htmlparser)\n",
    "        except UnicodeDecodeError:\n",
    "            tree = etree.parse(open(path, encoding='windows-1252'), htmlparser)\n",
    "        \n",
    "        date5 = tree.xpath(\"substring(substring-after(/html//script[@type='application/ld+json']/text(), 'datePublished'), 4, 23)\")\n",
    "\n",
    "        # a special cases for date 5\n",
    "        date5_1 = tree.xpath(\"substring(substring-after(/html//script[@type='application/ld+json']/text(), 'datePublished'), 4, 25)\")\n",
    "\n",
    "\n",
    "        # Getting date7 from National Archives --> the date the site was archived on --> the date the site was released on would have been even earlier\n",
    "        date7 = tree.xpath(\"substring(substring-after(/html/head/script/text(), 'timestamp'),9,14)\")\n",
    "\n",
    "  \n",
    "        # different parsing for for htmldate\n",
    "        try:\n",
    "            html_tree = html.parse(open(path))\n",
    "        except UnicodeDecodeError:\n",
    "            html_tree = html.parse(open(path, encoding='windows-1252'))\n",
    "            \n",
    "\n",
    "        # Getting date with the htmldate package\n",
    "        htmldate = find_date(html_tree)\n",
    "\n",
    "\n",
    "        # since html date gives out nonetype when it doesn't find anything, it has to be respecified to empty string so that it works with dateparser later on\n",
    "        if htmldate is None:\n",
    "            htmldate = ''\n",
    "\n",
    "        #htmldate = htmldate.astype('str')\n",
    "\n",
    "\n",
    "\n",
    "        # assinging the already succesfully parsed dates during crawling stage\n",
    "        date1 = line[\"date1\"]\n",
    "        date2 = line[\"date2\"]\n",
    "\n",
    "\n",
    "        # Storing the oldest date as final date variable in python date format\n",
    "        date_vars = [date1, date2, date4, date5, date5_1, date6, date7, htmldate]\n",
    "        \n",
    "        final_date = None \n",
    "\n",
    "        for date_var in date_vars:\n",
    "            try:\n",
    "                if final_date is None:\n",
    "                    final_date = parse(date_var, ignoretz = True)\n",
    "                elif parse(date_var, ignoretz=True) < final_date:\n",
    "                    final_date = parse(date_var, ignoretz=True)\n",
    "            except dateutil.parser._parser.ParserError:\n",
    "                pass \n",
    "\n",
    "\n",
    "\n",
    "        #deleting all non-body text content from the html\n",
    "        \n",
    "        # defining the html elements to delete\n",
    "        for script in soup(['script', 'style', 'meta', 'a', 'head', 'footer', 'navbar', 'header', 'search-box', 'global-cookie-message', 'id=\"global-cookie-message\"', 'global-bar', 'menu', 'noscript', 'global-cookie-message', 'search']):\n",
    "            script.decompose()    # deleting those elements\n",
    "            \n",
    "        # geting text\n",
    "        doctext = soup.get_text()\n",
    "\n",
    "        # break into lines and remove leading and trailing space on each\n",
    "        lines = (line.strip() for line in doctext.splitlines())\n",
    "        # break multi-headlines into a line each\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        # drop blank lines\n",
    "        doctext = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "        \n",
    "  \n",
    "\n",
    "        # saving the cleaned text into the directory\n",
    "        with open(\"{}/{}.txt\".format(text_dir, id), \"w\") as textfile:\n",
    "            textfile.write(doctext)\n",
    "        \n",
    "\n",
    " \n",
    "        #### In the following the matches for the search terms (agile, digital) are gathered ####\n",
    "        \n",
    "        # finding the matches for agile and agility\n",
    "        agile_term = []\n",
    "        agile_term = agile_regex.findall(doctext.lower())\n",
    "    \n",
    "\n",
    "        # finding the matches for digital\n",
    "        digital_term = []\n",
    "        digital_term = digital_regex.findall(doctext.lower())\n",
    "\n",
    "\n",
    "        # finding the context for agile\n",
    "        agile_context = []\n",
    "        agile_context = agile_context_regex.search(doctext)\n",
    "\n",
    "        agile_context_pre = \"\"\n",
    "        agile_context_post = \"\"\n",
    "        \n",
    "        if agile_context is not None:\n",
    "            agile_context_pre = \" \".join(agile_context.group(1,2,3,4))\n",
    "            agile_context_post = \" \".join(agile_context.group(5,6,7,8))\n",
    "        \n",
    "\n",
    "        # assigning all generated variables as line items to the array\n",
    "\n",
    "        if len(agile_term) == 0:\n",
    "            line[\"agile_term\"] = \"\" \n",
    "        else:\n",
    "            line[\"agile_term\"] = \",\".join(agile_term)\n",
    "\n",
    "        if len(digital_term) == 0:\n",
    "            line[\"digital_term\"] = \"\" \n",
    "        else:\n",
    "            line[\"digital_term\"] = \",\".join(digital_term)\n",
    "\n",
    "\n",
    "        line[\"agile_context_pre\"] = agile_context_pre\n",
    "        line[\"agile_context_post\"] = agile_context_post \n",
    "        line[\"date4\"] = date4\n",
    "        line[\"date5\"] = date5\n",
    "        line[\"date5_1\"] = date5_1\n",
    "        line[\"date6\"] = date6\n",
    "        line[\"date7\"] = date7\n",
    "        line[\"final_date\"] = final_date\n",
    "        line[\"htmldate\"] = htmldate\n",
    "        line[\"country\"] = \"Germany\"\n",
    "        line[\"level\"] = state\n",
    "        line[\"text_file_loc\"] = \"{}/{}.txt\".format(text_dir, id)\n",
    "        line['doctext'] = doctext\n",
    "\n",
    "        \n",
    "        # Printing processing status\n",
    "        processed += 1\n",
    "        print(\"processed: {}%\".format((processed/total_count)*100))\n",
    "        print(\"Current Time: \", datetime.now())\n",
    "\n",
    "    # opening the output csv\n",
    "    outputfile = open(\"/Users/mxm/Google Drive/Masterstudium/Inhalte/Master Thesis/GitHubRepo/agile-in-government/Analysis/2_Data_Preprocessing/DATA/CSVs/Germany/{}.csv\".format(state), \"w\")\n",
    "    \n",
    "    # writing all the array lines into the csv file\n",
    "    writer = csv.DictWriter(outputfile, fieldnames=[\"id\", \"country\", \"level\", 'url', 'domain', 'date1', 'date2', 'date3', 'date4', 'date5', 'date5_1', 'date6', 'date7', 'htmldate', 'final_date', 'heading', 'agile_term', 'agile_context_pre', 'agile_context_post', 'digital_term', 'text_file_loc', 'doctext',])\n",
    "    writer.writeheader()\n",
    "    for id, line in array.items():\n",
    "        writer.writerow(line)\n",
    "    outputfile.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "99233f1-8657-3a8d-89f2-db47982199b0\nprocessed: 66.95652173913044%\nCurrent Time:  2020-03-18 17:16:12.073894\nprocessing 65431631-a516-3176-9374-3f9d3f14dee6\nprocessed: 67.3913043478261%\nCurrent Time:  2020-03-18 17:16:12.727633\nprocessing 44c904e2-f11a-3180-86ac-c4e6664ef598\nprocessed: 67.82608695652173%\nCurrent Time:  2020-03-18 17:16:13.037049\nprocessing e556de22-a503-3053-8ba7-02809d537ad6\nprocessed: 68.26086956521739%\nCurrent Time:  2020-03-18 17:16:13.061158\nprocessing 12ebba58-454c-3dec-bdf3-cfdb9f93cacc\nprocessed: 68.69565217391305%\nCurrent Time:  2020-03-18 17:16:13.367566\nprocessing 855d53b7-7535-3b8a-82b2-baba0171caac\nprocessed: 69.1304347826087%\nCurrent Time:  2020-03-18 17:16:13.677728\nprocessing acde6450-5e12-31fb-a830-0add6cd5848e\nprocessed: 69.56521739130434%\nCurrent Time:  2020-03-18 17:16:13.993742\nprocessing b0b83a7a-de59-3bd8-a9ff-0395864140fc\nprocessed: 70.0%\nCurrent Time:  2020-03-18 17:16:14.289997\nprocessing 94979a42-98b2-3b60-86b4-29ddf3701696\nprocessed: 70.43478260869566%\nCurrent Time:  2020-03-18 17:16:14.587363\nprocessing b848744d-3442-365b-9513-89eb804ff13d\nprocessed: 70.86956521739131%\nCurrent Time:  2020-03-18 17:16:15.236203\nprocessing ac03a56b-e09c-3a91-ad4b-6de16fbf3561\nprocessed: 71.30434782608695%\nCurrent Time:  2020-03-18 17:16:15.540387\nprocessing e410c153-597f-3b5a-855b-1135f39962eb\nprocessed: 71.73913043478261%\nCurrent Time:  2020-03-18 17:16:15.843158\nprocessing c7ec211f-8d5d-3833-b597-df3e44a24fd4\nprocessed: 72.17391304347827%\nCurrent Time:  2020-03-18 17:16:16.156583\nprocessing 1b2598e0-475c-30c9-897f-0f11d52b63e2\nprocessed: 72.60869565217392%\nCurrent Time:  2020-03-18 17:16:16.470267\nprocessing 04ae987b-e161-3f82-930c-46b7da8c067d\nprocessed: 73.04347826086956%\nCurrent Time:  2020-03-18 17:16:16.833970\nprocessing a660699e-7e30-3f84-ba73-90b12c304c7e\nprocessed: 73.47826086956522%\nCurrent Time:  2020-03-18 17:16:16.858536\nprocessing 5e15380b-e1b5-3c31-94ae-774549a75f72\nprocessed: 73.91304347826086%\nCurrent Time:  2020-03-18 17:16:16.878352\nprocessing e6073f61-8fa0-35a7-9e89-7110b47501f6\nprocessed: 74.34782608695653%\nCurrent Time:  2020-03-18 17:16:17.241995\nprocessing 20faef20-6caf-3937-b9c2-85e4b2e774a0\nprocessed: 74.78260869565217%\nCurrent Time:  2020-03-18 17:16:17.547522\nprocessing 3c2641a9-fed2-3ee5-a076-c8a76401d776\nprocessed: 75.21739130434783%\nCurrent Time:  2020-03-18 17:16:18.248545\nprocessing d42d6f2d-ded3-35a9-b055-7f594c6d2cf0\nprocessed: 75.65217391304347%\nCurrent Time:  2020-03-18 17:16:18.273600\nprocessing 024d0465-b505-3879-89fb-92ec0e2bad78\nprocessed: 76.08695652173914%\nCurrent Time:  2020-03-18 17:16:18.572224\nprocessing 48abeb56-0d9f-36c3-a64c-635c06c39fa0\nprocessed: 76.52173913043478%\nCurrent Time:  2020-03-18 17:16:18.883817\nprocessing 94c9c384-9e15-356b-a023-57bf324c2c6a\nprocessed: 76.95652173913044%\nCurrent Time:  2020-03-18 17:16:18.942298\nprocessing fb7435c1-d841-3be6-b088-d3663f02dcfe\nprocessed: 77.39130434782608%\nCurrent Time:  2020-03-18 17:16:19.004549\nprocessing 4d407db3-ccec-3b05-a18f-d7158243e715\nprocessed: 77.82608695652173%\nCurrent Time:  2020-03-18 17:16:19.058767\nprocessing f2ef1fdc-2864-3b6a-8067-93cf510d8e5c\nprocessed: 78.26086956521739%\nCurrent Time:  2020-03-18 17:16:19.121538\nprocessing 8c48517f-a55e-3149-a83e-c39da13ece44\nprocessed: 78.69565217391305%\nCurrent Time:  2020-03-18 17:16:19.178762\nprocessing daeeee6a-3643-3810-b400-1e312dbc98ba\nprocessed: 79.13043478260869%\nCurrent Time:  2020-03-18 17:16:19.227224\nprocessing 78273b8b-c8f1-341f-84b6-a4581a0b7ac4\nprocessed: 79.56521739130434%\nCurrent Time:  2020-03-18 17:16:19.282250\nprocessing dd438fc4-a60e-3d4f-9dfb-e445c97c3686\nprocessed: 80.0%\nCurrent Time:  2020-03-18 17:16:19.348953\nprocessing d7146576-f63f-30a3-b9ca-9de55e0396ba\nprocessed: 80.43478260869566%\nCurrent Time:  2020-03-18 17:16:19.533574\nprocessing 3350ab36-6779-375b-a384-fbafce28c3a3\nprocessed: 80.8695652173913%\nCurrent Time:  2020-03-18 17:16:19.872834\nprocessing 11a009e3-0603-323c-8259-e3319dc37f5b\nprocessed: 81.30434782608695%\nCurrent Time:  2020-03-18 17:16:20.033257\nprocessing 23a8584b-ab24-3803-bb33-c5fab595d45d\nprocessed: 81.73913043478261%\nCurrent Time:  2020-03-18 17:16:20.349653\nprocessing 35b67607-6fd9-3613-ad87-94db968c88a4\nprocessed: 82.17391304347827%\nCurrent Time:  2020-03-18 17:16:20.654829\nprocessing 6d38ca5e-3bf1-369c-8250-b909d245658c\nprocessed: 82.6086956521739%\nCurrent Time:  2020-03-18 17:16:20.709021\nprocessing b1c10a1c-fc6e-349c-87df-bd020cdded4f\nprocessed: 83.04347826086956%\nCurrent Time:  2020-03-18 17:16:20.784450\nprocessing 51ccf1f0-fa4f-38bd-a5d3-f84441cfcd97\nprocessed: 83.47826086956522%\nCurrent Time:  2020-03-18 17:16:20.858040\nprocessing c09bb215-daad-33da-9ced-a0dc6f9e1c8a\nprocessed: 83.91304347826087%\nCurrent Time:  2020-03-18 17:16:20.933420\nprocessing 5f69f940-c73f-3965-8a63-1b2d5bfecbcf\nprocessed: 84.34782608695653%\nCurrent Time:  2020-03-18 17:16:21.005214\nprocessing f56ae34f-4477-389c-ac79-57b815ebfea0\nprocessed: 84.78260869565217%\nCurrent Time:  2020-03-18 17:16:21.073266\nprocessing 6ae368b4-529c-3eeb-8d57-947408b28822\nprocessed: 85.21739130434783%\nCurrent Time:  2020-03-18 17:16:21.158560\nprocessing 0581867e-8923-3616-83b2-4b69d9c1183f\nprocessed: 85.65217391304348%\nCurrent Time:  2020-03-18 17:16:21.242688\nprocessing 611b325d-7c2f-36c3-af67-6160eaa66149\nprocessed: 86.08695652173914%\nCurrent Time:  2020-03-18 17:16:21.334800\nprocessing 544042e8-9e99-390a-b4be-df11ce6f9bda\nprocessed: 86.52173913043478%\nCurrent Time:  2020-03-18 17:16:21.428694\nprocessing 8f978255-35e8-37b8-8cc7-557023f8680b\nprocessed: 86.95652173913044%\nCurrent Time:  2020-03-18 17:16:21.630201\nprocessing e9237636-ea2a-383d-91f1-7c56d76c2507\nprocessed: 87.39130434782608%\nCurrent Time:  2020-03-18 17:16:21.845615\nprocessing dd49482c-dbb5-3715-b3eb-1f9daa58e13b\nprocessed: 87.82608695652175%\nCurrent Time:  2020-03-18 17:16:22.034738\nprocessing 6b08ab01-b704-3262-b50c-3f960c38274b\nprocessed: 88.26086956521739%\nCurrent Time:  2020-03-18 17:16:22.252636\nprocessing e2aebcd4-ed4a-312e-bbbd-83a6897d0d89\nprocessed: 88.69565217391305%\nCurrent Time:  2020-03-18 17:16:22.442732\nprocessing 4c945f50-da2c-3a58-a415-fff66e467fb9\nprocessed: 89.13043478260869%\nCurrent Time:  2020-03-18 17:16:22.645223\nprocessing 70d3c1e1-a7b8-3b78-b1fd-ea6428c54e08\nprocessed: 89.56521739130436%\nCurrent Time:  2020-03-18 17:16:22.836664\nprocessing ca26b089-3ec9-3982-941b-83186ed02c1c\nprocessed: 90.0%\nCurrent Time:  2020-03-18 17:16:23.050665\nprocessing 439c6a5a-66d4-3dc6-80c1-8e6c1952d33f\nprocessed: 90.43478260869566%\nCurrent Time:  2020-03-18 17:16:23.748047\nprocessing 7f649ca5-de11-3046-b0b2-6a784117acbf\nprocessed: 90.8695652173913%\nCurrent Time:  2020-03-18 17:16:23.802057\nprocessing c35ec756-386d-3638-9dff-25af12cb80c6\nprocessed: 91.30434782608695%\nCurrent Time:  2020-03-18 17:16:24.148060\nprocessing 8606872e-6288-3856-9a0f-0b3bf5998b6a\nprocessed: 91.73913043478261%\nCurrent Time:  2020-03-18 17:16:24.209022\nprocessing 37455dd0-4fe7-3501-a74b-dbf8e71c5a93\nprocessed: 92.17391304347827%\nCurrent Time:  2020-03-18 17:16:24.267306\nprocessing e8e3f60a-36e4-33e3-a98b-ea74ba0bf1c4\nprocessed: 92.6086956521739%\nCurrent Time:  2020-03-18 17:16:24.583910\nprocessing c0299f13-8f7f-3ba2-91ee-bdc2b045585e\nprocessed: 93.04347826086956%\nCurrent Time:  2020-03-18 17:16:24.803746\nprocessing a73466ea-0f0b-30b9-a479-7efff8462a96\nprocessed: 93.47826086956522%\nCurrent Time:  2020-03-18 17:16:24.885469\nprocessing 9c7834d0-2010-3f5f-939a-8721c254ab1a\nprocessed: 93.91304347826087%\nCurrent Time:  2020-03-18 17:16:24.940965\nprocessing 525ae14b-2dff-3929-baf5-fed71b4a0eb5\nprocessed: 94.34782608695652%\nCurrent Time:  2020-03-18 17:16:25.289412\nprocessing e2452fd4-3642-35cd-8abf-bc55ad60087e\nprocessed: 94.78260869565217%\nCurrent Time:  2020-03-18 17:16:25.344886\nprocessing ce13d505-3281-3a47-b81a-971fbcccd16f\nprocessed: 95.21739130434783%\nCurrent Time:  2020-03-18 17:16:25.710755\nprocessing 4c1ed64e-f52b-3d03-9c3a-4bf6614d76ad\nprocessed: 95.65217391304348%\nCurrent Time:  2020-03-18 17:16:26.083089\nprocessing 0f6e86a6-5592-3e97-83f4-eb95ee07b570\nprocessed: 96.08695652173913%\nCurrent Time:  2020-03-18 17:16:26.129079\nprocessing 8066b76a-1935-3a89-aad3-2fefad6cab97\nprocessed: 96.52173913043478%\nCurrent Time:  2020-03-18 17:16:26.442702\nprocessing 78b23290-33b5-321f-87f5-03bd35f26b99\nprocessed: 96.95652173913044%\nCurrent Time:  2020-03-18 17:16:26.540144\nprocessing 190f0913-e3a2-304c-95b7-83fc7ea7c4e1\nprocessed: 97.3913043478261%\nCurrent Time:  2020-03-18 17:16:26.936419\nprocessing b130db56-1e5d-30d4-b06f-106fb4264bba\nprocessed: 97.82608695652173%\nCurrent Time:  2020-03-18 17:16:27.264534\nprocessing cc089e48-29e2-396f-8755-2bd4ff408678\nprocessed: 98.26086956521739%\nCurrent Time:  2020-03-18 17:16:27.583457\nprocessing cb073c6d-9bec-3496-9da6-a485b1ccc1c2\nprocessed: 98.69565217391305%\nCurrent Time:  2020-03-18 17:16:27.903624\nprocessing 85d2e41b-c495-3bcc-879b-78ce00061a37\nprocessed: 99.1304347826087%\nCurrent Time:  2020-03-18 17:16:28.110061\nprocessing b1db8da1-fdb7-3138-8a39-9c947c2288aa\nprocessed: 99.56521739130434%\nCurrent Time:  2020-03-18 17:16:28.421258\nprocessing 3f43e7cb-70fc-38f7-80fa-9dcc7ac93916\nprocessed: 100.0%\nCurrent Time:  2020-03-18 17:16:28.730853\nprocessing  Baden-Wuerttemberg\nprocessing e506461e-0203-3b25-a8bb-d5dbc43d5e55\nprocessed: 1.0%\nCurrent Time:  2020-03-18 17:16:28.991116\nprocessing 9406c169-152a-3b46-b16d-f1c71a9878c0\nprocessed: 2.0%\nCurrent Time:  2020-03-18 17:16:29.135056\nprocessing 0faa488a-65a4-3930-89af-399d368b214c\nprocessed: 3.0%\nCurrent Time:  2020-03-18 17:16:29.335326\nprocessing 3dbc311f-367d-3a81-9767-51a7951a0ab0\nprocessed: 4.0%\nCurrent Time:  2020-03-18 17:16:29.554468\nprocessing ef7e5ecc-d2c8-3b2a-aa76-dbda83c4190e\nprocessed: 5.0%\nCurrent Time:  2020-03-18 17:16:29.763127\nprocessing a3f8d11b-5417-368a-a960-480233083f93\nprocessed: 6.0%\nCurrent Time:  2020-03-18 17:16:30.308993\nprocessing eb841062-0e50-3635-a135-53996682b795\nprocessed: 7.000000000000001%\nCurrent Time:  2020-03-18 17:16:30.453439\nprocessing ba491361-efd9-3861-901f-742aaea95ea7\nprocessed: 8.0%\nCurrent Time:  2020-03-18 17:16:30.596227\nprocessing 595864eb-7ec6-36c0-bb38-364f2dd75b9b\nprocessed: 9.0%\nCurrent Time:  2020-03-18 17:16:30.742475\nprocessing 3db7f6e8-e4df-37d3-81f4-796b35d7d082\nprocessed: 10.0%\nCurrent Time:  2020-03-18 17:16:30.891228\nprocessing b17632e8-434b-3a39-b6dd-d22dd3898f93\nprocessed: 11.0%\nCurrent Time:  2020-03-18 17:16:31.037020\nprocessing 3dca04c5-a329-398d-9610-f9cb1f7a09cd\nprocessed: 12.0%\nCurrent Time:  2020-03-18 17:16:31.163324\nprocessing 81d8c8d3-0f31-36e2-b862-86c4b1edf0fb\nprocessed: 13.0%\nCurrent Time:  2020-03-18 17:16:31.413322\nprocessing e47a1a62-1a96-383e-8551-422723c6aa28\nprocessed: 14.000000000000002%\nCurrent Time:  2020-03-18 17:16:31.803526\nprocessing 9ad8f02b-356c-3cfd-8b06-2c9570710638\nprocessed: 15.0%\nCurrent Time:  2020-03-18 17:16:32.136813\nprocessing 6df644ea-0fa2-3472-a9b3-82c0ba2965ec\nprocessed: 16.0%\nCurrent Time:  2020-03-18 17:16:32.470500\nprocessing e8886d27-065b-3575-a3d9-95976cbad1e0\nprocessed: 17.0%\nCurrent Time:  2020-03-18 17:16:32.788522\nprocessing f2e42913-bdf5-3ba1-a2e8-456ec0e8b7c4\nprocessed: 18.0%\nCurrent Time:  2020-03-18 17:16:33.136589\nprocessing bf33f23c-a4fc-3642-830d-114cc733cb79\nprocessed: 19.0%\nCurrent Time:  2020-03-18 17:16:33.447567\nprocessing 989b86a8-c47a-3f4a-9554-217796dced6a\nprocessed: 20.0%\nCurrent Time:  2020-03-18 17:16:33.754658\nprocessing 0bac25b9-6534-3f77-af67-0aa8a136a06b\nprocessed: 21.0%\nCurrent Time:  2020-03-18 17:16:34.069506\nprocessing 99fb13de-ff45-3ba1-89ac-dcd2fef6f9f1\nprocessed: 22.0%\nCurrent Time:  2020-03-18 17:16:34.415939\nprocessing 6edd49f8-1467-35cf-bf5a-fbd626e4a8cd\nprocessed: 23.0%\nCurrent Time:  2020-03-18 17:16:34.735594\nprocessing 0bbf7f03-3f9f-3804-b318-3e8112f9546e\nprocessed: 24.0%\nCurrent Time:  2020-03-18 17:16:35.410865\nprocessing 51ce8f1b-17cb-39a9-b5b1-fb2392196037\nprocessed: 25.0%\nCurrent Time:  2020-03-18 17:16:35.749740\nprocessing 0ed80ad9-214a-3a50-96e1-b1b7e542ddc0\nprocessed: 26.0%\nCurrent Time:  2020-03-18 17:16:36.105930\nprocessing 33601d75-0c21-3a9b-9265-286a1400248a\nprocessed: 27.0%\nCurrent Time:  2020-03-18 17:16:36.448103\nprocessing c27a3918-ada5-3398-840f-f2bcaa991dbd\nprocessed: 28.000000000000004%\nCurrent Time:  2020-03-18 17:16:36.809171\nprocessing 6279ea6d-ae9b-30cc-a1af-c60763467659\nprocessed: 28.999999999999996%\nCurrent Time:  2020-03-18 17:16:37.172099\nprocessing eead38b4-8b34-302f-99fe-1c027af73211\nprocessed: 30.0%\nCurrent Time:  2020-03-18 17:16:37.524895\nprocessing 61278e88-96f5-3bcf-98ce-109c9342292f\nprocessed: 31.0%\nCurrent Time:  2020-03-18 17:16:37.868850\nprocessing aa25e291-85d9-3556-a3d6-4559875c4fdc\nprocessed: 32.0%\nCurrent Time:  2020-03-18 17:16:38.214491\nprocessing 55c8d820-97df-3ba6-931d-1dff5004f9fc\nprocessed: 33.0%\nCurrent Time:  2020-03-18 17:16:38.627136\nprocessing de8468cb-aaec-3a39-bb64-38a82292f051\nprocessed: 34.0%\nCurrent Time:  2020-03-18 17:16:38.967399\nprocessing f8e4b985-311d-38c5-be22-8baa064dd275\nprocessed: 35.0%\nCurrent Time:  2020-03-18 17:16:39.613457\nprocessing 4c8b1172-77d2-3ee4-81bd-10b462067ad1\nprocessed: 36.0%\nCurrent Time:  2020-03-18 17:16:39.924340\nprocessing 8785a877-5910-3c26-b5f6-e391890193bc\nprocessed: 37.0%\nCurrent Time:  2020-03-18 17:16:40.253991\nprocessing a2ee28aa-e75d-332d-b688-b8ea9c3e15c4\nprocessed: 38.0%\nCurrent Time:  2020-03-18 17:16:40.575818\nprocessing b078d509-a1e1-3aba-8200-891f039ec092\nprocessed: 39.0%\nCurrent Time:  2020-03-18 17:16:40.899509\nprocessing 8db9ddc3-a49d-3b9c-a40f-c79b0000b9d7\nprocessed: 40.0%\nCurrent Time:  2020-03-18 17:16:41.222131\nprocessing 5ad3ee0e-e8d7-30fe-862e-1697c05cee93\nprocessed: 41.0%\nCurrent Time:  2020-03-18 17:16:41.546416\nprocessing 0f522d84-cf3b-3834-972f-be3a8470f66c\nprocessed: 42.0%\nCurrent Time:  2020-03-18 17:16:41.874168\nprocessing cf9e71e8-b0f9-316d-bcb9-afbd554ce79a\nprocessed: 43.0%\nCurrent Time:  2020-03-18 17:16:42.223781\nprocessing 7b28cd0e-0075-3997-84f0-e351943fac2c\nprocessed: 44.0%\nCurrent Time:  2020-03-18 17:16:42.539998\nprocessing 29b8c1b4-fe8e-3e9e-800a-a2ea136c5731\nprocessed: 45.0%\nCurrent Time:  2020-03-18 17:16:42.844852\nprocessing bb89e9b8-8c27-37fc-bfa2-833de808f889\nprocessed: 46.0%\nCurrent Time:  2020-03-18 17:16:43.161044\nprocessing 6b77228f-3ad9-3474-b955-7155b757a88d\nprocessed: 47.0%\nCurrent Time:  2020-03-18 17:16:43.480435\nprocessing f37d79dc-2ce5-34b9-8ac5-71c7db241e37\nprocessed: 48.0%\nCurrent Time:  2020-03-18 17:16:44.147170\nprocessing b98aebfa-2a08-30c3-9c91-66169345b0a8\nprocessed: 49.0%\nCurrent Time:  2020-03-18 17:16:44.459258\nprocessing 44f4ba45-4bba-3561-9d58-43d074efb333\nprocessed: 50.0%\nCurrent Time:  2020-03-18 17:16:44.784149\nprocessing 1237c0ed-a900-34e1-a4a9-f43ab028ad0a\nprocessed: 51.0%\nCurrent Time:  2020-03-18 17:16:45.103369\nprocessing beeb3a27-b74c-3580-b860-0fa39e885081\nprocessed: 52.0%\nCurrent Time:  2020-03-18 17:16:45.416909\nprocessing 727fef34-7187-3edf-b5e8-bc855294a8a2\nprocessed: 53.0%\nCurrent Time:  2020-03-18 17:16:45.732429\nprocessing 6698436f-b869-3603-bff2-08cd9664462d\nprocessed: 54.0%\nCurrent Time:  2020-03-18 17:16:46.053916\nprocessing 7205bb9c-be55-3a9d-bdb4-115fa39859cc\nprocessed: 55.00000000000001%\nCurrent Time:  2020-03-18 17:16:46.368692\nprocessing 8d3537bf-2178-3f88-bb81-dc02c61e139b\nprocessed: 56.00000000000001%\nCurrent Time:  2020-03-18 17:16:46.682601\nprocessing 0c8d2988-0e32-3865-9ec9-db6daa399c48\nprocessed: 56.99999999999999%\nCurrent Time:  2020-03-18 17:16:47.019137\nprocessing ea2241ac-8f56-374e-b9a2-3efcd9157ab5\nprocessed: 57.99999999999999%\nCurrent Time:  2020-03-18 17:16:47.368358\nprocessing 9954d93e-1867-339b-af98-31401f50b6a7\nprocessed: 59.0%\nCurrent Time:  2020-03-18 17:16:48.055034\nprocessing d5016ac0-55ce-3414-bb38-c7e98ad78e46\nprocessed: 60.0%\nCurrent Time:  2020-03-18 17:16:48.373351\nprocessing b32911b7-5802-3a0d-94cf-6a22dddd6463\nprocessed: 61.0%\nCurrent Time:  2020-03-18 17:16:48.701412\nprocessing 9cc08892-c726-313b-a8da-12f544564289\nprocessed: 62.0%\nCurrent Time:  2020-03-18 17:16:49.024956\nprocessing 1ba0a610-b94a-3b1b-84ce-30a134ca08a0\nprocessed: 63.0%\nCurrent Time:  2020-03-18 17:16:49.336731\nprocessing c6455392-1866-39e2-b522-03baefc42e3a\nprocessed: 64.0%\nCurrent Time:  2020-03-18 17:16:49.665505\nprocessing 4a1b1c22-e92c-31dd-b7fb-a4b0c250aa34\nprocessed: 65.0%\nCurrent Time:  2020-03-18 17:16:50.000586\nprocessing 98688999-52ee-352b-8650-8d5a45fe5760\nprocessed: 66.0%\nCurrent Time:  2020-03-18 17:16:50.317981\nprocessing 90c9302e-69f0-38a0-8bf0-f6e8f48d659f\nprocessed: 67.0%\nCurrent Time:  2020-03-18 17:16:50.644245\nprocessing 4b9e30c1-43a6-3623-994e-8b73aced070f\nprocessed: 68.0%\nCurrent Time:  2020-03-18 17:16:50.958661\nprocessing f08f2b6a-7e91-3ee7-97da-87eda350b040\nprocessed: 69.0%\nCurrent Time:  2020-03-18 17:16:51.306482\nprocessing 98369a47-5fb6-34d0-b1f4-c2941d0dbc15\nprocessed: 70.0%\nCurrent Time:  2020-03-18 17:16:51.391959\nprocessing f9200a51-ebd6-39ca-a749-61cc3ebf786f\nprocessed: 71.0%\nCurrent Time:  2020-03-18 17:16:51.474509\nprocessing 03805c1b-1a52-3139-bae7-c392293b0da4\nprocessed: 72.0%\nCurrent Time:  2020-03-18 17:16:51.559055\nprocessing 485044ee-2b77-3534-8c8f-c6aa727c74bf\nprocessed: 73.0%\nCurrent Time:  2020-03-18 17:16:51.643679\nprocessing b734464a-e557-3374-a0c0-40013441fc61\nprocessed: 74.0%\nCurrent Time:  2020-03-18 17:16:51.830641\nprocessing 2f5f8ddb-d436-330c-a6c4-e04554cd78d0\nprocessed: 75.0%\nCurrent Time:  2020-03-18 17:16:51.983295\nprocessing 14675586-a89e-3896-ae73-c807697ac374\nprocessed: 76.0%\nCurrent Time:  2020-03-18 17:16:52.482695\nprocessing c77cbff6-4507-3741-a1a9-6d9a0e13257f\nprocessed: 77.0%\nCurrent Time:  2020-03-18 17:16:52.619658\nprocessing 32bfa954-579c-31e0-82a9-d5d40ff3d007\nprocessed: 78.0%\nCurrent Time:  2020-03-18 17:16:52.750866\nprocessing bf4df335-9adb-341f-a17a-b7979b6b792d\nprocessed: 79.0%\nCurrent Time:  2020-03-18 17:16:52.881270\nprocessing 2817f501-353d-3a81-b9d4-b780fb12fd03\nprocessed: 80.0%\nCurrent Time:  2020-03-18 17:16:53.024040\nprocessing 0a73fd72-a80c-3d98-9176-e7bfdf740269\nprocessed: 81.0%\nCurrent Time:  2020-03-18 17:16:53.163211\nprocessing 7b0354bc-705e-3da8-9ca8-e0de6b2a602d\nprocessed: 82.0%\nCurrent Time:  2020-03-18 17:16:53.358346\nprocessing 63d5f5af-0586-3855-8eff-54a758095e80\nprocessed: 83.0%\nCurrent Time:  2020-03-18 17:16:53.570877\nprocessing a922bc9a-2eef-3374-987d-773d7a7a2df6\nprocessed: 84.0%\nCurrent Time:  2020-03-18 17:16:53.805891\nprocessing ee9be5bf-46e7-3bce-b658-3bc5f51d081f\nprocessed: 85.0%\nCurrent Time:  2020-03-18 17:16:54.029491\nprocessing 324cc967-7999-30be-9bd3-706150f612bf\nprocessed: 86.0%\nCurrent Time:  2020-03-18 17:16:54.251851\nprocessing 336b93cb-f60f-38ff-aba1-10b8bea3c00a\nprocessed: 87.0%\nCurrent Time:  2020-03-18 17:16:54.469464\nprocessing f29fc08c-c312-36b3-9161-c88d22453c93\nprocessed: 88.0%\nCurrent Time:  2020-03-18 17:16:54.611607\nprocessing 3913a5b8-cf3d-350c-82da-127ef389090b\nprocessed: 89.0%\nCurrent Time:  2020-03-18 17:16:56.507786\nprocessing 1a083bfc-66b9-3a18-83bc-71e27676d00a\nprocessed: 90.0%\nCurrent Time:  2020-03-18 17:16:58.096952\nprocessing e0b9e646-0cde-3608-b529-ae41f0fedd2f\nprocessed: 91.0%\nCurrent Time:  2020-03-18 17:17:00.029930\nprocessing be410387-3d2d-313d-ac25-111af70ba360\nprocessed: 92.0%\nCurrent Time:  2020-03-18 17:17:01.576091\nprocessing 90116f8d-34ac-35d6-81d3-0c74787a0f5f\nprocessed: 93.0%\nCurrent Time:  2020-03-18 17:17:02.106883\nprocessing 9158383e-1cbc-33ef-9808-9f8bb47bc4ed\nprocessed: 94.0%\nCurrent Time:  2020-03-18 17:17:02.242390\nprocessing 38534b54-f9f8-30dd-a42c-57fb2c707f39\nprocessed: 95.0%\nCurrent Time:  2020-03-18 17:17:02.380961\nprocessing 020bea80-f3d3-3d82-a19c-02fe7b65dc52\nprocessed: 96.0%\nCurrent Time:  2020-03-18 17:17:02.559386\nprocessing 4ee57eb2-1e7b-32ac-abc4-7f1068d9f35f\nprocessed: 97.0%\nCurrent Time:  2020-03-18 17:17:02.717718\nprocessing f7f21193-27df-3aec-96eb-f0cf81ea1caf\nprocessed: 98.0%\nCurrent Time:  2020-03-18 17:17:02.966168\nprocessing 3503ca13-52d8-3520-9115-f899a673ade7\nprocessed: 99.0%\nCurrent Time:  2020-03-18 17:17:03.492244\nprocessing 945a4952-ba10-3d80-91b8-8669febc9499\nprocessed: 100.0%\nCurrent Time:  2020-03-18 17:17:03.729856\n"
    }
   ],
   "source": [
    "#######Processing Statewise Germany\n",
    "\n",
    "state_list = [\"Federal\", \"Schleswig-Holstein\",\"Bremen\",\"Hamburg\",\"Lower Saxony\",\"Thuringia\",\"Saxony-Anhalt\",\"Saxony\",\"Mecklenburg-Vorpommern\",\"Berlin\",\"Brandenburg\",\"North Rhine-Westphalia\",\"Rhineland-Palatinate\",\"Saarland\", \"Hesse\",\"Bavaria\",\"Baden-Wuerttemberg\"]\n",
    "\n",
    "for state in state_list:\n",
    "    print('processing ', state)\n",
    "    process_state_DE(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####Merging all German States\n",
    "state_list = [\"Federal\", \"Schleswig-Holstein\",\"Bremen\",\"Hamburg\",\"Lower Saxony\",\"Thuringia\",\"Saxony-Anhalt\",\"Saxony\",\"Mecklenburg-Vorpommern\",\"Berlin\",\"Brandenburg\",\"North Rhine-Westphalia\",\"Rhineland-Palatinate\",\"Saarland\" ,\"Hesse\" ,\"Bavaria\",\"Baden-Wuerttemberg\"]\n",
    "\n",
    "state_df = {}\n",
    "\n",
    "def reading_csv(state):\n",
    "    inputFileName = \"/Users/mxm/Google Drive/Masterstudium/Inhalte/Master Thesis/GitHubRepo/agile-in-government/Analysis/2_Data_Preprocessing/DATA/CSVs/Germany/{}.csv\".format(state)\n",
    "    outputFileName = \"/Users/mxm/Google Drive/Masterstudium/Inhalte/Master Thesis/GitHubRepo/agile-in-government/Analysis/2_Data_Preprocessing/DATA/CSVs/Germany/Combined.csv\"\n",
    "    with open(inputFileName, newline='') as inFile, open(outputFileName, 'w', newline='') as outFile:\n",
    "        df = pd.read_csv(inFile)\n",
    "    return df\n",
    "\n",
    "combined_dataframe_Germany = None \n",
    "\n",
    "for state in state_list:\n",
    "    temp_df = reading_csv(state)\n",
    "    if combined_dataframe_Germany is None:\n",
    "       combined_dataframe_Germany = temp_df\n",
    "    else: \n",
    "        combined_dataframe_Germany = pd.concat([combined_dataframe_Germany,temp_df])\n",
    "\n",
    "    combined_dataframe_Germany.to_csv(\"/Users/mxm/Google Drive/Masterstudium/Inhalte/Master Thesis/GitHubRepo/agile-in-government/Analysis/2_Data_Preprocessing/DATA/CSVs/Germany/Combined.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the data and selecting only relevant pages based on the co-searchterms dictionary\n",
    "\n",
    "inputFileName = \"/Users/mxm/Google Drive/Masterstudium/Inhalte/Master Thesis/GitHubRepo/agile-in-government/Analysis/2_Data_Preprocessing/DATA/CSVs/Germany/Combined.csv\"\n",
    "outputFileName = \"/Users/mxm/Google Drive/Masterstudium/Inhalte/Master Thesis/GitHubRepo/agile-in-government/Analysis/2_Data_Preprocessing/DATA/CSVs/Germany/Combined_cleaned.csv\"\n",
    "\n",
    "# loading the spacy model\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "# switching off irrelevant spacy functions\n",
    "nlp.disable_pipes('tagger', 'ner')\n",
    "\n",
    "# defining the keywords search function\n",
    "def searchfunction(row):\n",
    "    matchstring = \"\"\n",
    "    count = 0\n",
    "    for term in searchterms_de_eng_lemma:\n",
    "        if term.lower() in row['doctext_lemma']:\n",
    "            count += 1\n",
    "            matchstring = matchstring + term.lower() + \", \"\n",
    "    row['co_searchterms_nr'] = count\n",
    "    row['co_searchterms_matches'] = matchstring\n",
    "    return row\n",
    "\n",
    "\n",
    "# opening and cleaning / preprocessing dataframe\n",
    "with open(inputFileName, newline='') as inFile, open(outputFileName, 'w', newline='') as outFile:\n",
    "    # reading the csv\n",
    "    df = pd.read_csv(inFile)\n",
    "    # sorting the values by final date\n",
    "    df.sort_values(by=['final_date'])\n",
    "    # dropping all sites which didn't yield any match for the agile term\n",
    "    df.dropna(axis=0, how='any', thresh=None, subset=(['agile_term']), inplace=True)\n",
    "    # dropping potential duplicates based on the heading and the context of the first agile term mention\n",
    "    df.drop_duplicates(subset=(['heading', 'agile_context_pre', 'agile_context_post']), inplace=True)\n",
    "    # changing htmldate to string for later processing\n",
    "    df['htmldate'] = df['htmldate'].astype(str)\n",
    "    # deleting sites with csv previews\n",
    "    df = df[~df.url.str.endswith(\"csv/preview\")]\n",
    "    # deleting job search site entries\n",
    "    df = df[~df.domain.str.contains(\"www.yojo.de\")]\n",
    "    # deleting search sites\n",
    "    df = df[~df.url.str.contains('/search/')] \n",
    "    # deleting search results for sachsen\n",
    "    df = df[~df.domain.str.contains('search.sachsen.de')] \n",
    "    # deleting terms related to agilis which is a lizard\n",
    "    df = df[~df.agile_term.str.contains('agilis')]\n",
    "    # deleting ticket counts\n",
    "    df = df[~df.url.str.contains('/tickets/')] \n",
    "    # deleting content where date has been wrongly classified and true date is none-attainable\n",
    "    df = df[~df.htmldate.str.contains('1999-01-01')] \n",
    "    # deleting non-german content\n",
    "    df = df[~df.url.str.contains('/FR/')] # deleting non-german content\n",
    "    df = df[~df.url.str.contains('/fr/')] # deleting non-german content\n",
    "    df = df[~df.url.str.contains( '/EN/'  )] # deleting non-german content\n",
    "    df = df[~df.url.str.contains('/en/' )] # deleting non-german content\n",
    "    df = df[~df.url.str.contains('/breg-en/' )] # deleting non-german content\n",
    "    df = df[~df.url.str.contains('/breg-fr/' )] # deleting non-german content\n",
    "    # deleting agile metioned in a tweet that occurs on multiple sites \n",
    "    df['doctext'] = df['doctext'].map(lambda x: x.strip('#Agile Development lessons learned from Gov.uk'))\n",
    "    #deleting cases where no date has been found (htmldate attributes 2020-01-01 for no dates)\n",
    "    df = df[df['final_date'] != '2020-01-01 00:00:00']\n",
    "    #deleting all cases from 2020 because the analysis focuses on everything up until 2019\n",
    "    df = df[df['final_date'] < '2020-01-01 00:00:00']\n",
    "    # changing the final_date column to a pandas datetime object\n",
    "    df['final_date'] = pd.to_datetime(df['final_date'])\n",
    "\n",
    "    # lemmatizing the text\n",
    "    lemma = []\n",
    "    for doc in nlp.pipe(df['doctext'].astype('unicode').values, batch_size=50, n_threads=3):\n",
    "        if doc.is_parsed:\n",
    "            lemma.append([n.lemma_ for n in doc])\n",
    "        else:\n",
    "            lemma.append(None)\n",
    "\n",
    "    # storing the lemmas in a new column \n",
    "    df['doctext_lemma'] = lemma\n",
    "    \n",
    "    #making lemmas lowercase\n",
    "    df['doctext_lemma'] = df['doctext_lemma'].map(lambda lemmas: [x.lower() for x in lemmas])\n",
    "    \n",
    "    #joining list of lemmas into one string agein, so it becomes matchable by the searchfunction\n",
    "    df['doctext_lemma'] = df['doctext_lemma'].map(lambda lemmas: \" \".join(lemmas))\n",
    " \n",
    "    #applying the keyword search function \n",
    "    df = df.apply(searchfunction, axis = 1)\n",
    "\n",
    "    # delete everything that didn't yield a co-searchterm match\n",
    "    df = df[df['co_searchterms_nr'] != 0 ]\n",
    "    df.to_csv(outFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### COMBINING ALL DATAFRAMES ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_germany =  pd.read_csv(\"/Users/mxm/Google Drive/Masterstudium/Inhalte/Master Thesis/GitHubRepo/agile-in-government/Analysis/2_Data_Preprocessing/DATA/CSVs/Germany/Combined_cleaned.csv\")\n",
    "df_uk = pd.read_csv('/Users/mxm/Google Drive/Masterstudium/Inhalte/Master Thesis/GitHubRepo/agile-in-government/Analysis/2_Data_Preprocessing/DATA/CSVs/UK/General_cleaned_org.csv')\n",
    "df_master = pd.concat([df_germany,df_uk])\n",
    "\n",
    "df_master.to_csv('/Users/mxm/Google Drive/Masterstudium/Inhalte/Master Thesis/GitHubRepo/agile-in-government/Analysis/2_Data_Preprocessing/DATA/CSVs/Master_File.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}



Check disk space:

df -h



Truncating the last 10000 lines:
First install moreutils: 
sudo apt-get install moreutils
Then do in the respective folder
tail -n 10000 filename.log | sponge filename.log

tail -n 10000 Bremen-2020-03-30-11-13-40.log | sponge Bremen-2020-03-30-11-13-40.log



USE: scrapy crawl somespider -s JOBDIR=crawls/somespider-1
to be able to pause and resume crawler


on my machine
cat states.txt | xargs -P 4 -I{} sh -c 'echo "Running $1"; scrapy crawl "$1" -s JOBDIR="CRAWLS/$1" >& "LOGS/$1-$2.log"' -- {} $(date +%s)

on google platform
cat states.txt | xargs -P 2 -I{} sh -c 'echo "Running $1"; scrapy crawl "$1" -s JOBDIR="CRAWLS/$1" --logfile "LOGS/$1-$2.log"' -- {} $(date +%Y-%m-%d-%H-%M-%S)



Attaching the session again: 

tmux attach-session -t Crawler

top
htop
q


SYNCING:
cd "/Users/mxm/Google Drive/Masterstudium/Inhalte/Master Thesis/GitHubRepo/agile-in-government/Analysis/1_Data_Collection/"
rsync -a mxm@35.241.150.189:/home/jupyter/agile-in-government/Analysis/1_Data_Collection/DATA/ DATA_GoogleCloud/


TODO: 

Bavaria didn't work ?!? -- DONE
 
- Hamburg made troubles: builtins.OSError: [Errno 22] Invalid argument DONE



writing 
    doctext into textfile and storing location into csv
    date --> few more cases from nationalarchives.gov.uk
    agil_context into separate files
